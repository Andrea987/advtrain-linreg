{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ggDSA-ktpXgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd80fc3f-f2d3-4933-debd-af08116bc8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end block\n"
          ]
        }
      ],
      "source": [
        "from itertools import cycle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import lasso_path\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import tqdm\n",
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_q(p):\n",
        "    if p != np.inf and p > 1:\n",
        "        q = p / (p - 1)\n",
        "    elif p == 1:\n",
        "        q = np.inf\n",
        "    else:\n",
        "        q = 1\n",
        "    return q\n",
        "\n",
        "\n",
        "class AdversarialTraining:\n",
        "    def __init__(self, X, y, p):\n",
        "        m, n = X.shape\n",
        "        q = compute_q(p)\n",
        "        # Formulate problem\n",
        "        param = cp.Variable(n)\n",
        "        param_norm = cp.pnorm(param, p=q)\n",
        "        adv_radius = cp.Parameter(name='adv_radius', nonneg=True)\n",
        "        abs_error = cp.abs(X @ param - y)\n",
        "        adv_loss = 1 / m * cp.sum((abs_error + adv_radius * param_norm) ** 2)\n",
        "        prob = cp.Problem(cp.Minimize(adv_loss))\n",
        "        self.prob = prob\n",
        "        self.adv_radius = adv_radius\n",
        "        self.param = param\n",
        "        self.warm_start = False\n",
        "\n",
        "    def __call__(self, adv_radius, **kwargs):\n",
        "        try:\n",
        "            self.adv_radius.value = adv_radius\n",
        "            self.prob.solve(warm_start=self.warm_start, **kwargs)\n",
        "            v = self.param.value\n",
        "        except:\n",
        "            v = np.zeros(self.param.shape)\n",
        "        return v\n",
        "\n",
        "\n",
        "def get_lasso_path(X, y, eps_lasso=1e-2):\n",
        "    alphas, coefs, _ = lasso_path(X, y, eps=eps_lasso)\n",
        "    coefs= np.concatenate([np.zeros([X.shape[1], 1]), coefs], axis=1)\n",
        "    alphas = np.concatenate([1e2 * np.ones([1]), alphas], axis=0)\n",
        "    return alphas, coefs, []\n",
        "\n",
        "\n",
        "def get_path(X, y, estimator, amax, eps=1e-5, n_alphas=200):\n",
        "    amin = eps * amax\n",
        "    alphas = np.logspace(np.log10(amin), np.log10(amax), n_alphas)\n",
        "    coefs_ = []\n",
        "    for a in tqdm.tqdm(alphas):\n",
        "        coefs = estimator(X, y, a)\n",
        "        coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "    return alphas, np.stack((coefs_)).T\n",
        "\n",
        "\n",
        "def plot_coefs(alphas, coefs, ax):\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.semilogx(1/alphas, coef_l, c=c)\n",
        "\n",
        "\n",
        "def plot_coefs_l1norm(coefs, ax):\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    l1norm = np.abs(coefs).mean(axis=0)\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.plot(l1norm, coef_l, c=c)\n",
        "\n",
        "\n",
        "def multiple_imputation(nbr_mi, X_nan):\n",
        "    for i in range(nbr_mi):\n",
        "       n_i = np.random.randint(0, 1000)\n",
        "       ice = IterativeImputer()\n",
        "       ice_mean = IterativeImputer(random_state=n_i, max_iter=50, sample_posterior=True)\n",
        "       res = ice_mean.fit_transform(X_nan)\n",
        "       #print(\"fin res \", res)\n",
        "       return res\n",
        "\n",
        "#X, y = datasets.load_diabetes(return_X_y=True)\n",
        "#print(X.shape, y.shape)\n",
        "# Standardize data\n",
        "\n",
        "n = 200\n",
        "d = 4\n",
        "X = np.random.rand(n, d)\n",
        "X -= X.mean(axis=0)\n",
        "X /= X.std(axis=0)\n",
        "b = np.random.rand(d)\n",
        "y = X @ b\n",
        "\n",
        "masks = np.random.binomial(1, 0.1, (n, d))  # 1 missing, 0 seen\n",
        "#M = np.sum(masks, axis=1)\n",
        "#X_nan = X.copy()\n",
        "#X_nan[masks == 1] = np.nan\n",
        "\n",
        "#X = multiple_imputation(1, X_nan)\n",
        "print(\"end block\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(num='advtrain_linf')\n",
        "linfadvtrain = AdversarialTraining(X, y, p=np.inf)\n",
        "estimator = lambda X, y, a:  linfadvtrain(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf  = get_path(X, y, estimator, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf, ax)"
      ],
      "metadata": {
        "id": "N1uZArXkprXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(num='lasso')\n",
        "alphas_lasso, coefs_lasso, _ = get_lasso_path(X, y)\n",
        "plot_coefs_l1norm(coefs_lasso, ax)"
      ],
      "metadata": {
        "id": "qOOJdRgbp-3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation given mean and covariance matrix\n",
        "\n",
        "def imputation_elliptic(mu, sigma, x, masks):\n",
        "  # mu, mean elliptical distribution (,d)\n",
        "  # sigma, cov matrix elliptical distribution (d, d)\n",
        "  # x: dataset (n, d)\n",
        "  # masks: mask data, 0 seen, 1 missing\n",
        "  n, d = x.shape\n",
        "  print(n, d)\n",
        "  for i in range(n):\n",
        "    x_c = x[i, :]\n",
        "    m_bool = (m[i, :] == 0)\n",
        "    sigma_aa = sigma[m_bool, :][:, m_bool]\n",
        "    sigma_ma = sigma\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qyWskXpdOW9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = 4\n",
        "S = np.random.randint(low=1, high=4, size=(r, r))\n",
        "S = S.T @ S + np.eye(r)\n",
        "print(S)\n",
        "m = np.array([0, 0, 1, 0])  # 1 missing, 0 seen\n",
        "m_bool = (m == 0)\n",
        "print(m_bool)\n",
        "S_sub = S[m_bool, :][:, m_bool]\n",
        "print(S_sub)\n",
        "\n",
        "S_ma = S[m_bool, :][:, ~m_bool]\n",
        "print(S_ma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mlM-FR-OfL4",
        "outputId": "4e6276dc-afa7-4aaa-de7a-d1cb2d58ec34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[29. 13. 20. 24.]\n",
            " [13.  8. 10. 12.]\n",
            " [20. 10. 19. 19.]\n",
            " [24. 12. 19. 27.]]\n",
            "[ True  True False  True]\n",
            "[[29. 13. 24.]\n",
            " [13.  8. 12.]\n",
            " [24. 12. 27.]]\n",
            "[[[29. 13. 20. 24.]\n",
            "  [13.  8. 10. 12.]\n",
            "  [20. 10. 19. 19.]\n",
            "  [24. 12. 19. 27.]]\n",
            "\n",
            " [[29. 13. 20. 24.]\n",
            "  [13.  8. 10. 12.]\n",
            "  [20. 10. 19. 19.]\n",
            "  [24. 12. 19. 27.]]]\n"
          ]
        }
      ]
    }
  ]
}