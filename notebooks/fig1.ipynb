{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea987/advtrain-linreg/blob/main/notebooks/fig1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Sgo-CifolM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ggDSA-ktpXgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1aba87-28bd-47a4-f377-c08821c582c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CLARABEL', 'CVXOPT', 'GLPK', 'GLPK_MI', 'HIGHS', 'OSQP', 'SCIPY', 'SCS']\n",
            "coef :  [ 2.00355591e+00 -9.91924836e-01  2.42505116e-02  1.44015120e-03]\n",
            "intercpt  -0.011952031234424948\n",
            "coef :  [ 2.00355591e+00 -9.91924836e-01  2.42505116e-02  1.44015120e-03]\n",
            "intercpt  -0.011952031234424948\n",
            "coef :  [ 2.00353236e+00 -9.91917979e-01  2.42484396e-02  1.43509774e-03]\n",
            "intercpt  -0.011953781999160138\n",
            "coef :  [ 2.00353236e+00 -9.91917979e-01  2.42484396e-02  1.43509774e-03]\n",
            "intercpt  -0.011953781999160138\n",
            "coef :  [ 2.00118001e+00 -9.91232810e-01  2.40416285e-02  9.31125631e-04]\n",
            "intercpt  -0.012128443333004307\n",
            "coef :  [ 2.00118001e+00 -9.91232810e-01  2.40416285e-02  9.31125631e-04]\n",
            "intercpt  -0.012128443333004307\n",
            "coef :  [ 1.98000915 -0.98501416  0.022159   -0.00349085]\n",
            "intercpt  -0.013679856072411659\n",
            "coef :  [ 1.98000915 -0.98501416  0.022159   -0.00349085]\n",
            "intercpt  -0.013679856072411659\n",
            "end block\n"
          ]
        }
      ],
      "source": [
        "from re import VERBOSE\n",
        "from itertools import cycle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.linear_model import lasso_path\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import ridge_regression\n",
        "import tqdm\n",
        "import cvxpy as cp\n",
        "print(cp.installed_solvers())\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import traceback\n",
        "\n",
        "\n",
        "def compute_q(p):\n",
        "    if p != np.inf and p > 1:\n",
        "        q = p / (p - 1)\n",
        "    elif p == 1:\n",
        "        q = np.inf\n",
        "    else:\n",
        "        q = 1\n",
        "    return q\n",
        "\n",
        "\n",
        "class AdversarialTraining:\n",
        "    def __init__(self, X, y, S_dict, p):  # S is the matrix such that ||S^(-1) @ Dx||\\leq delta. As a consequence, S appears in the unconstrained problem\n",
        "        # S: (d, d) matrix, or S = np.concatenate(tS), with tS = [S1,..,S_m], so S is (d * n, d)\n",
        "        n, d = X.shape\n",
        "        q = compute_q(p)\n",
        "\n",
        "        #print(\"who is X\", X)\n",
        "        #print(\"who is y\", y)\n",
        "        #print(\"who is S\", S)\n",
        "        #print(\"who is q in AdversarialTraining: \", q)\n",
        "        #Formulate problem\n",
        "        param = cp.Variable(d)\n",
        "        #print(\"shape param \", param.shape)\n",
        "        #print(\"dim \", n)\n",
        "        print(\"X \", n,\" \", d)\n",
        "        print(\"y shape\", y.shape)\n",
        "        #print(\"S_dict \", S_dict)\n",
        "        #print(\"S in adv training\", S)\n",
        "        print(\"nm \", d*n)\n",
        "        S_dts = S_dict['S_dts']\n",
        "        S_mis = S_dict['S_mis']\n",
        "        adv_radius_times_scale_dts = cp.Parameter(name='adv_radius_times_dts', nonneg=True)\n",
        "        adv_radius_times_scale_mis = cp.Parameter(name='adv_radius_times_mis', nonneg=True)\n",
        "        #scale_dts = cp.Parameter(name='scale_dts', nonneg=True)\n",
        "        #scale_mis = cp.Parameter(name='scale_mis', nonneg=True)\n",
        "        print(\"S_mis in Adbvt training \", S_mis)\n",
        "        #if np.sum(S_mis * S_mis) == 0:\n",
        "        if np.all(S_dict['S_mis'] == 0):\n",
        "          print(\"no missing part\")\n",
        "          S = S_dts * adv_radius_times_scale_dts\n",
        "        else:  # S_mis.shape == (n, d, d):\n",
        "          S_dts_tiled = np.concatenate([S_dts] * n)\n",
        "          S_mis_conc = np.concatenate(S_mis)\n",
        "          #np.concatenate([yyy] * 2)\n",
        "          S = S_dts_tiled * adv_radius_times_scale_dts + S_mis_conc * adv_radius_times_scale_mis\n",
        "          print(\"S type \", type(S))\n",
        "          #S = np.concatenate(S)\n",
        "          print(\"S is a tensor, concatenated\")\n",
        "          print(\"final S after conc \\n\", S)\n",
        "\n",
        "        if S.shape == (d, d):\n",
        "          print(\"one matrix in input, S.shape = (n, n)\")\n",
        "          partial = S @ param  # should be (m * n,)\n",
        "          param_norm = cp.pnorm(partial, p=q)\n",
        "        elif S.shape == (d * n, d):  # should be a stack of matrices\n",
        "          print(\"multiple matrices in input, S conc\")\n",
        "          partial = S @ param  # should be (m * n,)\n",
        "          partial = cp.reshape(partial, (n, d), order='C')\n",
        "          param_norm = cp.pnorm(partial, p=q, axis=1)\n",
        "        else:\n",
        "          print(\"--------> ERROR: NO MATRIX S FOUND IN ADVERSARIAL TRAINING\")\n",
        "        #elif S.shape == (m , n):  # stack of diagonal matrices\n",
        "        #  print(\"multiple matrices in input, S_i diag\")\n",
        "          #S_cvx = cp.Constant(S)\n",
        "        #  partial = cp.multiply(cp.Parameter(S), param)\n",
        "        #  param_norm = cp.pnorm(partial, p=q, axis=1)\n",
        "        abs_error = cp.abs(X @ param - y)\n",
        "        adv_loss = 1 / n * cp.sum((abs_error + param_norm) ** 2)\n",
        "        prob = cp.Problem(cp.Minimize(adv_loss))\n",
        "        self.prob = prob\n",
        "        self.adv_radius_times_scale_dts = adv_radius_times_scale_dts\n",
        "        self.adv_radius_times_scale_mis = adv_radius_times_scale_mis\n",
        "        #self.scale_dts = scale_dts\n",
        "        #self.scale_mis = scale_mis\n",
        "        self.param = param\n",
        "        self.warm_start = False\n",
        "\n",
        "\n",
        "    def __call__(self, dict_hyper_p, **kwargs):\n",
        "        try:\n",
        "            #print(\"dic thyper p \", dict_hyper_p)\n",
        "            self.adv_radius_times_scale_dts.value = dict_hyper_p['adv_radius_times_dts']\n",
        "            self.adv_radius_times_scale_mis.value = dict_hyper_p['adv_radius_times_mis']\n",
        "            #self.scale_dts.value = dict_hyper_p['scale_dts\n",
        "            #self.scale_mis.value = dict_hyper_p['scale_mis']\n",
        "            self.prob.solve(warm_start=self.warm_start, solver=cp.CLARABEL, max_iter=10000, **kwargs)\n",
        "            v = self.param.value\n",
        "        except Exception as e:\n",
        "          print(\"------------------> Error occurred:\")\n",
        "          traceback.print_exc()\n",
        "          v = np.zeros(self.param.shape)\n",
        "        #except:\n",
        "        #    print(\"----------------------> you are in except\")\n",
        "        #    v = np.zeros(self.param.shape)\n",
        "        return v\n",
        "\n",
        "'''\n",
        "    def __call__(self, adv_radius, **kwargs):\n",
        "        try:\n",
        "            self.adv_radius.value = adv_radius\n",
        "            self.prob.solve(warm_start=self.warm_start, solver=cp.CLARABEL, max_iter=10000, **kwargs)\n",
        "            v = self.param.value\n",
        "        except Exception as e:\n",
        "          print(\"------------------> Error occurred:\")\n",
        "          traceback.print_exc()\n",
        "          v = np.zeros(self.param.shape)\n",
        "        #except:\n",
        "        #    print(\"----------------------> you are in except\")\n",
        "        #    v = np.zeros(self.param.shape)\n",
        "        return v\n",
        "'''\n",
        "\n",
        "\n",
        "def get_lasso_path(X, y, eps_lasso=1e-5):\n",
        "    alphas, coefs, _ = lasso_path(X, y, eps=eps_lasso)\n",
        "    coefs= np.concatenate([np.zeros([X.shape[1], 1]), coefs], axis=1)\n",
        "    alphas = np.concatenate([1e2 * np.ones([1]), alphas], axis=0)\n",
        "    return alphas, coefs, []\n",
        "\n",
        "# dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'eps_adv_rad_times_delta_dts': 1e-4 'eps_adv_rad_times_delta_dts': 1e-4}}\n",
        "def get_path(X, y, estimator, S_dict): #eps_amax=1e-4, eps_dts_max=1e-3, eps_mis_max=1e-3, n_alphas=100, n_deltas_dts=2, n_deltas_mis=3):\n",
        "    _, m = X.shape\n",
        "\n",
        "    if S_dict['algo_superv_learn'] == 'adv':\n",
        "      #n_a_dts = S_dict['n_a_dts']\n",
        "      #a_d_dts_max = S_dict['adv_rad_times_delta_dts_max']\n",
        "      #a_d_dts_min = a_d_dts_max * S_dict['eps_adv_rad_times_delta_dts']\n",
        "\n",
        "      if np.all(S_dict['S_dts'] == 0):\n",
        "        n_a_dts, a_d_dts_max, a_d_dts_min = 1, 0, 0\n",
        "      else:\n",
        "        n_a_dts = S_dict['n_a_dts']\n",
        "        a_d_dts_max = S_dict['adv_rad_times_delta_dts_max']\n",
        "        a_d_dts_min = a_d_dts_max * S_dict['eps_adv_rad_times_delta_dts']\n",
        "\n",
        "      if np.all(S_dict['S_mis'] == 0):\n",
        "        n_a_mis, a_d_mis_max, a_d_mis_min = 1, 0, 0\n",
        "      else:\n",
        "        n_a_mis = S_dict['n_a_mis']\n",
        "        a_d_mis_max = S_dict['adv_rad_times_delta_mis_max']\n",
        "        a_d_mis_min = a_d_mis_max * S_dict['eps_adv_rad_times_delta_mis']\n",
        "\n",
        "\n",
        "      if a_d_dts_max < 0 or a_d_mis_max < 0 or n_a_dts < 1 or n_a_mis <1:\n",
        "        print(\"WARNING: some bad values for the grid of cross validation, the number of grid point should be strictly potive, the radius strictly positive\")\n",
        "      alphas_dts = np.logspace(np.log10(a_d_dts_min), np.log10(a_d_dts_max), n_a_dts) if a_d_dts_max > 0 else np.zeros(1)\n",
        "      alphas_mis = np.logspace(np.log10(a_d_mis_min), np.log10(a_d_mis_max), n_a_mis) if a_d_mis_max > 0 else np.zeros(1)\n",
        "      #alphas_dts = np.logspace(np.log10(a_d_dts_min), np.log10(a_d_dts_max), n_a_dts)\n",
        "      #alphas_mis = np.logspace(np.log10(a_d_mis_min), np.log10(a_d_mis_max), n_a_mis)\n",
        "      print(\"dts deltas \", alphas_dts)\n",
        "      print(\"mis deltas \", alphas_mis)\n",
        "      #hyper_p = {'scale_dts': dts_deltas, 'scale_mis': mis_deltas}\n",
        "      hyper_p_ret_ = []\n",
        "      coefs_ = []\n",
        "      for a_mis_value in tqdm.tqdm(alphas_mis):\n",
        "        for a_dts_value in tqdm.tqdm(alphas_dts):\n",
        "            #tuple_key = (scale_dts_value, scale_mis_value)\n",
        "            #coefs_ = []\n",
        "            #for a in tqdm.tqdm(alphas):\n",
        "              #dict_hyper_p_values = {'adv_radius': a, 'scale_dts': scale_dts_value, 'scale_mis': scale_mis_value}\n",
        "              dict_hyper_p_values = {'adv_radius_times_dts': a_dts_value, 'adv_radius_times_mis': a_mis_value}\n",
        "              #print(\"dict hyper in get path \", dict_hyper_p_values)\n",
        "              coefs = estimator(X, y, dict_hyper_p_values)\n",
        "              #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "              coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "              hyper_p_ret_.append([a_dts_value, a_mis_value])\n",
        "            #res[tuple_key] = np.stack((coefs_)).T\n",
        "    elif S_dict['algo_superv_learn'] == 'ridge':\n",
        "      n_a_rid = S_dict['n_a_rid']\n",
        "      a_rid_max = S_dict['alpha_ridge_reg_max']\n",
        "      a_rid_min = a_rid_max * S_dict['eps_alpha_ridge_reg']\n",
        "      alphas_rid = np.logspace(np.log10(a_rid_min), np.log10(a_rid_max), n_a_rid) if a_rid_max > 0 else np.zeros(1)\n",
        "      print(\"rid alphas \", alphas_rid)\n",
        "      hyper_p_ret_ = []\n",
        "      coefs_ = []\n",
        "      print(\"S_dts_inv in get path, ridge regression \\n\", S_dict['S_dts'])\n",
        "      S_dts_inv = np.linalg.inv(S_dict['S_dts'])  # (d, d)\n",
        "      print(\"S_dts_inv in get path, ridge regression \\n\", S_dts_inv)\n",
        "      for a_rid in tqdm.tqdm(alphas_rid):\n",
        "            #dict_hyper_p_values = {'adv_radius_times_dts': a_dts_value, 'adv_radius_times_mis': a_mis_value}\n",
        "            #print(\"dict hyper in get path \", dict_hyper_p_values)\n",
        "            coefs = estimator(X @ S_dts_inv, y, a_rid)\n",
        "            coefs = S_dts_inv @ coefs\n",
        "            #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "            coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "            hyper_p_ret_.append([a_rid, 0])  #([a_dts_value, a_mis_value])\n",
        "\n",
        "\n",
        "    '''\n",
        "    for scale_dts_value in tqdm.tqdm(dts_deltas):\n",
        "        for scale_mis_value in tqdm.tqdm(mis_deltas):\n",
        "          #tuple_key = (scale_dts_value, scale_mis_value)\n",
        "          #coefs_ = []\n",
        "          for a in tqdm.tqdm(alphas):\n",
        "              #dict_hyper_p_values = {'adv_radius': a, 'scale_dts': scale_dts_value, 'scale_mis': scale_mis_value}\n",
        "              dict_hyper_p_values = {'adv_radius_times_scale_dts': a * scale_dts_value, 'adv_radius_times_scale_mis': a * scale_mis_value}\n",
        "              coefs = estimator(X, y, dict_hyper_p_values)\n",
        "              #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "              coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "              hyper_p_ret_.append([a, scale_dts_value, scale_mis_value])\n",
        "          #res[tuple_key] = np.stack((coefs_)).T\n",
        "    '''\n",
        "    return np.stack((hyper_p_ret_)).T, np.stack((coefs_)).T\n",
        "\n",
        "#dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg': 1,\n",
        "#                             'eps_adv_rad_times_delta_dts': 1e-4, 'eps_adv_rad_times_delta_mis': 1e-4, 'eps_alpha_ridge_reg': 1e-4,\n",
        "#                             'n_a_dts': 25, 'n_a_mis':4, 'n_a_rid': 25}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "def get_path(X, y, estimator, amax, eps=1e-5, n_alphas=200):\n",
        "    _, m = X.shape\n",
        "    amin = eps * amax\n",
        "    alphas = np.logspace(np.log10(amin), np.log10(amax), n_alphas)\n",
        "    coefs_ = []\n",
        "    for a in tqdm.tqdm(alphas):\n",
        "        coefs = estimator(X, y, a)\n",
        "        #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "        coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "    return alphas, np.stack((coefs_)).T\n",
        "'''\n",
        "\n",
        "\n",
        "def plot_coefs(alphas, coefs, ax):\n",
        "    #print(\"you are printing coefs in function of 1/alphas\")\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    #l1norm = np.abs(coefs).sum(axis=0)\n",
        "    ax.set_xlabel(\"1/alphas\")\n",
        "    ax.set_ylabel(\"coef\")\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.semilogx(1/alphas, coef_l, c=c)\n",
        "        #ax.semilogx(1/alphas, l1norm, c=c)\n",
        "        #ax.plot(1/alphas, coef_l, c=c)\n",
        "\n",
        "\n",
        "def plot_coefs_l1norm(coefs, ax):\n",
        "    #print(\"you are printing coeff in function of l1 norm\")\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    #l1norm = np.abs(coefs).mean(axis=0)\n",
        "    l1norm = np.abs(coefs).sum(axis=0)\n",
        "    #print(\"coef \", coefs)\n",
        "    #print(\"l1norm \", l1norm)\n",
        "    ax.set_xlabel(\"l1norm\")\n",
        "    ax.set_ylabel(\"coef\")\n",
        "\n",
        "\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.plot(l1norm, coef_l, c=c)\n",
        "\n",
        "\n",
        "def train_and_plot(X, y, S_dict, list_ax):\n",
        "\n",
        "    if S_dict['algo_superv_learn'] == 'adv':\n",
        "      linfadvtrain = AdversarialTraining(X, y, S_dict, p=np.inf)\n",
        "      estimator = lambda X, y, dic_h:  linfadvtrain(dict_hyper_p=dic_h)\n",
        "      hyper_p, coefs_advtrain_linf  = get_path(X, y, estimator, S_dict)\n",
        "    elif S_dict['algo_superv_learn'] == 'ridge':\n",
        "      estimator = lambda XX, yy, rad: ridge_regression(XX, yy, alpha=rad, return_intercept=False)#, random_state=0)\n",
        "      hyper_p, coefs_advtrain_linf  = get_path(X, y, estimator, S_dict)\n",
        "      #estimator = lambda X, y, a: linear_model.Ridge(alpha=a).fit(X, y).coef_\n",
        "    #print(\"hyper_p used\\n \", hyper_p)\n",
        "    if len(list_ax) > 0:\n",
        "      plot_coefs_l1norm(coefs_advtrain_linf, list_ax[0])\n",
        "      plot_coefs(alphas_adv, coefs_advtrain_linf, list_ax[1])\n",
        "    return hyper_p, coefs_advtrain_linf\n",
        "\n",
        "\n",
        "X = np.random.randn(100, 4) #rng.randn(100, 4)\n",
        "\n",
        "y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * np.random.randn(100)\n",
        "\n",
        "alphas = [0.00001, 0.001, 0.1, 1]\n",
        "estim = lambda XX, yy, rad: ridge_regression(XX, yy, alpha=rad, return_intercept=True, random_state=0)\n",
        "for a in alphas:\n",
        "  coef, intercept = estim(X, y, a)\n",
        "  print(\"coef : \", coef)\n",
        "  print(\"intercpt \", intercept)\n",
        "  coef, intercept = ridge_regression(X, y, alpha=a, return_intercept=True, random_state=0)\n",
        "  print(\"coef : \", coef)\n",
        "  print(\"intercpt \", intercept)\n",
        "\n",
        "\n",
        "'''\n",
        "def add_rectangles_old(x, y, box_width, box_height, ax):\n",
        "  r_c = (np.random.binomial(1, 1, size=x.size) == 1)  # 1 taken, 0 not taken\n",
        "  #print(r_c)\n",
        "\n",
        "  for xi, yi in zip(x[r_c], y[r_c]):\n",
        "      rect = patches.Rectangle(\n",
        "        (xi-box_width/2, yi-box_height/2),\n",
        "        box_width, box_height,\n",
        "        linewidth=1, edgecolor='r', facecolor='none'\n",
        "      )\n",
        "      ax.add_patch(rect)\n",
        "'''\n",
        "\n",
        "def add_rectangles(x, y, S, ax):\n",
        "  r_c = (np.random.binomial(1, 1, size=x.size) == 1)  # 1 taken, 0 not taken\n",
        "  #print(r_c)\n",
        "  d = S.shape[-1]\n",
        "  #S = S * 100\n",
        "  if S.ndim == 2 or S.shape == (1, d, d):\n",
        "    S = S.squeeze()\n",
        "    print(\"------------------------> who is S in add_rectangles\\n\", S)\n",
        "    box_width = S[0, 0]\n",
        "    box_height = S[1, 1]\n",
        "    for xi, yi in zip(x[r_c], y[r_c]):\n",
        "        rect = patches.Rectangle(\n",
        "          (xi-box_width/2, yi-box_height/2),\n",
        "          box_width, box_height,\n",
        "          linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "  else:  # S is something like (n, d, d)\n",
        "    #print(\"---------------> who is S in add_rectangles (mult imp)\\n\", S)\n",
        "    box_width = S[:, 0, 0]\n",
        "    box_height = S[:, 1, 1]\n",
        "    #print(\"bw\\n \", box_width)\n",
        "    #print(\"bh\\n \", box_height)\n",
        "    #print(\"------------------------------> boxes printed\")\n",
        "    for xi, yi, bw, bh in zip(x[r_c], y[r_c], box_width[r_c], box_height[r_c]):\n",
        "        #print(\"bw, bh \", bw, \",   \", bh)\n",
        "        rect = patches.Rectangle(\n",
        "          (xi-bw/2, yi-bh/2),\n",
        "          bw, bh, linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "\n",
        "\n",
        "print(\"end block\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation's block\n",
        "\n",
        "def clear_dataset(X, y, masks):\n",
        "  # remove observations full NaN\n",
        "  # X is an (n, d) matrix, y is a (n,) vector,\n",
        "  # masks is an (n, d) binary matrix associated to X. 1 missing, 0 seen\n",
        "  M = np.sum(1 - masks, axis=1) > 0\n",
        "  print(\"X shape in clear data \", X.shape)\n",
        "  print(\"y shape in clear data \", y.shape)\n",
        "  print(\"M shape in clear data \", M.shape)\n",
        "  M_col = np.sum(1 - masks, axis=0) > 0  # True if in the column there is at least one seen component\n",
        "  if np.sum(M_col) < masks.shape[1]:\n",
        "    print(\"Careful, there is one column full of nan\")\n",
        "  return X[M, :][:, M_col], y[M], masks[M, :][:, M_col]\n",
        "\n",
        "\n",
        "def single_imputation(X_nan, impute_estimator):\n",
        "    ice = IterativeImputer(estimator=impute_estimator)\n",
        "    return ice.fit_transform(X_nan)\n",
        "\n",
        "\n",
        "def multiple_imputation(nbr_mi, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    res = np.zeros((nbr_mi, n, d))\n",
        "    for i in range(nbr_mi):\n",
        "       n_i = np.random.randint(0, 100000)\n",
        "       ice = IterativeImputer(random_state=n_i, max_iter=50, sample_posterior=True)\n",
        "       res[i, :, :] = ice.fit_transform(X_nan)\n",
        "       #print(\"fin res shape\", res.shape)\n",
        "       #if nbr_mi == 1:\n",
        "        #res = res[0, :, :]\n",
        "        #print(\"fin res shape\", res.shape)\n",
        "    return res\n",
        "\n",
        "\n",
        "def imputation_elliptic(mu, sigma, x, masks):\n",
        "  # mu, mean elliptical distribution (,d)\n",
        "  # sigma, cov matrix elliptical distribution (d, d)\n",
        "  # x: dataset (n, d)\n",
        "  # masks: mask data, 0 seen, 1 missing\n",
        "  n, d = x.shape\n",
        "  print(n, d)\n",
        "  x_imp = x.copy()\n",
        "  #print(\"x_imp clean\", x_imp)\n",
        "  for i in range(n):\n",
        "    if not (masks[i, :] == 0).all():  # if we have at least one missing component\n",
        "      #print(\"nbr : \", i)\n",
        "      x_c = x[i, :]\n",
        "      m_bool = (masks[i, :] == 0)  # True seen, False missing\n",
        "      sigma_aa_inv = np.linalg.inv(sigma[m_bool, :][:, m_bool])\n",
        "      sigma_ma = sigma[~m_bool, :][:, m_bool]\n",
        "      mu_cond = mu[~m_bool] + sigma_ma @ sigma_aa_inv @ (x_c[m_bool] - mu[m_bool])\n",
        "      x_imp[i, ~m_bool] = mu_cond\n",
        "  return x_imp\n",
        "\n",
        "\n",
        "def listwise_delection(X, masks):\n",
        "  # masks: 1 missing, 0 seen\n",
        "    M = np.sum(masks, axis=1) == 0  # zeros components are the one with full entries\n",
        "    ret = X[M, :] if X.ndim == 2 else X[M]\n",
        "    return ret\n"
      ],
      "metadata": {
        "id": "qyWskXpdOW9e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElCvHxBiO_2t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA7J67yAuQM8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.random.seed(42)\n",
        "\n",
        "#p_miss_2d = [0.2, 0.4, 0.4]\n",
        "#beta_2d = np.array([0.5, 2])  # ground truth\n",
        "\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.model_selection import train_test_split\n",
        "'''\n",
        "def generate_masks_2d(nbr_of_sample, p_missing):\n",
        "    # nbr_of_sample is the number of masks\n",
        "    # p_missing=[p00, p01, p10], where p00 is the probability of seeing both components,\n",
        "    # p10 is the probability of seeing the right component, p01 is the probability of seeing the left component\n",
        "    masks = np.zeros((nbr_of_sample, 2))\n",
        "    v = np.random.choice(a=3, size=nbr_of_sample, p=p_missing)\n",
        "    masks[v == 0, :] = np.array([0, 0])  # both seen\n",
        "    masks[v == 1, :] = np.array([0, 1])  # left seen\n",
        "    masks[v == 2, :] = np.array([1, 0])  # right seen\n",
        "    return masks\n",
        "'''\n",
        "\n",
        "def generate_masks(dictio_data):#nbr_of_sample, dim, p_missing):\n",
        "    # nbr_of_sample is the number of masks\n",
        "    # p_missing=[p00, p01, p10], where p00 is the probability of seeing both components,\n",
        "    # p10 is the probability of seeing the right component, p01 is the probability of seeing the left component\n",
        "    dim = len(dictio_data['beta_gt'][0])\n",
        "    nbr_of_sample = dictio_data['n_train'][-1]  # last one should be the biggest one\n",
        "    p_missing = dictio_data['p_miss'][0]\n",
        "    print(\"p_missing in generate mask \", p_missing)\n",
        "    if dim == 2:\n",
        "      if len(p_missing) < 3:\n",
        "        print(\"WARNING: p_missing should be a list with a length of 3 if the dimension is 2\")\n",
        "      masks = np.zeros((nbr_of_sample, 2))\n",
        "      v = np.random.choice(a=3, size=nbr_of_sample, p=p_missing)\n",
        "      masks[v == 0, :] = np.array([0, 0])  # both seen\n",
        "      masks[v == 1, :] = np.array([0, 1])  # left seen\n",
        "      masks[v == 2, :] = np.array([1, 0])  # right seen\n",
        "    else:\n",
        "      # in this branch, p_missing = [p1,.., pl],\n",
        "      masks = np.array([np.random.binomial(1, 1-pr, (nbr_of_sample, dim)) for pr in p_missing])\n",
        "      masks = np.cumsum(masks, axis=0)  # each round\n",
        "      masks[masks>1] = 1\n",
        "    return masks\n",
        "\n",
        "def best_predictor(X, coeff, y):\n",
        "  hat_y = (X @ coeff).T  # (n, d) @ (d, m) = (n, m)\n",
        "  r = hat_y - y  # residual\n",
        "  score = np.mean(r * r, axis=1)\n",
        "  print(\"scores:  \", score)\n",
        "  i_min = np.argmin(score)\n",
        "  return coeff[:, i_min], score[i_min]\n",
        "\n",
        "def best_idx_predictor(X, coeff, y):\n",
        "  hat_y = (X @ coeff).T  # (n, d) @ (d, m) = (n, m)\n",
        "  r = hat_y - y  # residual\n",
        "  #score = np.mean(r * r, axis=1)\n",
        "  score = np.mean(r * r, axis=1)\n",
        "  #print(\"score in best idx\", score)\n",
        "  i_min = np.argmin(score)\n",
        "  #### find the minimum value with a threshold, so we get bigger uncertainty set that are visible\n",
        "  min = np.min(score)\n",
        "  max = np.max(score)\n",
        "  score[ score < min + -1 ] = max\n",
        "  ####\n",
        "  #print(\"score after \", score)\n",
        "  i_min = np.argmin(score)\n",
        "  return i_min, score[i_min]\n",
        "\n",
        "\n",
        "\n",
        "def generate_X(data, dim):\n",
        "    if data == 'Gaussian':\n",
        "      def generator(n):\n",
        "        return np.random.randn(n, dim)\n",
        "    elif data == 'Uniform':\n",
        "      def generator(n):\n",
        "        return np.random.rand(n, dim)\n",
        "    elif data == 'moons':\n",
        "      def generator(n):\n",
        "        return make_moons(n, noise=0.1)[0]\n",
        "    elif data == 'circles':\n",
        "      def generator(n):\n",
        "        return make_circles(n, noise=0.1, factor=0.4)[0]\n",
        "    return generator\n"
      ],
      "metadata": {
        "id": "AN61ok0A_Mbv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jB0J9uh-dJBp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwSkM31ztfUZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment 2d with dataset generated externally\n",
        "\n",
        "def imputations(info, dict_obs_for_imp):  # X_nan, y):\n",
        "  # info contains the method and possible extra information\n",
        "  # X_nan is the dataset with nan in place of the missing components\n",
        "  # y is return as it is, unless the method require to change it, like in\n",
        "  # listwise deletion\n",
        "    #print(info)\n",
        "    X_nan = dict_obs_for_imp['X_nan']\n",
        "    y = dict_obs_for_imp['y_train']\n",
        "    mask_from_X_nan = np.isnan(X_nan).astype(int)\n",
        "    if info['imp_method'] == 'BR_si':  # Baeysian_Ridge_single_imputation\n",
        "        X = single_imputation(X_nan, BayesianRidge())\n",
        "    elif info['imp_method'] in  ['mi', 'mi_pure']:\n",
        "        X = multiple_imputation(info['mi_nbr'], X_nan)  # size (info['mi_nbr], n, d)\n",
        "    elif info['imp_method'] == 'l_d':  # listwise_deletion\n",
        "        #mask_from_X_nan = np.isnan(X_nan).astype(int)\n",
        "        X = listwise_delection(X_nan, mask_from_X_nan)\n",
        "        y = listwise_delection(y, mask_from_X_nan)\n",
        "        if len(X) == 0:  # no elements left, add an artificial element\n",
        "            X = np.zeros((1, X_nan.shape[-1]))\n",
        "            y = np.zeros(1)\n",
        "        mask_from_X_nan = np.zeros_like(X)\n",
        "    elif info['imp_method'] == 'oracle':\n",
        "        X = dict_obs_for_imp['X_train_masked'][0]\n",
        "        mask_from_X_nan = np.zeros_like(X)\n",
        "    else:\n",
        "      print(\"-------------------> ERROR: WRONG KEYWORD (in imputations)\")\n",
        "    return X, y, mask_from_X_nan\n",
        "\n",
        "\n",
        "def cov_strategy(info, dict_observations):\n",
        "    X_imputed = dict_observations['X_imputed']\n",
        "    X_nan = dict_observations['X_nan']\n",
        "    masks = dict_observations['masks_after_imputation']\n",
        "    print(np.sum(masks, axis=-1))\n",
        "    if info['cov_strategy'] == 'sd':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      #print(\"sd in cov strategy \", sd)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(sd)\n",
        "    elif info['cov_strategy'] == 'inv_sd':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(1 / sd)\n",
        "    elif info['cov_strategy'] == 'zero':\n",
        "      #sd = np.std(X_imputed, axis=0)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.zeros((X_imputed.shape[-1], X_imputed.shape[-1]))\n",
        "    elif info['cov_strategy'] == 'eye':\n",
        "      S = np.eye(X_imputed.shape[-1])\n",
        "    elif info['cov_strategy'] == 'threshold':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      sd[sd < info['threshold']] = info['threshold']\n",
        "      #S = np.diag(sd) The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(sd)\n",
        "    elif info['cov_strategy'] == 'std_nan':\n",
        "      if info['imp_method'] in ['oracle']:\n",
        "        print(\"DON'T USE std_nan with oracle and ld because you do not have any nan. Use sd\")\n",
        "      else:\n",
        "        std_columnwise = np.nanstd(X_nan, axis=0)\n",
        "        S = np.diag(std_columnwise)\n",
        "    elif info['imp_method'] in ['mi_pure', 'mi']:\n",
        "      if info['cov_strategy'] == 'std_mi':   # std of the imputed dataset, then the mean\n",
        "        std_vectors = np.std(X_imputed, axis=-2)  # shape: (m, d)\n",
        "        #print(\"std vectors \", std_vectors)\n",
        "        #s_within = np.mean(std_vectors, axis=0)  # within imputation variance  # shape : d\n",
        "        S = std_vectors[:, None, :] * np.eye(std_vectors.shape[-1])  # should be (m, d, d), with each diagonal the diagonals of std_vectors\n",
        "        #S = s_within\n",
        "        #S = np.diag(s_within)\n",
        "        print(\"final S.shape in cov strategy std_mi \", S.shape)\n",
        "      elif info['cov_strategy'] == 'RR':\n",
        "        #if info['mi_nbr'] == 1:\n",
        "        #  X_imputed = np.array([X_imputed])\n",
        "        # X shape = (m, n, d)\n",
        "        std_vectors = np.std(X_imputed, axis=-2)  # shape: (m, d)\n",
        "        #print(\"std vectors \", std_vectors)\n",
        "        s_within = np.mean(std_vectors, axis=0)  # within imputation variance  # shape : d\n",
        "        print(\"s_within \", s_within)\n",
        "        print(\"cov computed\")\n",
        "        #print(s_mean)\n",
        "        s_between = np.std(std_vectors, axis=0) # between imputation variance  # shape: d. That's already scaled because we are computing the std\n",
        "        print(\"s_between \", s_between)\n",
        "        S = np.diag(s_within + s_between * (1 + 1 / info['mi_nbr']))\n",
        "        print(\"final S in cov strategy RR \", S)\n",
        "        #mu = np.mean(X_imputed, axis=0)\n",
        "        #sigma = np.cov(X_imputed, rowvar=False)\n",
        "      elif info['cov_strategy'] == 'RR_scaled (to check)':\n",
        "        print(\"Rub Rule right scaled\")\n",
        "        #if info['mi_nbr'] == 1:\n",
        "        #  X_imputed = np.array([X_imputed])\n",
        "        # X shape = (m, n, d)\n",
        "        std_vectors = np.std(X_imputed, axis=-2) # shape: (m, d)\n",
        "        #print(\"std vectors \", std_vectors)\n",
        "        s_within = np.mean(std_vectors, axis=0)  # within imputation variance  # shape : d\n",
        "        print(\"s_within \", s_within)\n",
        "        print(\"cov computed\")\n",
        "        #print(s_mean)\n",
        "        s_between = np.std(std_vectors, axis=0) # between imputation variance  # shape: d\n",
        "        #s_between = np.sqrt(s_between)\n",
        "        print(\"s_between \", s_between)\n",
        "        S = np.diag(s_within + s_between * (1 + 1 / info['mi_nbr']))\n",
        "        #S = np.sqrt(S)\n",
        "        print(\"final S in cov strategy RR \", S)\n",
        "      #elif info['cov_strategy'] == 'cond_var':\n",
        "        # we have imputed [X1,..,X_m]\n",
        "        #s = np.std(X_imputed, axis=0)\n",
        "        #print(\"s\\n \", s)\n",
        "        #eye = np.array([np.eye(X_imputed.shape[-1])] * X_imputed.shape[-2])\n",
        "        #S = eye * s[:, None, :]\n",
        "        #S = np.concatenate(S, axis=0)\n",
        "        #print(\"S in cond variance \", S)\n",
        "    elif info['cov_strategy'] == 'lounici':\n",
        "      mu = np.nanmean(X_nan, axis=0)\n",
        "      print(\"means \", mu)\n",
        "      delta = 1 - np.mean(masks) # parameter missingness\n",
        "      print(\"delta \", delta)\n",
        "      X_0 = np.nan_to_num(X_nan - mu)  # check if this is correct\n",
        "      print(\"nbr obs\", X_0.shape[0])\n",
        "      S =  X_0.T @ X_0 / X_0.shape[0]\n",
        "      S = (1/delta - 1/(delta**2)) * np.diag(np.diag(S)) + 1/(delta**2) * S\n",
        "    else:\n",
        "      raise ValueError(\"-------------> ERROR: NO COVARIANCE METHOD HAS BEEN CHOSEN\")\n",
        "      #print(\"-------------> ERROR: NO COVARIANCE METHOD HAS BEEN CHOSEN\")\n",
        "      #S = np.diag(S)\n",
        "      #mu = np.mean(X_imputed, axis=0)\n",
        "      #sigma = np.cov(X_imputed, rowvar=False)\n",
        "    return S\n",
        "\n",
        "\n",
        "def cov_strategy_missing(info, dict_observations):\n",
        "    # undertainty that come from the imputed part. It is zero\n",
        "    X_imputed = dict_observations['X_imputed']\n",
        "    if info['imp_method'] in ['mi', 'mi_pure'] and 'cov_strategy_between' in info.keys():\n",
        "      m, n, d = X_imputed.shape\n",
        "      if info['cov_strategy_between'] == 'cond_var':\n",
        "        # we have imputed [X1,..,X_m], so shape (m, n, d)\n",
        "        s = np.std(X_imputed, axis=0)\n",
        "        s[s<1e-14] = 0  # set to zero values that are basically zero\n",
        "        #print(\"var \", s)\n",
        "        eye = np.array([np.eye(X_imputed.shape[-1])] * X_imputed.shape[-2])\n",
        "        S_mis = eye * s[:, None, :]\n",
        "        if info['post_imp'] == 'conc':\n",
        "          S_mis = np.tile(S_mis, (m, 1, 1))\n",
        "      elif info['cov_strategy_between'] == 'zero':\n",
        "        d = dict_observations['X_test'].shape[-1]\n",
        "        S_mis = np.zeros((d, d))\n",
        "    else:  # not using a mi method, so uncertainty on missing part should be zero\n",
        "      print(\"shape oject in cov strategy missing \", dict_observations['X_test'].shape[-1])\n",
        "      print(\"shape oject in cov strategy missing \", dict_observations['X_test'].shape)\n",
        "      d = dict_observations['X_test'].shape[-1]\n",
        "      S_mis = np.zeros((d, d))\n",
        "    return S_mis\n",
        "\n",
        "\n",
        "def post_imputation(info_imp, dict_dataset):\n",
        "  # X_imptued should be a matrix (n, d) or tensor (m, d, n) (in multiple imputations methods)\n",
        "    X_imputed = dict_dataset['X_imputed']\n",
        "    y_train = dict_dataset['y_from_X_imputed']\n",
        "    #print(\"info imp in post_imp\", info_imp)\n",
        "    print(\"shape X_imputed in post_imputation \", X_imputed.shape)\n",
        "    mask_train = dict_dataset['masks_after_imputation']\n",
        "    if 'post_imp' not in info_imp.keys():\n",
        "      X_train = X_imputed\n",
        "    elif info_imp['post_imp'] == 'mean':\n",
        "      #print(\"entered in pst_iputation, in mi_mean\")\n",
        "      X_train = np.mean(X_imputed, axis=0)\n",
        "    elif info_imp['post_imp'] == 'conc':\n",
        "      print(\"shape X_imputed \", X_imputed.shape)\n",
        "      X_train = np.concatenate(X_imputed)\n",
        "      y_train = np.tile(y_train, X_imputed.shape[0])\n",
        "    else:\n",
        "      X_train = X_imputed\n",
        "    return X_train, y_train, mask_train\n",
        "\n",
        "\n",
        "def generate_dataset(data, n_tot, dim, beta_gt, perc_test, p_miss, err):\n",
        "    print(data)\n",
        "    if data['data'] == 'Gaussian':\n",
        "      X_complete = np.random.randn(n_tot, dim)\n",
        "    elif data['data'] == 'Normal':\n",
        "      #print(\"you are here\")\n",
        "      if len(beta_gt) != len(data['mean']) or len(beta_gt) != data['cov'].shape[0]:\n",
        "        print(\"ERROR: DIMENSION MISSMATCH\")\n",
        "      X_complete = np.random.multivariate_normal(mean=data['mean'], cov=data['cov'], size=n_tot)\n",
        "    elif data['data'] == 'LogNormal':\n",
        "      if len(beta_gt) != len(data['mean']) or len(beta_gt) != data['cov'].shape[0]:\n",
        "        print(\"ERROR: DIMENSION MISSMATCH\")\n",
        "      X_complete = np.random.lognormal(mean=data['mean'], sigma=data['cov'], size=n_tot)\n",
        "    elif data['data'] == 'Uniform':\n",
        "      X_complete = np.random.rand(n_tot, dim) -0.5\n",
        "    elif data['data'] == 'Logistic':\n",
        "      X_complete = np.random.logistic(loc=0.0, scale=1.0, size=(n_tot, dim))\n",
        "    elif data['data'] == 'moons':\n",
        "      X_complete = make_moons(n_tot, noise=0.1)[0]\n",
        "    elif data['data'] == 'circles':\n",
        "      X_complete = make_circles(n_tot, noise=0.1, factor=0.4)[0]\n",
        "\n",
        "    if err['type'] == 'Gaussian_on_y':\n",
        "      #print(\"---> you have entered in GAUSSIAN ERROR \", \"scaling : \", err['scaling'])\n",
        "      error = np.random.randn(n_tot) * err['scaling']\n",
        "    elif err['type'] == 'Uniform_on_y':\n",
        "      error = (np.random.rand(n_tot)-0.5) * err['scaling']\n",
        "    elif err['type'] == 'Gaussian_on_X':\n",
        "      error = (np.random.randn(n_tot, dim) @ beta_gt) * err['scaling']  # error is of the form DX@beta_gt + error\n",
        "    elif err['type'] == 'Uniform_on_X':\n",
        "      error = ((np.random.rand(n_tot, dim)-0.5) @ beta_gt) * err['scaling']\n",
        "    #elif err['type'] == 'Gaussian':\n",
        "    #  error = np.random.randn(n_tot) * err['scaling']\n",
        "\n",
        "    print(X_complete.shape)\n",
        "\n",
        "    y_complete = X_complete @ beta_gt + error  #np.random.randn(n_tot) * err  # (np.random.rand(n_tot) - 0.5) * err\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_complete, y_complete, test_size=perc_test)\n",
        "    n_train = X_train.shape[0]\n",
        "    # masks_train = generate_masks_2d(n_train, p_miss)  # 1 missing, 0 observed\n",
        "    # masks_train = generate_masks_binomial(n_train, p_miss)  # 1 missing, 0 observed\n",
        "    #X_train, y_train, masks_train = clear_dataset(X_train, y_train, masks_train)\n",
        "    # M = np.sum(masks, axis=1)  # M[i] > 0 iff i has missing component\n",
        "    # dict_obs = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test, 'masks_train': masks_train}\n",
        "    dict_obs = {'X_train_masked': (X_train, []), 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}#, 'masks_train': masks_train}\n",
        "    return dict_obs\n",
        "\n",
        "\n",
        "def experiment_2d_ext_dataset(dict_obs, dict_imp, ax):\n",
        "    # dict_obs contains info on the observations, i.e. train, test, masks\n",
        "    # dict_imp contains info on the imputation an covariance methods used,\n",
        "    # dict_imp = {'imp_method': , 'cov_strategy': , .... }\n",
        "    # ax contains info for the plots\n",
        "\n",
        "    X_test = dict_obs['X_test']\n",
        "    y_test = dict_obs['y_test']\n",
        "    mask = dict_obs['X_train_masked'][1]\n",
        "\n",
        "    M = np.sum(mask, axis=1)  # M[i] > 0 iff i has missing component\n",
        "\n",
        "    X_nan_train = dict_obs['X_train_masked'][0].copy()\n",
        "    oracle_sd = np.std(X_nan_train, axis=0)\n",
        "    print(\"-------> ORACLE SD, std of the original dataset (with no missing)\", oracle_sd)\n",
        "    X_nan_train[mask == 1] = np.nan\n",
        "    #print(\"dict imp -----> \", dict_imp)\n",
        "    dict_obs = dict_obs | {'X_nan': X_nan_train} #, 'y_from_X_imputed': y_from_X_imputed, 'masks_after_imputation': mask_from_X_imputed}\n",
        "    if len(dict_obs['imp_ds'][dict_imp['imp_method']]) == 0:  # no previous imputation has been done\n",
        "      #results = imputations(dict_imp, X_nan_train, dict_obs['y_train'])\n",
        "      print(\"NO PREVIOUS IMPUTATION HAS BEEN DONE\")\n",
        "      results = imputations(dict_imp, dict_obs)\n",
        "      X_imputed, y_from_X_imputed, mask_from_X_imputed = results  # imputations(dict_imp, X_nan_train, dict_obs['y_train'])\n",
        "      dict_obs['imp_ds'][dict_imp['imp_method']].append(results)\n",
        "      print(\"crush test-------------------------------------------------> \", np.sum(X_imputed))\n",
        "    else:\n",
        "      print(\"A PREVIOUS IMPUTATION HAS BEEN DONE\")\n",
        "      X_imputed, y_from_X_imputed, mask_from_X_imputed = dict_obs['imp_ds'][dict_imp['imp_method']][0]\n",
        "      print(\"crush test-------------------------------------------------> \", np.sum(X_imputed))\n",
        "    #print(\"X_imputed \", X_imputed)\n",
        "    n_imputed, n_test = X_imputed.shape[-2], X_test.shape[-2]\n",
        "    #print(\"X_train\\n \", X_train)\n",
        "    M = np.sum(mask_from_X_imputed, axis=1)  # M[i] > 0 iff i has missing component\n",
        "\n",
        "    dict_obs = dict_obs | {'X_imputed': X_imputed, 'y_from_X_imputed': y_from_X_imputed, 'masks_after_imputation': mask_from_X_imputed}\n",
        "    #  print(dict_obs)\n",
        "    S_dataset = cov_strategy(dict_imp, dict_obs) #* dict_imp['multip_dataset']\n",
        "    print(\"S dataset \\n\", S_dataset)\n",
        "    #  dict_obs = dict_obs | {'cov_within': S_within}\n",
        "    S_missing = cov_strategy_missing(dict_imp, dict_obs)  #* dict_imp['multip_missing']\n",
        "    print(\"S missing shape\\n \", S_missing.shape)\n",
        "    print(\"S missing\\n \", S_missing)\n",
        "    if 'post_imp' in dict_obs.keys():\n",
        "      if dict_obs['post_imp'] == 'conc':\n",
        "        print(S_missing)\n",
        "    #  dict_obs = dict_obs | {'cov_between': S_between}\n",
        "    S_dict = {'S_dts': S_dataset, 'S_mis': S_missing} | dict_obs['info_algo'] | {'algo_superv_learn': dict_imp['algo_superv_learn']}  # , 'multipliers_dts': dict_imp['multip_dataset'], 'multipliers_mis': dict_imp['multip_missing']}\n",
        "    # dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'eps_adv_rad_times_delta_dts': 1e-4 'eps_adv_rad_times_delta_dts': 1e-4}}\n",
        "\n",
        "    #if True:  # check what to do of this part later\n",
        "      #S = S_dataset * dict_imp['multip_dataset'] + S_missing * dict_imp['multip_missing']\n",
        "      #if S.ndim == 2:\n",
        "      #  print(\"final S \\n\", S)\n",
        "\n",
        "\n",
        "    #print(\"matrices S \\n\", S)\n",
        "    #print(\"---....---....----....--> diag matrix: \", np.diag(S))\n",
        "\n",
        "    #if dict_imp['imp_method'] == 'mi':  # prepare the training set in case of multiple imputation\n",
        "    #  X_train = np.concatenate(X_train)  # X_train, if the method is mi, should be (mi_nbr, n, dim)\n",
        "    #  y_train = np.tile(y_train, reps=dict_imp['mi_nbr'])\n",
        "    #  mask_train = np.tile(mask_train, reps=(dict_imp['mi_nbr'], 1))\n",
        "    #  M = np.sum(mask_train, axis=1)\n",
        "    #print(\"final matrices (exp 2d ext run)\\n \", S)\n",
        "    X_train, y_train, mask_train = post_imputation(dict_imp, dict_obs)\n",
        "    n_train = X_train.shape[-2]\n",
        "    print(\"y_train length \", y_train.shape[0])\n",
        "    print(\"-------> size test: \", n_test, \" , size train: \", n_train, \"nbr_full_seen (train): \", np.sum(M == 0), \" nbr_at_least_one_miss : \", np.sum(M > 0))\n",
        "\n",
        "#    plt.tight_layout()\n",
        "    #S_between = S.copy()\n",
        "    if dict_imp['imp_method'] == 'mi' and dict_imp['cov_strategy'] == 'std_mi':  # run a standard multiple imputation procedure\n",
        "      best_coeff = np.zeros(X_train.shape[-1])\n",
        "      best_alpha = 0\n",
        "      min_score = 0\n",
        "      temporary_dictionary = copy.deepcopy(S_dict)\n",
        "      for i in range(dict_imp['mi_nbr']):\n",
        "        print(\"i  mi .-------------> \", i)\n",
        "        #dict_obs_i = {'X_imputed': X_train[i, :, :], 'X_nan': X_nan_train, 'masks': mask_train}\n",
        "        #dict_imp_new = {'imp_method': dict_imp['imp_method'], 'cov_strategy': dict_imp['cov_strategy_within']}\n",
        "        #S_within = cov_strategy(dict_imp_new, dict_obs_i)  # within the dataset\n",
        "        #print(\"S_within \", S_within)\n",
        "        #S = S_within[None, :, :] + S_between\n",
        "        #S = np.concatenate(S, axis=0)\n",
        "        #print(S)\n",
        "        #alphas_used, coeff_results = train_and_plot(X_train[i, :, :], y_train, S, [ax[1], ax[2]])\n",
        "        temporary_dictionary['S_dts'] = S_dict['S_dts'][i, :, :]\n",
        "        print(\"temporary dict \", temporary_dictionary)\n",
        "        hyper_p_used, coeff_results = train_and_plot(X_train[i, :, :], y_train, temporary_dictionary, [])\n",
        "        idx_best, min_score_partial = best_idx_predictor(X_test, coeff_results, y_test)\n",
        "        print(\"weee \", idx_best)\n",
        "        print(coeff_results.shape)\n",
        "        print(hyper_p_used.shape)\n",
        "        best_coeff_partial, _ = coeff_results[:, idx_best], hyper_p_used[:, idx_best]\n",
        "        print(\"best coeff partial \", best_coeff_partial)\n",
        "        best_coeff += best_coeff_partial\n",
        "        min_score += min_score_partial\n",
        "        #best_alpha += best_alpha_partial\n",
        "        if len(ax) > 0:\n",
        "          ax[0].scatter(X_train[i, M == 0, 0], X_train[i, M == 0, 1])\n",
        "          ax[0].scatter(X_train[i, M == 1, 0], X_train[i, M == 1, 1])\n",
        "          ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', n_s: ' + str(np.sum(M == 0)) + \" n_m: \" + str(np.sum(M > 0)))  # n_s = nbr seen, n_m = nbr missing\n",
        "          add_rectangles(X_train[i, :, 0], X_train[i, :, 1], S[0, 0] * best_alpha_partial, S[1, 1] * best_alpha_partial, ax[0])\n",
        "      best_coeff /= dict_imp['mi_nbr']\n",
        "      min_score /=  dict_imp['mi_nbr']\n",
        "      best_hyper_p = 0  # not important right now\n",
        "      best_alpha_delta_dts = 1  # not important right now\n",
        "      #best_alpha /= dict_imp['mi_nbr']\n",
        "    else:\n",
        "      #alphas_used, coeff_results = train_and_plot(X_train, y_train, S_dict, [ax[1], ax[2]])\n",
        "      hyper_p_used, coeff_results = train_and_plot(X_train, y_train, S_dict, [])\n",
        "      idx_best, min_score = best_idx_predictor(X_test, coeff_results, y_test)\n",
        "      #best_coeff, best_alpha = coeff_results[:, idx_best], alphas_used[idx_best]\n",
        "      #print(\"-----------------> shape hyper_p used \", hyper_p_used.shape)\n",
        "      best_coeff, best_hyper_p = coeff_results[:, idx_best], hyper_p_used[:, idx_best]\n",
        "      #print(\"hyper_p_used \", hyper_p_used.T)\n",
        "      #input()\n",
        "      #print(X_br_train[M == 0, 0])\n",
        "      best_alpha_delta_dts, best_alpha_delta_mis = best_hyper_p[0], best_hyper_p[1]\n",
        "#     print(\"best alpha ----> \", best_alpha_dts)\n",
        "      if len(ax) > 0:\n",
        "        ax[0].scatter(X_train[M == 0, 0], X_train[M == 0, 1])\n",
        "        ax[0].scatter(X_train[M == 1, 0], X_train[M == 1, 1])\n",
        "        #ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', n_s: ' + str(np.sum(M == 0)) + \" n_m: \" + str(np.sum(M > 0)))  # n_s = nbr seen, n_m = nbr missing\n",
        "        # 'multip_betw': 1, 'multip_with':1\n",
        "        ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', dts:'+str(dict_imp['multip_dataset']) + ', mis:' + str(dict_imp['multip_missing']) )  # n_s = nbr seen, n_m = nbr missing\n",
        "        S_plot = S_dict['S_dts'] * best_alpha_delta_dts + S_dict['S_mis'] * best_alpha_delta_mis\n",
        "        #print(\"S_plot \", S_plot)\n",
        "        add_rectangles(X_train[:, 0], X_train[:, 1], S_plot, ax[0])\n",
        "        ax[0].set_aspect('equal')  # equal proportion of the axis\n",
        "    #print(\"X_train \", X_train)\n",
        "    #print(\"y_train \", y_train)\n",
        "    #print(\"mask_train \", mask_train)\n",
        "    #print(\"M \", M)\n",
        "\n",
        "\n",
        "    print(\"X_test shape, \", X_test.shape, \",   y_test shape \", y_test.shape)\n",
        "    #print(\"X_test shape, \", X_test.shape)\n",
        "    print(\"---------------------------------> best idx \", idx_best, \" best hyperp [best_alpha_delta_dst, best_alpha_delta_mis]: \", best_hyper_p, \", min score \", min_score)\n",
        "    print(\"---------------------------------> best coeff \", best_coeff)\n",
        "    #input()\n",
        "    #print(\"best 1/alpha \", 1 / best_alpha)\n",
        "    #print(\"min score \", min_score)\n",
        "\n",
        "    #\n",
        "    #add_rectangles(X_train[:, 0], X_train[:, 1], S[0, 0] * best_alpha, S[1, 1] * best_alpha, ax[0])\n",
        "\n",
        "\n",
        "    # obsere that one day you shoul add the return of alpha_delta_mis also\n",
        "    return best_coeff, min_score, -np.log10(best_hyper_p)  # -np.log10((best_alpha_delta_dts + best_alpha_delta_mis)/2)\n",
        "\n"
      ],
      "metadata": {
        "id": "OhNXUBahJgBL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(dictio, methods_strategy):  # ---------------------> new\n",
        "  # dictio: dictionary of lists that contains the parameters of generate_dataset.\n",
        "  # Each list should have the same length\n",
        "  # methods_strategy = list of dictionary, each one of the form\n",
        "  # {'imp_method': .., 'cov_strategy':.., extra info}\n",
        "\n",
        "    l = len(dictio['data'])  # how many trials shall we do\n",
        "    m = len(methods_strategy)\n",
        "    nbr_iter = len(methods_strategy)\n",
        "    coeff_fin = np.zeros((nbr_iter, 2, l))\n",
        "    scores_fin = np.zeros((nbr_iter, l))\n",
        "\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(3 * l , 9 *l), num='advtrain_linf_')\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(6 * l , 9 *l), num='advtrain_linf_')\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(6 * l / 2, 9 *l / 2), num='advtrain_linf_')\n",
        "    print(dictio['plots'])\n",
        "    print(dictio['plots'][0])\n",
        "    nbr_ima = len(dictio['plots'][0])\n",
        "    if nbr_ima == 1:\n",
        "      #nbr_ima = 1\n",
        "      fig, ax = plt.subplots(nbr_ima * nbr_iter, l, figsize=(3 * l, 8/3 * m), squeeze=False)#, num='advtrain_linf_')\n",
        "    elif nbr_ima == 3:  # == 3, one day should be more general\n",
        "      #nbr_ima = 3\n",
        "      fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(3 * l, 8 * m), squeeze=False)#, num='advtrain_linf_')\n",
        "\n",
        "    res = {}\n",
        "    for info_imp_cov_dict in methods_strategy:\n",
        "      key_list = []\n",
        "      for value in info_imp_cov_dict.values():\n",
        "        print(value)\n",
        "        key_list.append(value)\n",
        "      key_tuple = tuple(key_list)\n",
        "      res[key_tuple] = {'best_coeff':[], 'l2_dist_best_coeff_gt':[], 'best_score':[], 'best_hyper_p':[], 'best_alpha_dts':[], 'best_alpha_mis':[]}\n",
        "      #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])] = {'best_coeff':[], 'l2_dist_best_coeff_gt':[], 'best_score':[], 'best_alpha':[]}\n",
        "\n",
        "    if dictio['generation'] == 'fixed':  # use this if you want to fix the generated data, and not change at every iteartion\n",
        "      dictio_obser_fixed  = generate_dataset(data=dictio['data'][0],\n",
        "                                    n_tot=dictio['n_tot'][-1],  # last one should be the biggest\n",
        "                                    dim=dictio['dim'][0],\n",
        "                                    beta_gt=dictio['beta_gt'][0],\n",
        "                                    perc_test=dictio['perc_test'][-1],\n",
        "                                    p_miss=dictio['p_miss'][0],\n",
        "                                    err=dictio['err'][0]\n",
        "                                             )  # return {'X_train_masked':(X_train, mask_train) , 'X_test':.., 'y_train':, 'y_test'}\n",
        "      #mask_no_both_seen = generate_masks_2d(dictio['n_train'][0], [0, 0.5, 0.5]) # generate a mask where there are no entries both seen. The idea then will be to consider percentage of this mask seen\n",
        "      full_masks = generate_masks(dictio)\n",
        "    dictio_obser_fixed_copy = copy.deepcopy(dictio_obser_fixed)\n",
        "\n",
        "    for i in range(l):\n",
        "      print(\"---------------------------------------------------------------------------------------------------------------------------> iteration \", i)\n",
        "      #  dict_obs = {'X_train_masked': (X_train, masks_train), 'X_test': ....., 'y_train': ....., 'y_test': ....}\n",
        "      dict_obser_partial = generate_dataset(data=dictio['data'][i],\n",
        "                                    n_tot=dictio['n_tot'][i],\n",
        "                                    dim=dictio['dim'][i],\n",
        "                                    beta_gt=dictio['beta_gt'][i],\n",
        "                                    perc_test=dictio['perc_test'][i],\n",
        "                                    p_miss=dictio['p_miss'][i],\n",
        "                                    err=dictio['err'][i])\n",
        "      if dictio['generation'] == 'fixed':\n",
        "        dict_obser = dictio_obser_fixed\n",
        "        if len(dictio['beta_gt'][0]) == 2:\n",
        "          #mask_partial = dict_obser_partial['X_train_masked'][1]\n",
        "          p_i = dictio['p_miss'][i][0]  # probability of seen both component at round i\n",
        "          n_train = full_masks.shape[0]\n",
        "          mask_partial = full_masks.copy()\n",
        "          mask_partial[0:int(n_train * p_i), :] = 0\n",
        "          tuple_partial = (dictio_obser_fixed['X_train_masked'][0], mask_partial)\n",
        "          dict_obser['X_train_masked'] = tuple_partial\n",
        "        else:\n",
        "          #print(\"size in run experiment\", dictio_obser_fixed_copy['X_train_masked'][0].shape, \"wee \", dictio_obser_fixed['y_train'].shape)\n",
        "          ## we use the next line of code with dictio_obser_fixed_copy because we need to test the mask with the original dataset, otherwise we get size error (the dataset change if an observation get fully hidden)\n",
        "          n_train = dictio['n_train']\n",
        "          print(\"n_tot_fll \", n_train, \",  \", n_train[i])\n",
        "          #print(dictio_obser_fixed_copy['X_train_masked'][0][0:n_train[i], :].shape)\n",
        "          #print(dictio_obser_fixed_copy['X_train_masked'][0].shape)\n",
        "          X_train_cleaned, y_train_cleaned, masks_train_cleaned = clear_dataset(dictio_obser_fixed_copy['X_train_masked'][0][0:n_train[i], :], dictio_obser_fixed_copy['y_train'][0:n_train[i]], full_masks[i][0:n_train[i], :])\n",
        "          print(\"shapes X_train cleaned, mask train cleaned, y train cleaned\")\n",
        "          print(X_train_cleaned.shape)\n",
        "          print(masks_train_cleaned.shape)\n",
        "          print(y_train_cleaned.shape)\n",
        "          #tuple_partial = (dictio_obser_fixed['X_train_masked'][0], full_masks[i])\n",
        "          print(\"full masks in run experiment \", full_masks[i])\n",
        "          dict_obser['X_train_masked'] = (X_train_cleaned, masks_train_cleaned)\n",
        "          dict_obser['y_train'] = y_train_cleaned\n",
        "      else:\n",
        "        dict_obser = dict_obser_partial\n",
        "\n",
        "      #print(\"dict obser \", dict_obser)\n",
        "      print(\"info algo in run experiments \", dictio['info_algo'])\n",
        "      dict_obser = dict_obser | {'imp_ds':{'BR_si':[], 'l_d':[], 'oracle':[], 'mi':[]}} | {'info_algo': dictio['info_algo']}  # add an entry for imputed dataset, and info for algorithm\n",
        "      print(\"ciaoooooo dict obser in run experiments \\n \", dict_obser)\n",
        "      for idx, info_imp_cov_dict in enumerate(methods_strategy):\n",
        "        print(\"----------------------------------------------> new method tested: \", info_imp_cov_dict)\n",
        "        if nbr_ima > 0:\n",
        "          coeff_round, score_round, hyper_p_round = experiment_2d_ext_dataset(dict_obser, info_imp_cov_dict, ax[(idx * nbr_ima):((idx+1) * nbr_ima), i])\n",
        "        else:  # == 0\n",
        "          coeff_round, score_round, hyper_p_round = experiment_2d_ext_dataset(dict_obser, info_imp_cov_dict, [])\n",
        "        r = coeff_round - dictio['beta_gt'][i]\n",
        "        l2_dist = np.linalg.norm(r)\n",
        "        key_list = []\n",
        "        for value in info_imp_cov_dict.values():\n",
        "          print(value)\n",
        "          key_list.append(value)\n",
        "        key_tuple = tuple(key_list)\n",
        "        res[key_tuple]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "        res[key_tuple]['best_coeff'].append(coeff_round)\n",
        "        res[key_tuple]['best_score'].append(score_round)\n",
        "        res[key_tuple]['best_hyper_p'].append(hyper_p_round)  # both hyperparameters\n",
        "        res[key_tuple]['best_alpha_dts'].append(hyper_p_round[0])  # one of the hyperparameter\n",
        "        res[key_tuple]['best_alpha_mis'].append(hyper_p_round[1])  # the other hyperparameter\n",
        "\n",
        "        #res[key_tuple]['best_alpha'].append(alpha_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_coeff'].append(coeff_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_score'].append(score_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_alpha'].append(alpha_round)\n",
        "    plt.tight_layout()\n",
        "    return res\n",
        "\n",
        "\n",
        "def plot_res(x_axis_info, res, extra_info):\n",
        "  x_axis = x_axis_info['vector']\n",
        "  print(\"x_axis for print in plot_res----> \", x_axis)\n",
        "  l = len(x_axis)\n",
        "  lb = extra_info['what_to_plot']\n",
        "  nbr_plot = len(lb)\n",
        "  fig_res, ax_res = plt.subplots(1, nbr_plot,\n",
        "                                 figsize=(45 * (nbr_plot / 3 ), 7.5))  # , num='advtrain_linf_res')\n",
        "  positions = range(l)\n",
        "\n",
        "  for key, values in res.items():\n",
        "    print(\"key in plot_res\", key, \": values\\n\", values)\n",
        "    #print(\"values \", values)\n",
        "  #print(\"res\\n \", res)\n",
        "\n",
        "  ch = ['o', 'x', '+', '*', '<', '>', 'p', 'D', 'd', 'v']\n",
        "  # lb = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha']\n",
        "  for i in range(nbr_plot):\n",
        "    for idx, (key, dictio) in enumerate(res.items()):\n",
        "      #print(dictio)\n",
        "      print(\"lb[i] in plot_res \", lb[i], \"  \", dictio[lb[i]])\n",
        "      print(\"key: \", key)\n",
        "      ax_res[i].plot(positions, dictio[lb[i]], marker=ch[idx], label=str(key), color=key[-1])  # the marker is linked to the key (= method), different key correspond to different marker\n",
        "      #ax_res[1].plot(positions, dictio[lb[idx]], marker=ch[idx], label=str(key))\n",
        "      #ax_res[2].plot(positions, -np.log(dictio['best_alpha']), marker=ch[idx], label=str(key))\n",
        "      #ax_res[0].xticks(positions, n_tot)  # Set custom labels for the x-axis\n",
        "    ax_res[i].set_xticks(positions)         # Set the tick positions\n",
        "    ax_res[i].set_xticklabels(x_axis)        # Set the labels at those positions\n",
        "    ax_res[i].set_xlabel(x_axis_info['name'])\n",
        "    #ax_res[i].legend(loc='upper center', bbox_to_anchor=(1, 1))\n",
        "    ax_res[i].legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0., fontsize=20,)\n",
        "  ax_res[0].set_ylabel(\"||hat_Beta - Beta^*||_2\", fontweight='bold', fontsize=16)\n",
        "  ax_res[1].set_ylabel(\"||hat_y - y||_2^2 / n_test\", fontweight='bold', fontsize=16)\n",
        "  dict_err = extra_info['err'][0]\n",
        "  #size_train = extra_info['n_tot'][0]\n",
        "  #ax_res[0].set_title(\"\")\n",
        "  n_test = extra_info['n_test'][0]\n",
        "  #ax_res[1].set_title(\"err: \" + dict_err['type'] + \", scale: \" + str(dict_err['scaling'])  + \", n_test: \" + str(n_test))\n",
        "  #ax_res[0].set_title('n_test: ' + str(n_test) + extra_info['title_infer_error'])\n",
        "  #ax_res[1].set_title('n_test: ' + str(n_test) + extra_info['title_test_error'])\n",
        "  ax_res[0].set_title(extra_info['title_infer_error'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[1].set_title(extra_info['title_test_error'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[2].set_title(extra_info['title_dts_radius'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[3].set_title(extra_info['title_mis_radius'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[2].set_ylabel(\"-log10(alpha)\", fontsize=16, fontweight='bold')\n",
        "  ax_res[3].set_ylabel(\"-log10(alpha)\", fontsize=16, fontweight='bold')\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "def make_dictionary_data(nbr_experiments, n_train, n_test, data, beta_gt, p_miss, err_vector, plots):\n",
        "  # make a dictionary where each element is a list of nbr_experiments element made by the other element of the function\n",
        "  if isinstance(n_train, int):  # in case n_train is just a number\n",
        "    n_train = [n_train] * nbr_experiments\n",
        "  else:  # should be a list of integer\n",
        "    print(\"change nbr_experiments to match the size of n_train\")\n",
        "    nbr_experiments = len(n_train)\n",
        "  if isinstance(n_test, int):  # in case n_test is just a number\n",
        "    n_test = [n_test] * nbr_experiments\n",
        "  n_tot = [x + y for x, y in zip(n_train, n_test)]\n",
        "  perc_test = [x / (x+y) for x, y in zip(n_test, n_train)]\n",
        "  dim = beta_gt.size\n",
        "\n",
        "  list_errors = []\n",
        "  for i in range(nbr_experiments):\n",
        "    err_dic_app = {'type': err_vector[0], 'scaling': err_vector[1][i]}\n",
        "    list_errors.append(err_dic_app)\n",
        "\n",
        "  dictio = {'data':[data] * nbr_experiments,\n",
        "        'n_tot': n_tot,\n",
        "        'n_train': n_train,\n",
        "        'n_test': n_test,\n",
        "        'dim': [dim] * nbr_experiments,\n",
        "        'beta_gt': [beta_gt] * nbr_experiments,\n",
        "        'perc_test': perc_test,\n",
        "        #'p_miss': [p_miss] * nbr_experiments,\n",
        "        'err': list_errors,\n",
        "        'plots': [plots] * nbr_experiments\n",
        "        }\n",
        "  dictio['p_miss'] = p_miss\n",
        "\n",
        "  return dictio\n",
        "\n",
        "def make_probabilities(list_prob):\n",
        "  l = []\n",
        "  for x in list_prob:\n",
        "    l.append([x, 0.5 - x/2, 0.5 - x/2])\n",
        "  return l\n",
        "\n",
        "def make_info_axis(vector, name):\n",
        "  if name == 'train':\n",
        "    dictio = {'name': 'size train set', 'vector': vector}\n",
        "  elif name == 'p_seen':\n",
        "    dictio = {'name': 'probability seen full entries', 'vector': vector}\n",
        "  elif name == 'error':\n",
        "    dictio = {'name': 'error', 'vector': vector}\n",
        "  else:\n",
        "    print(\"wrong info_axis\")\n",
        "  return dictio\n",
        "\n",
        "def make_dictionary_method(list_meth):\n",
        "  # make a dictionary where each element is a list of nbr_experiments element made by the other element of the function\n",
        "  list_dictio=[]\n",
        "  list_key = ['imp_method', 'cov_strategy', 'mi_nbr']\n",
        "  for meth in list_meth:\n",
        "    dictio_imp = {}\n",
        "    for i in range(len(meth)):\n",
        "      dictio_imp[list_key[i]] = meth[i] #= {list_key[i]: meth[i]}\n",
        "      #print(dictio_imp)\n",
        "    list_dictio.append(dictio_imp)\n",
        "  return list_dictio\n",
        "\n",
        "\n",
        "def run_multiple_experiments(nbr_exp, rdm_seed, dictio, info_x_axis):\n",
        "  #rdm_seed = 4654321\n",
        "  np.random.seed(rdm_seed)\n",
        "  res = run_experiments(dicc, list_methods_strategy)\n",
        "  plot_res(info_x_axis, res, dicc)\n",
        "  '''\n",
        "  if nbr_exp > 1:\n",
        "    for k in res:\n",
        "      for h in res[k]:\n",
        "        res[k][h] = [res[k][h]]\n",
        "    for i in range(nbr_exp-1):\n",
        "      print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "      #np.random.seed(rdm_seed * (i+2))\n",
        "      res_partial = run_experiments(dictio, list_methods_strategy)\n",
        "      plot_res(info_x_axis, res_partial, dictio)\n",
        "      print(res)\n",
        "      for k in res:\n",
        "        res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "        res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "        res[k]['best_alpha'].append(res_partial[k]['best_alpha'])\n",
        "        #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "        #res.append(res['l2_dist_best_coeff_gt'])\n",
        "  '''\n",
        "  for k in res:\n",
        "    for h in res[k]:\n",
        "      res[k][h] = [res[k][h]]\n",
        "  for i in range(nbr_exp-1):\n",
        "      print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "      #np.random.seed(rdm_seed * (i+2))\n",
        "      res_partial = run_experiments(dictio, list_methods_strategy)\n",
        "      print(\"res partial \\n\")\n",
        "      for k, value in res_partial.items():\n",
        "        print(\"key: \", k, \" value: \", value)\n",
        "      plot_res(info_x_axis, res_partial, dictio)\n",
        "      print(\"res in run multipl experiments\\n\")\n",
        "#      for k, value in res.items():\n",
        "#        print(\"key: \", k, \" value: \", value)\n",
        "      for k in res:\n",
        "        res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "        res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "        res[k]['best_hyper_p'].append(res_partial[k]['best_hyper_p'])\n",
        "        res[k]['best_alpha_dts'].append(res_partial[k]['best_alpha_dts'])\n",
        "        res[k]['best_alpha_mis'].append(res_partial[k]['best_alpha_mis'])\n",
        "\n",
        "        #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "        #res.append(res['l2_dist_best_coeff_gt'])\n",
        "\n",
        "  print(\"final step, let's take the mean of the results\")\n",
        "  #print(\"res, after all the experimetns \", res)\n",
        "  for k in res:\n",
        "    print(\"key in res \", k)\n",
        "    print(np.array(res[k]['l2_dist_best_coeff_gt']))\n",
        "    print(\"mean l2_dist              \", np.mean(np.array(res[k]['l2_dist_best_coeff_gt']), axis=0))\n",
        "    print(\"mean_l2_dist diff method: \", np.mean(res[k]['l2_dist_best_coeff_gt'], axis=0))\n",
        "  #mean_res = {k: np.mean(v, axis=0) for k, v in res.items()}\n",
        "  mean_res = {k: {v: np.mean(w, axis=0) for v, w in res[k].items()} for k in res}\n",
        "  print(\"final dictionary, dictionary of the means:\")\n",
        "  for k, v in mean_res.items():\n",
        "    print(\"k:   \", k)\n",
        "    for s, t in v.items():\n",
        "      print(s, \": \", t)\n",
        "  return mean_res\n",
        "  #print(np.mean(res, axis=0))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_2LB5UnMpgCC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#info_axis = 'train'\n",
        "#n_train = [400, 800, 1200, 1600, 2000]\n",
        "#p_seen = make_probabilities([0.8, 0.8, 0.8, 0.8, 0.8])\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen\n",
        "#info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "\n",
        "# def get_path(X, y, estimator, amax, dts_max, mis_max, S_dict, eps_amax=1e-4, eps_dts_max=1e-3, eps_mis_max=1e-3, n_alphas=100, n_deltas_dts=2, n_deltas_mis=3):\n",
        "gen = 'fixed'\n",
        "info_axis = 'train'  # train or p_seen\n",
        "#p_seen_both = [1, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.02]\n",
        "#p_seen_both = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "#p_seen_both = [1, 0.9, 0.8/0.9, 0.7/0.8, 0.6/0.7, 0.5/0.6]\n",
        "#p_seen_both = [1, 0.9, 0.8/0.9, 0.7/0.8, 0.6/0.7, 0.5/0.6, 0.4/0.5, 0.3/0.4]\n",
        "n_train = [100, 150, 200, 250, 300]  # check how dataset are generated, there should be some problems with 'fixed'\n",
        "lenght_vec = len(n_train)\n",
        "p_seen_both = [0.75, 1, 1, 1, 1, 1]\n",
        "#p_seen_both = [1, 0.9, 0.8]\n",
        "length_vec = len(p_seen_both)\n",
        "#n_train = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
        "error_vec =  [0] * length_vec\n",
        "#p_seen = make_probabilities(p_seen_both)\n",
        "p_seen = [p_seen_both] * length_vec\n",
        "if info_axis == 'train':\n",
        "  main_vec = n_train\n",
        "  fix_vec = 'prob_seen:' + str(p_seen_both[0])\n",
        "elif info_axis == 'p_seen':\n",
        "  main_vec = np.cumprod(p_seen_both)  # p_seen_both\n",
        "  fix_vec = 'n_train:' + str(n_train[0])\n",
        "elif info_axis == 'error':\n",
        "  main_vec = error_vec\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen_both\n",
        "info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "number_test = 20000\n",
        "#cov_var = 0.6\n",
        "#beta_gt = np.array([-0.5, 2, 1, 3, -2, -3, 4, 0.5, 7, -9, -1, -2, -3, 4, 5, 6, 7, 8])\n",
        "#beta_gt = np.array([2, 4, -0.5, 2, 1, 3, -2, -3, 4, 0.5, 7, -9, -1, -2, -3, 4])\n",
        "beta_gt = np.random.randn(6)\n",
        "print(beta_gt)\n",
        "dim = len(beta_gt)\n",
        "mean = np.array([0] * dim)\n",
        "matr = np.random.randn(dim, dim) * 1\n",
        "cov = matr.T @ matr + np.eye(dim) * 0.5\n",
        "# np.array([[1, cov_var], [cov_var, 1]])\n",
        "data_type = 'Uniform'\n",
        "\n",
        "dicc = make_dictionary_data(\n",
        "    nbr_experiments= len(main_vec), n_train = n_train, n_test=number_test,\n",
        "    data = {'data': data_type, 'mean': mean, 'cov': cov},\n",
        "    beta_gt = beta_gt,\n",
        "    p_miss = p_seen,\n",
        "    err_vector = ['Gaussian_on_X', error_vec],\n",
        "    plots = []#['points', 'l1_vs_coef', '1/alpha_vs_coef']\n",
        ")\n",
        "#dicc = dicc | {'generation':gen}\n",
        "dicc['what_to_plot'] = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts']\n",
        "dicc = dicc | {'generation': gen, 'title_infer_error':' inference_error', 'title_test_error':'  test_error'}\n",
        "dicc = dicc | {'title_dts_radius': 'dts_radius', 'title_mis_radius': 'mis_radius'}\n",
        "dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg_max': 1,\n",
        "                             'eps_adv_rad_times_delta_dts': 1e-5, 'eps_adv_rad_times_delta_mis': 1e-5, 'eps_alpha_ridge_reg': 1e-5,\n",
        "                             'n_a_dts': 30, 'n_a_mis':30, 'n_a_rid': 1}}\n",
        "dicc['what_to_plot'] = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts', 'best_alpha_mis']\n",
        "\n",
        "for key, value in dicc.items():\n",
        "  print(key, \": \" , value)\n",
        "\n",
        "# (imp method, cov strategy, mi_nbr)\n",
        "#list_imp_cov_methods = [('BR_si', 'sd'), ('l_d', 'sd'), ('mi', 'sd', 1)]\n",
        "\n",
        "#list_methods_strategy = make_dictionary_method(list_imp_cov_methods)\n",
        "mi_nbr = 10\n",
        "# def get_path(X, y, estimator, amax, dts_max, mis_max, S_dict, eps_amax=1e-4, eps_dts_max=1e-3, eps_mis_max=1e-3, n_alphas=100, n_deltas_dts=2, n_deltas_mis=3):\n",
        "\n",
        "list_methods_strategy = [{'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'adv', 'color': 'b'},  #, 'multip_dataset': 3, 'multip_missing':0},\n",
        "                         {'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'ridge', 'color': 'k'},  #, 'multip_dataset': 3, 'multip_missing':0},\n",
        "                        #{'imp_method': 'l_d', 'cov_strategy': 'std_nan', 'multip_dataset': 3, 'multip_missing':3},\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'zero', 'algo_superv_learn': 'adv'},\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'zero', 'algo_superv_learn': 'ridge'},  #, 'multip_dataset': 3, 'multip_missing': 0}\n",
        "                        {'imp_method': 'oracle', 'cov_strategy': 'sd', 'algo_superv_learn': 'adv', 'color': 'orange'},\n",
        "                        {'imp_method': 'oracle', 'cov_strategy': 'sd', 'algo_superv_learn': 'ridge', 'color': 'purple'},\n",
        "                        #{'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'ridge'},#, 'multip_dataset': 3, 'multip_missing':0},\n",
        "                        #{'imp_method': 'l_d', 'cov_strategy': 'std_nan', 'multip_dataset': 3, 'multip_missing':3},\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'sd', 'algo_superv_learn':'ridge'},#, 'multip_dataset': 3, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'algo_superv_learn':'adv'}, #, 'multip_dataset': 3, 'multip_missing': 3},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'algo_superv_learn':'adv', 'color': 'g'}, #, 'multip_dataset': 3, 'multip_missing': 3},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'zero', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'algo_superv_learn':'adv', 'color': 'r'}\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'algo_superv_learn':'adv'}\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 0, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 1},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 3},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'std_mi', 'mi_nbr': mi_nbr},\n",
        "                        #{'imp_method': 'mi_pure', 'cov_strategy': 'cond_var', 'cov_strategy_within': 'sd', 'mi_nbr': 5},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 1},\n",
        "                        #{'imp_method': 'mi_mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'eye', 'mi_nbr': 5},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'multip_betw': 0, 'multip_with': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.2},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.4},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.6},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 0, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 0, 'multip_missing': 1},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 3, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'conc', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr}#, 'multip_dataset': 3, 'multip_missing': 3}\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 5},\n",
        "                        ]\n",
        "print(list_methods_strategy)\n",
        "for el in list_methods_strategy:\n",
        "  for key, value in el.items():\n",
        "    print(key,\": \" , value)\n",
        "\n",
        "print(\"----> Starting experiments\")\n",
        "\n",
        "'''\n",
        "nbr_exp = 2\n",
        "#res[key_tuple]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "#res[key_tuple]['best_coeff'].append(coeff_round)\n",
        "#res[key_tuple]['best_score'].append(score_round)\n",
        "#res[key_tuple]['best_alpha'].append(alpha_round)\n",
        "res_l2 = []\n",
        "\n",
        "rdm_seed = 4654321\n",
        "np.random.seed(rdm_seed)\n",
        "res = run_experiments(dicc, list_methods_strategy)\n",
        "plot_res(info_x_axis, res, dicc)\n",
        "if nbr_exp > 1:\n",
        "  for k in res:\n",
        "    for h in res[k]:\n",
        "      res[k][h] = [res[k][h]]\n",
        "  for i in range(nbr_exp-1):\n",
        "    print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "    #np.random.seed(rdm_seed * (i+2))\n",
        "    res_partial = run_experiments(dicc, list_methods_strategy)\n",
        "    plot_res(info_x_axis, res_partial, dicc)\n",
        "    print(res)\n",
        "    for k in res:\n",
        "      res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "      res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "      res[k]['best_alpha'].append(res_partial[k]['best_alpha'])\n",
        "      #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "    #res.append(res['l2_dist_best_coeff_gt'])\n",
        "\n",
        "print(\"final \")\n",
        "print(res)\n",
        "for k in res:\n",
        "  print(k)\n",
        "  print(np.array(res[k]['l2_dist_best_coeff_gt']))\n",
        "  print(np.mean(np.array(res[k]['l2_dist_best_coeff_gt']), axis=0))\n",
        "  print(np.mean(res[k]['l2_dist_best_coeff_gt'], axis=0))\n",
        "#mean_res = {k: np.mean(v, axis=0) for k, v in res.items()}\n",
        "mean_res = {k: {v: np.mean(w, axis=0) for v, w in res[k].items()} for k in res}\n",
        "for k, v in mean_res.items():\n",
        "  print(\"k:   \", k)\n",
        "  for s, t in v.items():\n",
        "    print(s, \": \", t)\n",
        "#print(np.mean(res, axis=0))\n",
        "'''\n",
        "\n",
        "nbr_exp = 5\n",
        "seed = 67\n",
        "mean_res = run_multiple_experiments(nbr_exp, seed, dicc, info_x_axis)\n",
        "print(\"PLOT OF THE MEANS\")\n",
        "dicc['title_infer_error'] = 'seed: ' + str(seed) + ', nbr_exp: ' + str(nbr_exp) + ', data: '+ data_type + ', dim: ' + str(dim) # ', cov: ' + str(cov_var)\n",
        "dicc['title_test_error'] = 'sigma_err: ' + str(error_vec[0]) + ', ' + fix_vec + ', n_test: ' + str(number_test)\n",
        "#dicc['title_dts_radius'] = 'dts_radius'\n",
        "#dicc['title_mis_radius'] = 'mis_radius'\n",
        "#dicc = dicc | {'title_dts_radius': 'title_dts_radius', 'title_mis_radius': 'title_mis_radius'}\n",
        "#dicc = dicc | {'generation':gen, 'title_infer_error':'mean_infer_error, rep: ' + str(nbr_exp), 'title_mean_error':'mean_test_error'}\n",
        "#dicc['what_to_plot'] = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts', 'best_alpha_mis']\n",
        "plot_res(info_x_axis, mean_res, dicc)\n",
        "\n",
        "## you can see if you manage to take the index i that maximize alpha\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bXcjBX8GqIAF",
        "outputId": "4d8c947c-42cf-4e61-d4c6-c0aeba2feffa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.86470788  0.62528015  0.55009825  0.83920284  1.00969363 -1.25692648]\n",
            "change nbr_experiments to match the size of n_train\n",
            "data :  [{'data': 'Uniform', 'mean': array([0, 0, 0, 0, 0, 0]), 'cov': array([[ 5.34970991, -0.52841586,  2.54507316,  1.72584165, -1.26274061,\n",
            "         1.44519273],\n",
            "       [-0.52841586,  3.30305498,  1.43869502, -0.68965551,  1.16917263,\n",
            "         1.47245705],\n",
            "       [ 2.54507316,  1.43869502,  7.11782747,  2.61412317,  0.60116181,\n",
            "         1.0596404 ],\n",
            "       [ 1.72584165, -0.68965551,  2.61412317,  3.07879342, -0.0898754 ,\n",
            "        -0.40772367],\n",
            "       [-1.26274061,  1.16917263,  0.60116181, -0.0898754 ,  2.20026913,\n",
            "        -0.09742042],\n",
            "       [ 1.44519273,  1.47245705,  1.0596404 , -0.40772367, -0.09742042,\n",
            "         3.75949907]])}, {'data': 'Uniform', 'mean': array([0, 0, 0, 0, 0, 0]), 'cov': array([[ 5.34970991, -0.52841586,  2.54507316,  1.72584165, -1.26274061,\n",
            "         1.44519273],\n",
            "       [-0.52841586,  3.30305498,  1.43869502, -0.68965551,  1.16917263,\n",
            "         1.47245705],\n",
            "       [ 2.54507316,  1.43869502,  7.11782747,  2.61412317,  0.60116181,\n",
            "         1.0596404 ],\n",
            "       [ 1.72584165, -0.68965551,  2.61412317,  3.07879342, -0.0898754 ,\n",
            "        -0.40772367],\n",
            "       [-1.26274061,  1.16917263,  0.60116181, -0.0898754 ,  2.20026913,\n",
            "        -0.09742042],\n",
            "       [ 1.44519273,  1.47245705,  1.0596404 , -0.40772367, -0.09742042,\n",
            "         3.75949907]])}, {'data': 'Uniform', 'mean': array([0, 0, 0, 0, 0, 0]), 'cov': array([[ 5.34970991, -0.52841586,  2.54507316,  1.72584165, -1.26274061,\n",
            "         1.44519273],\n",
            "       [-0.52841586,  3.30305498,  1.43869502, -0.68965551,  1.16917263,\n",
            "         1.47245705],\n",
            "       [ 2.54507316,  1.43869502,  7.11782747,  2.61412317,  0.60116181,\n",
            "         1.0596404 ],\n",
            "       [ 1.72584165, -0.68965551,  2.61412317,  3.07879342, -0.0898754 ,\n",
            "        -0.40772367],\n",
            "       [-1.26274061,  1.16917263,  0.60116181, -0.0898754 ,  2.20026913,\n",
            "        -0.09742042],\n",
            "       [ 1.44519273,  1.47245705,  1.0596404 , -0.40772367, -0.09742042,\n",
            "         3.75949907]])}, {'data': 'Uniform', 'mean': array([0, 0, 0, 0, 0, 0]), 'cov': array([[ 5.34970991, -0.52841586,  2.54507316,  1.72584165, -1.26274061,\n",
            "         1.44519273],\n",
            "       [-0.52841586,  3.30305498,  1.43869502, -0.68965551,  1.16917263,\n",
            "         1.47245705],\n",
            "       [ 2.54507316,  1.43869502,  7.11782747,  2.61412317,  0.60116181,\n",
            "         1.0596404 ],\n",
            "       [ 1.72584165, -0.68965551,  2.61412317,  3.07879342, -0.0898754 ,\n",
            "        -0.40772367],\n",
            "       [-1.26274061,  1.16917263,  0.60116181, -0.0898754 ,  2.20026913,\n",
            "        -0.09742042],\n",
            "       [ 1.44519273,  1.47245705,  1.0596404 , -0.40772367, -0.09742042,\n",
            "         3.75949907]])}, {'data': 'Uniform', 'mean': array([0, 0, 0, 0, 0, 0]), 'cov': array([[ 5.34970991, -0.52841586,  2.54507316,  1.72584165, -1.26274061,\n",
            "         1.44519273],\n",
            "       [-0.52841586,  3.30305498,  1.43869502, -0.68965551,  1.16917263,\n",
            "         1.47245705],\n",
            "       [ 2.54507316,  1.43869502,  7.11782747,  2.61412317,  0.60116181,\n",
            "         1.0596404 ],\n",
            "       [ 1.72584165, -0.68965551,  2.61412317,  3.07879342, -0.0898754 ,\n",
            "        -0.40772367],\n",
            "       [-1.26274061,  1.16917263,  0.60116181, -0.0898754 ,  2.20026913,\n",
            "        -0.09742042],\n",
            "       [ 1.44519273,  1.47245705,  1.0596404 , -0.40772367, -0.09742042,\n",
            "         3.75949907]])}]\n",
            "n_tot :  [20100, 20150, 20200, 20250, 20300]\n",
            "n_train :  [100, 150, 200, 250, 300]\n",
            "n_test :  [20000, 20000, 20000, 20000, 20000]\n",
            "dim :  [6, 6, 6, 6, 6]\n",
            "beta_gt :  [array([-0.86470788,  0.62528015,  0.55009825,  0.83920284,  1.00969363,\n",
            "       -1.25692648]), array([-0.86470788,  0.62528015,  0.55009825,  0.83920284,  1.00969363,\n",
            "       -1.25692648]), array([-0.86470788,  0.62528015,  0.55009825,  0.83920284,  1.00969363,\n",
            "       -1.25692648]), array([-0.86470788,  0.62528015,  0.55009825,  0.83920284,  1.00969363,\n",
            "       -1.25692648]), array([-0.86470788,  0.62528015,  0.55009825,  0.83920284,  1.00969363,\n",
            "       -1.25692648])]\n",
            "perc_test :  [0.9950248756218906, 0.9925558312655087, 0.9900990099009901, 0.9876543209876543, 0.9852216748768473]\n",
            "err :  [{'type': 'Gaussian_on_X', 'scaling': 0}, {'type': 'Gaussian_on_X', 'scaling': 0}, {'type': 'Gaussian_on_X', 'scaling': 0}, {'type': 'Gaussian_on_X', 'scaling': 0}, {'type': 'Gaussian_on_X', 'scaling': 0}]\n",
            "plots :  [[], [], [], [], []]\n",
            "p_miss :  [[0.75, 1, 1, 1, 1, 1], [0.75, 1, 1, 1, 1, 1], [0.75, 1, 1, 1, 1, 1], [0.75, 1, 1, 1, 1, 1], [0.75, 1, 1, 1, 1, 1], [0.75, 1, 1, 1, 1, 1]]\n",
            "what_to_plot :  ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts', 'best_alpha_mis']\n",
            "generation :  fixed\n",
            "title_infer_error :   inference_error\n",
            "title_test_error :    test_error\n",
            "title_dts_radius :  dts_radius\n",
            "title_mis_radius :  mis_radius\n",
            "info_algo :  {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg_max': 1, 'eps_adv_rad_times_delta_dts': 1e-05, 'eps_adv_rad_times_delta_mis': 1e-05, 'eps_alpha_ridge_reg': 1e-05, 'n_a_dts': 30, 'n_a_mis': 30, 'n_a_rid': 1}\n",
            "[{'imp_method': 'BR_si', 'cov_strategy': 'sd', 'algo_superv_learn': 'adv', 'color': 'b'}, {'imp_method': 'BR_si', 'cov_strategy': 'sd', 'algo_superv_learn': 'ridge', 'color': 'k'}, {'imp_method': 'oracle', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'adv', 'color': 'orange'}, {'imp_method': 'oracle', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'ridge', 'color': 'purple'}]\n",
            "imp_method :  BR_si\n",
            "cov_strategy :  sd\n",
            "algo_superv_learn :  adv\n",
            "color :  b\n",
            "imp_method :  BR_si\n",
            "cov_strategy :  sd\n",
            "algo_superv_learn :  ridge\n",
            "color :  k\n",
            "imp_method :  oracle\n",
            "cov_strategy :  std_nan\n",
            "algo_superv_learn :  adv\n",
            "color :  orange\n",
            "imp_method :  oracle\n",
            "cov_strategy :  std_nan\n",
            "algo_superv_learn :  ridge\n",
            "color :  purple\n",
            "----> Starting experiments\n",
            "[[], [], [], [], []]\n",
            "[]\n",
            "BR_si\n",
            "sd\n",
            "adv\n",
            "b\n",
            "BR_si\n",
            "sd\n",
            "ridge\n",
            "k\n",
            "oracle\n",
            "std_nan\n",
            "adv\n",
            "orange\n",
            "oracle\n",
            "std_nan\n",
            "ridge\n",
            "purple\n",
            "{'data': 'Uniform', 'mean': array([0, 0, 0, 0, 0, 0]), 'cov': array([[ 5.34970991, -0.52841586,  2.54507316,  1.72584165, -1.26274061,\n",
            "         1.44519273],\n",
            "       [-0.52841586,  3.30305498,  1.43869502, -0.68965551,  1.16917263,\n",
            "         1.47245705],\n",
            "       [ 2.54507316,  1.43869502,  7.11782747,  2.61412317,  0.60116181,\n",
            "         1.0596404 ],\n",
            "       [ 1.72584165, -0.68965551,  2.61412317,  3.07879342, -0.0898754 ,\n",
            "        -0.40772367],\n",
            "       [-1.26274061,  1.16917263,  0.60116181, -0.0898754 ,  2.20026913,\n",
            "        -0.09742042],\n",
            "       [ 1.44519273,  1.47245705,  1.0596404 , -0.40772367, -0.09742042,\n",
            "         3.75949907]])}\n",
            "(20300, 6)\n",
            "p_missing in generate mask  [0.75, 1, 1, 1, 1, 1]\n",
            "---------------------------------------------------------------------------------------------------------------------------> iteration  0\n",
            "{'data': 'Uniform', 'mean': array([0, 0, 0, 0, 0, 0]), 'cov': array([[ 5.34970991, -0.52841586,  2.54507316,  1.72584165, -1.26274061,\n",
            "         1.44519273],\n",
            "       [-0.52841586,  3.30305498,  1.43869502, -0.68965551,  1.16917263,\n",
            "         1.47245705],\n",
            "       [ 2.54507316,  1.43869502,  7.11782747,  2.61412317,  0.60116181,\n",
            "         1.0596404 ],\n",
            "       [ 1.72584165, -0.68965551,  2.61412317,  3.07879342, -0.0898754 ,\n",
            "        -0.40772367],\n",
            "       [-1.26274061,  1.16917263,  0.60116181, -0.0898754 ,  2.20026913,\n",
            "        -0.09742042],\n",
            "       [ 1.44519273,  1.47245705,  1.0596404 , -0.40772367, -0.09742042,\n",
            "         3.75949907]])}\n",
            "(20100, 6)\n",
            "n_tot_fll  [100, 150, 200, 250, 300] ,   100\n",
            "X shape in clear data  (100, 6)\n",
            "y shape in clear data  (100,)\n",
            "M shape in clear data  (100,)\n",
            "shapes X_train cleaned, mask train cleaned, y train cleaned\n",
            "(100, 6)\n",
            "(100, 6)\n",
            "(100,)\n",
            "full masks in run experiment  [[1 0 0 0 0 0]\n",
            " [0 1 0 1 0 0]\n",
            " [1 0 0 0 0 0]\n",
            " ...\n",
            " [0 0 1 1 0 0]\n",
            " [1 0 1 0 0 0]\n",
            " [0 1 1 0 0 0]]\n",
            "info algo in run experiments  {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg_max': 1, 'eps_adv_rad_times_delta_dts': 1e-05, 'eps_adv_rad_times_delta_mis': 1e-05, 'eps_alpha_ridge_reg': 1e-05, 'n_a_dts': 30, 'n_a_mis': 30, 'n_a_rid': 1}\n",
            "ciaoooooo dict obser in run experiments \n",
            "  {'X_train_masked': (array([[-3.47594306e-01, -3.47425159e-01, -6.22503650e-02,\n",
            "        -3.98352345e-01,  2.70283110e-01,  4.23332782e-01],\n",
            "       [ 1.14811480e-01, -1.81638266e-01,  3.07984545e-01,\n",
            "         3.63271625e-02, -4.35388421e-01, -1.23648355e-02],\n",
            "       [ 2.14495760e-01, -3.35666466e-01, -4.24016138e-01,\n",
            "         1.15379032e-01,  1.75869647e-01,  1.32024631e-01],\n",
            "       [ 3.67426082e-01, -4.21183072e-01,  9.07368214e-02,\n",
            "        -4.34078731e-01, -3.48526283e-01, -4.59846147e-01],\n",
            "       [-1.95663381e-01, -4.18896944e-01, -1.72266638e-01,\n",
            "        -2.36704393e-01,  1.49605965e-01, -2.57197837e-01],\n",
            "       [ 3.75111490e-02,  3.80447439e-01,  4.37939302e-01,\n",
            "         1.13716780e-01, -3.87250748e-01,  3.71589359e-01],\n",
            "       [-1.04436901e-01, -9.55596825e-03,  4.08508526e-01,\n",
            "         3.98631512e-01, -1.10948062e-01,  2.24133116e-01],\n",
            "       [ 3.51427383e-01,  3.95290079e-01, -6.57447443e-02,\n",
            "         1.02499233e-01,  2.46747799e-01, -1.66928329e-01],\n",
            "       [ 4.50330650e-01,  1.58922472e-01,  1.06479257e-01,\n",
            "         3.04990341e-01, -2.37925114e-01,  3.36339093e-01],\n",
            "       [-1.93262528e-01, -2.29336884e-01, -1.50678678e-01,\n",
            "        -2.53330008e-01,  2.22834619e-01,  3.35561623e-01],\n",
            "       [-1.44809515e-01, -1.68663724e-01, -3.04626221e-01,\n",
            "         3.99914613e-01, -3.61148571e-01, -2.71661932e-01],\n",
            "       [ 3.34429696e-01, -4.11826436e-01, -3.97626272e-02,\n",
            "        -4.71627571e-01,  3.58355360e-01, -2.25748779e-01],\n",
            "       [-1.89084626e-01,  4.27165103e-01,  4.24433900e-02,\n",
            "        -4.02968246e-01, -4.07236413e-01, -4.03264521e-01],\n",
            "       [ 4.10474708e-01,  7.20677161e-02, -7.66127779e-02,\n",
            "        -1.19494039e-01,  2.66741143e-01,  4.57361389e-01],\n",
            "       [ 4.79238609e-01,  2.68017243e-01,  5.73655935e-02,\n",
            "         3.51169799e-02,  1.25478117e-01,  1.43004417e-01],\n",
            "       [-8.18159594e-02,  1.25524182e-01,  4.01773679e-01,\n",
            "        -4.91198325e-01, -9.14059483e-02,  1.65104788e-01],\n",
            "       [-1.42635287e-01,  2.63938777e-01,  3.19646679e-01,\n",
            "        -3.45427576e-01,  2.56518074e-01,  6.44382720e-02],\n",
            "       [-1.80592926e-01, -2.60421266e-02, -2.13862378e-01,\n",
            "         1.88409703e-01, -4.50939575e-01,  2.45938715e-02],\n",
            "       [-2.92860449e-01,  2.03768594e-01,  1.51841326e-02,\n",
            "         2.60448709e-01,  2.34826588e-01,  3.33516269e-01],\n",
            "       [ 4.43991698e-01, -1.19347163e-01,  4.51224394e-01,\n",
            "         1.04039081e-01, -8.02469559e-02,  4.56035651e-01],\n",
            "       [ 2.22010585e-01,  1.20591975e-01, -3.64521439e-01,\n",
            "         4.50841888e-02,  4.53711702e-01,  1.34714306e-01],\n",
            "       [-1.34182505e-01,  1.05767047e-01,  3.78528385e-01,\n",
            "         3.60101755e-01, -4.75160106e-01,  1.97289598e-01],\n",
            "       [-4.14753039e-01, -7.85700423e-03, -1.49949198e-01,\n",
            "        -1.04167642e-01,  1.70833828e-02, -3.88286561e-01],\n",
            "       [ 2.25986146e-01, -8.42663353e-03,  2.21903167e-01,\n",
            "        -3.29601314e-01,  8.28112343e-02, -1.02026865e-01],\n",
            "       [-8.49092928e-02, -1.68891337e-01,  2.87065371e-01,\n",
            "        -2.25477671e-01, -2.54506424e-01,  2.51509184e-02],\n",
            "       [-1.93196633e-01,  1.86698405e-01, -2.86936823e-01,\n",
            "         3.18980199e-02, -1.39222908e-01, -1.23894797e-02],\n",
            "       [-3.02564815e-03, -2.02139126e-01, -4.71823039e-01,\n",
            "        -3.20552492e-01, -3.89329623e-01, -2.81546630e-01],\n",
            "       [ 4.95783860e-01, -4.12168554e-01, -2.89873518e-01,\n",
            "         3.58090420e-01,  2.56502763e-01, -2.31816007e-01],\n",
            "       [-3.63530410e-01, -3.17083950e-01, -1.48234098e-02,\n",
            "        -2.97658241e-01, -9.06180513e-02,  1.25939593e-01],\n",
            "       [ 3.42020890e-01, -2.98121843e-01, -4.00329041e-01,\n",
            "         4.63876691e-01, -1.83371296e-02,  2.80909962e-01],\n",
            "       [-1.09442501e-01, -4.02542217e-01, -8.50766838e-02,\n",
            "         3.74716755e-01, -5.12277535e-02,  4.89975185e-01],\n",
            "       [-2.47343160e-01, -4.95324320e-01, -2.50145187e-01,\n",
            "        -1.21195251e-02,  4.28340327e-01, -3.78616903e-01],\n",
            "       [-2.86935511e-01,  3.31391127e-01, -1.56309328e-01,\n",
            "        -4.27247820e-01, -3.83402724e-01,  3.90646041e-01],\n",
            "       [-3.61752683e-01, -8.30505007e-02,  3.11369437e-01,\n",
            "        -4.02786258e-01,  7.93618059e-02,  3.11117655e-01],\n",
            "       [-1.07923060e-01, -2.68998255e-01, -4.49351543e-01,\n",
            "        -5.46178971e-02,  2.04507215e-01,  4.74683740e-01],\n",
            "       [-2.89321812e-01, -4.73430188e-01, -1.37989836e-01,\n",
            "         4.43078728e-01, -2.69678824e-01,  6.93906926e-02],\n",
            "       [ 4.33944138e-01, -1.33928446e-01,  3.29220695e-01,\n",
            "        -3.58773211e-01,  2.11288066e-01, -5.25126644e-02],\n",
            "       [ 3.57396176e-01,  3.60816779e-01, -4.86407516e-01,\n",
            "        -4.46019832e-01,  1.94046256e-01,  7.50303645e-02],\n",
            "       [ 4.24256493e-02,  4.79551256e-01,  8.07054565e-02,\n",
            "         5.02348981e-03, -4.69395240e-01, -5.13291276e-02],\n",
            "       [-3.86334987e-01, -3.01561511e-01,  5.76297124e-02,\n",
            "         8.56796372e-02, -2.80182398e-01,  2.10191479e-02],\n",
            "       [-5.18361540e-02, -1.71304371e-01, -2.91164825e-01,\n",
            "        -3.78720949e-01,  3.87028408e-01, -3.77035911e-01],\n",
            "       [-2.86616491e-01,  1.90338372e-01, -2.38392617e-01,\n",
            "         2.17046227e-01,  3.81049121e-01, -4.40003801e-01],\n",
            "       [-3.39175843e-01,  3.39546251e-02,  3.92748424e-01,\n",
            "        -4.17006743e-01,  2.17681744e-01,  1.83397133e-01],\n",
            "       [ 1.23886424e-01,  2.24427714e-01,  3.83838553e-01,\n",
            "         3.53497602e-01,  4.80105112e-01, -1.30832476e-01],\n",
            "       [ 1.37078983e-01, -2.21249950e-01,  3.20718778e-01,\n",
            "        -2.17763065e-02, -4.95205275e-01, -3.37510146e-01],\n",
            "       [ 1.55204181e-02, -2.67928069e-01, -4.74741111e-01,\n",
            "        -3.83630669e-01,  6.84579572e-03, -3.06786902e-01],\n",
            "       [-3.79980314e-01, -1.58241112e-01, -4.83143653e-01,\n",
            "         4.19228160e-01,  2.92369196e-01, -2.82545592e-01],\n",
            "       [-3.49734195e-01,  1.08312177e-01, -4.06150233e-01,\n",
            "         4.87953106e-01, -2.55707510e-01,  2.62305568e-01],\n",
            "       [-3.38790411e-04, -9.59073235e-02, -3.07427038e-01,\n",
            "        -3.32644821e-01, -2.34474856e-01,  9.64044299e-02],\n",
            "       [-1.54440830e-01,  1.20275791e-01, -4.14176136e-01,\n",
            "         1.74483490e-01,  4.78402021e-01, -3.80585221e-01],\n",
            "       [ 1.75507307e-01, -7.76513686e-02, -1.77661467e-01,\n",
            "         1.71793075e-01,  4.71705624e-01,  2.82277290e-02],\n",
            "       [ 2.90530829e-02,  7.93177323e-02, -4.66302244e-01,\n",
            "        -1.25927932e-01, -3.98622630e-02, -7.43181432e-02],\n",
            "       [-2.67596008e-01, -4.41042959e-01, -3.97211121e-01,\n",
            "         1.90197469e-01,  1.08798054e-01, -2.57033322e-01],\n",
            "       [-4.37807062e-01, -4.30164677e-02,  4.47677513e-01,\n",
            "         4.17736297e-01, -2.50930709e-01,  2.66041396e-02],\n",
            "       [ 3.11438538e-01, -4.11063757e-01,  2.00066144e-01,\n",
            "        -5.67639519e-02,  4.92194505e-01, -3.88280381e-01],\n",
            "       [ 3.18139442e-01, -4.77235115e-02,  4.05234686e-01,\n",
            "         2.09699200e-01,  3.72473555e-01,  2.78089999e-02],\n",
            "       [-2.56875608e-01,  3.15577132e-01,  4.26794041e-01,\n",
            "         3.94310371e-02,  1.03530075e-01,  2.48491408e-02],\n",
            "       [-1.64807460e-01, -2.18058366e-01,  9.12141407e-02,\n",
            "        -4.88994291e-01,  4.81452386e-01,  7.53178133e-02],\n",
            "       [ 1.43398699e-02,  3.25567380e-03, -3.84595325e-01,\n",
            "         9.33025006e-02, -9.58807308e-03, -1.39547078e-01],\n",
            "       [-6.29616866e-02,  3.12735656e-01,  1.49890291e-01,\n",
            "         2.12387302e-01,  1.88020440e-01, -4.54189602e-01],\n",
            "       [-2.16381796e-01, -2.60137213e-02, -2.77108862e-01,\n",
            "        -2.12264482e-01, -9.17434897e-02,  4.20011231e-01],\n",
            "       [ 1.50622904e-01, -1.30384810e-01,  6.56627080e-02,\n",
            "         1.69323201e-01, -2.66325911e-01, -1.26894543e-01],\n",
            "       [-2.61670405e-01,  4.86994131e-02,  2.20704162e-01,\n",
            "         2.80459485e-01, -2.84171660e-01,  1.53115319e-01],\n",
            "       [-2.61777421e-01,  4.14835707e-02,  4.56985165e-02,\n",
            "        -2.74502821e-01,  9.14731217e-02,  2.73039778e-01],\n",
            "       [-3.27782944e-01,  4.15341598e-03, -4.83347865e-01,\n",
            "        -3.26216733e-01,  1.88865153e-01,  1.23890516e-01],\n",
            "       [-5.31966555e-02,  2.63296202e-01, -6.57005292e-02,\n",
            "         4.20041845e-01, -8.95714042e-03,  2.57489625e-01],\n",
            "       [ 1.77523258e-01, -4.08864955e-01, -1.21460777e-01,\n",
            "         1.05481504e-01, -3.51341373e-01,  2.36878408e-01],\n",
            "       [ 8.89164471e-02,  3.97421725e-01,  5.64617284e-02,\n",
            "        -3.75902021e-02,  2.61616312e-01,  4.81581226e-01],\n",
            "       [ 5.98822299e-02,  4.50213393e-01,  4.84811618e-01,\n",
            "        -1.54987507e-01, -2.32550177e-01,  3.62867000e-01],\n",
            "       [ 2.55564000e-01,  9.00452014e-02,  2.02524870e-01,\n",
            "         3.24079623e-01,  1.34678213e-02, -2.14345462e-01],\n",
            "       [ 2.26437207e-01, -2.33621541e-01, -3.59447215e-01,\n",
            "        -1.18757301e-01, -1.52948581e-01, -2.90121606e-01],\n",
            "       [ 1.16691142e-02, -1.61904230e-01,  3.74390212e-01,\n",
            "        -2.32644778e-01, -1.10118989e-01,  4.09593164e-01],\n",
            "       [-3.99735004e-01, -1.06369960e-01,  2.59951356e-02,\n",
            "         3.31752505e-01,  4.01074065e-01, -3.26714714e-01],\n",
            "       [-2.61602186e-01,  1.31898441e-01,  3.57409296e-01,\n",
            "         4.05343522e-01,  3.73059866e-01, -3.04190149e-01],\n",
            "       [ 4.90845286e-01,  1.36002259e-01, -4.23215583e-01,\n",
            "        -2.81067913e-01, -2.50774851e-01, -4.26194984e-01],\n",
            "       [ 4.22522324e-01,  2.30975268e-01, -3.74492026e-01,\n",
            "        -4.63886484e-01,  3.85001715e-01, -3.79550647e-01],\n",
            "       [-1.46133939e-01, -4.03498226e-01, -3.69280056e-01,\n",
            "         2.61409558e-01,  3.26895742e-01, -1.96673387e-01],\n",
            "       [ 2.70215947e-01, -1.52940640e-01,  9.83227844e-02,\n",
            "        -1.10022278e-01, -6.43110240e-02, -3.18281503e-02],\n",
            "       [ 3.43593660e-01, -5.34683765e-03, -8.25808308e-03,\n",
            "         3.75779858e-01,  2.62435937e-01, -7.37841904e-02],\n",
            "       [-4.86041170e-01, -5.80814001e-02, -2.66793443e-01,\n",
            "         2.22200085e-01,  3.48490454e-01,  2.08873490e-01],\n",
            "       [-2.22988278e-01,  3.95113071e-02,  1.64369656e-01,\n",
            "        -4.65453589e-01,  2.57157110e-01,  3.14872128e-01],\n",
            "       [ 2.73341285e-02,  3.01560802e-02, -4.10536211e-01,\n",
            "         1.64324435e-01, -4.53998308e-01, -3.43455374e-02],\n",
            "       [-3.36629930e-01,  2.55961995e-01,  2.00009682e-01,\n",
            "         3.06697809e-01,  3.62973129e-03, -4.09521566e-01],\n",
            "       [ 1.96204751e-01, -3.88323549e-01, -1.12823691e-01,\n",
            "         3.11046086e-01, -4.51827710e-01, -4.53178246e-01],\n",
            "       [ 1.94370381e-01, -1.73341209e-01, -3.73145448e-01,\n",
            "        -2.94602880e-01, -4.40539857e-01,  6.43263199e-02],\n",
            "       [-2.76104363e-01, -4.00378435e-01,  8.84164663e-02,\n",
            "        -3.08410167e-01, -1.83329596e-01,  1.80490503e-01],\n",
            "       [ 2.90913180e-01, -4.00923016e-01,  7.35747462e-02,\n",
            "        -4.41904790e-01, -6.16371841e-02, -1.18608372e-01],\n",
            "       [ 2.62778357e-01,  7.71898452e-02, -4.95493077e-02,\n",
            "         1.91745354e-01,  2.20570494e-01,  3.84738809e-01],\n",
            "       [-1.57182098e-01,  2.60361983e-01,  3.69696415e-02,\n",
            "        -4.87364667e-01, -2.77618787e-01, -1.23258312e-01],\n",
            "       [ 1.93064405e-01,  2.20155474e-01, -1.97252628e-01,\n",
            "        -2.26867583e-01,  1.26394767e-01, -2.11936775e-01],\n",
            "       [-1.47665264e-02,  3.69259783e-02,  3.89081487e-01,\n",
            "        -7.63403813e-02, -3.21080367e-01, -3.51709627e-01],\n",
            "       [ 9.37245130e-02, -1.98258935e-01,  7.48348190e-03,\n",
            "         8.68014139e-02, -1.57305991e-01, -1.26655133e-02],\n",
            "       [ 3.78163660e-02, -3.96602096e-01,  4.34870464e-01,\n",
            "         3.91379460e-01,  4.17015806e-02, -1.94487149e-01],\n",
            "       [ 4.04028584e-01, -4.03890738e-01,  2.62953732e-02,\n",
            "        -2.83882555e-01,  5.07770471e-02, -4.75623849e-01],\n",
            "       [-2.78526055e-01,  2.50852409e-01, -2.20155501e-01,\n",
            "        -2.63158108e-01,  4.23190139e-01,  3.93544744e-02],\n",
            "       [ 2.08499833e-01,  1.02716149e-01, -3.25351464e-01,\n",
            "        -3.83484331e-01, -2.26746354e-01, -4.31469345e-01],\n",
            "       [ 4.47304203e-01,  4.23912667e-01, -7.07630499e-02,\n",
            "        -3.85014561e-01, -1.80502357e-01, -2.31913285e-01],\n",
            "       [-2.44862736e-01, -1.41260245e-01, -1.58968414e-01,\n",
            "        -3.87765652e-01,  3.51542495e-01, -4.38423468e-01],\n",
            "       [ 3.75060754e-01,  2.84098443e-01, -2.67749430e-01,\n",
            "        -8.57742001e-03, -2.29020151e-03, -3.05257715e-01],\n",
            "       [-2.78947488e-01, -1.94134885e-01, -2.96187191e-01,\n",
            "         1.05094890e-01, -4.55836474e-01,  4.94604843e-01]]), array([[1, 0, 0, 0, 0, 0],\n",
            "       [0, 1, 0, 1, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0],\n",
            "       [0, 1, 1, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 1],\n",
            "       [1, 0, 1, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 0, 1],\n",
            "       [0, 0, 1, 0, 0, 0],\n",
            "       [0, 1, 1, 0, 0, 0],\n",
            "       [0, 1, 0, 1, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 1],\n",
            "       [0, 0, 1, 0, 1, 0],\n",
            "       [0, 0, 1, 1, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [1, 0, 1, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 1, 0],\n",
            "       [0, 1, 0, 1, 0, 0],\n",
            "       [0, 0, 1, 0, 0, 1],\n",
            "       [0, 0, 1, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 1],\n",
            "       [0, 1, 1, 1, 0, 0],\n",
            "       [0, 0, 0, 1, 0, 0],\n",
            "       [1, 0, 0, 0, 1, 0],\n",
            "       [0, 0, 0, 1, 0, 0],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [1, 0, 1, 1, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 1, 0, 0, 1, 0],\n",
            "       [1, 0, 1, 0, 0, 0],\n",
            "       [1, 1, 0, 0, 0, 1],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [1, 0, 0, 1, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 0, 0, 0, 0],\n",
            "       [1, 0, 1, 0, 1, 0],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 1, 0],\n",
            "       [0, 0, 0, 0, 0, 1],\n",
            "       [0, 0, 1, 1, 1, 1],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 1, 0, 0, 1],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 1, 1, 0, 1, 0],\n",
            "       [0, 0, 1, 0, 0, 1],\n",
            "       [1, 0, 1, 0, 1, 1],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [1, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [0, 0, 0, 1, 0, 0],\n",
            "       [0, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 1, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 1],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [0, 0, 1, 1, 0, 1],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [0, 1, 0, 1, 1, 0],\n",
            "       [0, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 1, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 1, 0],\n",
            "       [1, 0, 1, 0, 0, 0],\n",
            "       [0, 1, 0, 0, 0, 0],\n",
            "       [1, 1, 0, 0, 0, 1],\n",
            "       [0, 0, 0, 1, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 0, 1],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 1, 0, 0, 0],\n",
            "       [1, 0, 1, 0, 1, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 1, 0, 0],\n",
            "       [0, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 1, 1],\n",
            "       [0, 0, 0, 1, 0, 0],\n",
            "       [0, 1, 1, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 1],\n",
            "       [1, 0, 0, 0, 0, 0],\n",
            "       [0, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 1],\n",
            "       [0, 1, 0, 1, 0, 1],\n",
            "       [0, 0, 0, 0, 0, 0]])), 'X_test': array([[-0.33335053, -0.1843324 ,  0.23417966,  0.12758944,  0.11412876,\n",
            "         0.11913091],\n",
            "       [ 0.32605308, -0.05143709, -0.10589121,  0.31434519,  0.02016726,\n",
            "         0.08877521],\n",
            "       [-0.21774635,  0.03866937, -0.47302561,  0.46598926, -0.21588285,\n",
            "        -0.00519598],\n",
            "       ...,\n",
            "       [ 0.42610659, -0.39996743,  0.39639939,  0.08795844, -0.41117158,\n",
            "         0.25592893],\n",
            "       [ 0.22798029,  0.44619233,  0.35035735, -0.24295682, -0.31434818,\n",
            "        -0.34151715],\n",
            "       [ 0.0422646 , -0.03565507, -0.00345246,  0.09051397,  0.12823457,\n",
            "        -0.18662855]]), 'y_train': array([-0.5444078 , -0.4370128 , -0.52015667, -0.66935155,  0.08819211,\n",
            "       -0.31627398,  0.24984238,  0.45209291, -0.63849557, -0.4685491 ,\n",
            "        0.16460064, -0.3187767 ,  0.21146661, -0.7578453 , -0.23884014,\n",
            "       -0.34178193,  0.35233731, -0.30587837,  0.42547074, -0.77724901,\n",
            "        0.00952547, -0.03515573,  0.68911946, -0.14336058, -0.35207638,\n",
            "        0.0277231 , -0.6915551 ,  0.00498744, -0.39166194, -0.68468955,\n",
            "       -0.55699016,  0.66477405, -0.86733802, -0.21677593, -0.75805327,\n",
            "       -0.10943468, -0.29961671, -0.62368514, -0.0976487 , -0.06020683,\n",
            "        0.32440349,  1.35565609,  0.16989131,  1.19021561, -0.17450455,\n",
            "       -0.3715282 ,  0.96601028, -0.03167259, -0.86586859,  1.08874885,\n",
            "        0.28691955, -0.28455398,  0.32965077,  0.66170739,  0.52109175,\n",
            "        0.43509111,  0.76061589,  0.03742454,  0.02208982,  1.27140725,\n",
            "       -0.78028493, -0.14296486,  0.13410959, -0.20375612, -0.21864261,\n",
            "        0.19430252, -1.03994177, -0.17003301, -0.32454464,  0.50170725,\n",
            "       -0.42904338, -0.72662557,  1.38746881,  1.60447943, -0.52559143,\n",
            "        0.04956464,  0.4675676 , -0.39246121,  0.36808254,  0.51300479,\n",
            "       -0.21878638, -0.50794314,  1.33694457, -0.10009998, -1.25462239,\n",
            "       -0.63375003, -0.74577121, -0.30618475, -0.21532838,  0.06582723,\n",
            "        0.30370614, -0.2709623 ,  0.57354257, -0.1765881 ,  0.43357323,\n",
            "       -0.3034806 , -0.37450937,  0.61656151,  0.08021054, -1.03685383]), 'y_test': array([ 0.37438298, -0.19977561,  0.13187055, ..., -1.06351701,\n",
            "        0.18256635,  0.37927543]), 'imp_ds': {'BR_si': [], 'l_d': [], 'oracle': [], 'mi': []}, 'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg_max': 1, 'eps_adv_rad_times_delta_dts': 1e-05, 'eps_adv_rad_times_delta_mis': 1e-05, 'eps_alpha_ridge_reg': 1e-05, 'n_a_dts': 30, 'n_a_mis': 30, 'n_a_rid': 1}}\n",
            "----------------------------------------------> new method tested:  {'imp_method': 'BR_si', 'cov_strategy': 'sd', 'algo_superv_learn': 'adv', 'color': 'b'}\n",
            "-------> ORACLE SD, std of the original dataset (with no missing) [0.2703752  0.25696074 0.28750158 0.29708937 0.28776028 0.28322079]\n",
            "NO PREVIOUS IMPUTATION HAS BEEN DONE\n",
            "crush test------------------------------------------------->  -12.260463081095942\n",
            "[1 2 1 2 2 2 0 0 1 2 1 2 2 2 2 2 0 1 2 2 2 2 1 0 1 0 2 2 3 1 2 1 1 3 1 0 2\n",
            " 2 3 1 2 0 2 3 1 0 0 2 1 4 0 2 0 3 2 4 1 2 1 1 1 0 0 0 1 1 0 0 1 1 3 1 3 1\n",
            " 1 1 1 2 1 3 1 0 1 2 0 2 3 0 2 1 3 1 2 0 1 1 1 1 3 0]\n",
            "S dataset \n",
            " [[0.22955862 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.22529171 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.23698748 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.26179551 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.25309803 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.25265991]]\n",
            "shape oject in cov strategy missing  6\n",
            "shape oject in cov strategy missing  (20000, 6)\n",
            "S missing shape\n",
            "  (6, 6)\n",
            "S missing\n",
            "  [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "shape X_imputed in post_imputation  (100, 6)\n",
            "y_train length  100\n",
            "-------> size test:  20000  , size train:  100 nbr_full_seen (train):  21  nbr_at_least_one_miss :  79\n",
            "X  100   6\n",
            "y shape (100,)\n",
            "nm  600\n",
            "S_mis in Adbvt training  [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "no missing part\n",
            "one matrix in input, S.shape = (n, n)\n",
            "dts deltas  [1.00000000e-05 1.48735211e-05 2.21221629e-05 3.29034456e-05\n",
            " 4.89390092e-05 7.27895384e-05 1.08263673e-04 1.61026203e-04\n",
            " 2.39502662e-04 3.56224789e-04 5.29831691e-04 7.88046282e-04\n",
            " 1.17210230e-03 1.74332882e-03 2.59294380e-03 3.85662042e-03\n",
            " 5.73615251e-03 8.53167852e-03 1.26896100e-02 1.88739182e-02\n",
            " 2.80721620e-02 4.17531894e-02 6.21016942e-02 9.23670857e-02\n",
            " 1.37382380e-01 2.04335972e-01 3.03919538e-01 4.52035366e-01\n",
            " 6.72335754e-01 1.00000000e+00]\n",
            "mis deltas  [0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 15/30 [00:00<00:00, 149.25it/s]\u001b[A\n",
            "100%|██████████| 30/30 [00:00<00:00, 142.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n",
            "/tmp/ipython-input-4-29649289.py:374: RuntimeWarning: divide by zero encountered in log10\n",
            "  return best_coeff, min_score, -np.log10(best_hyper_p)  # -np.log10((best_alpha_delta_dts + best_alpha_delta_mis)/2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test shape,  (20000, 6) ,   y_test shape  (20000,)\n",
            "---------------------------------> best idx  0  best hyperp [best_alpha_delta_dst, best_alpha_delta_mis]:  [1.e-05 0.e+00] , min score  0.011485794862196363\n",
            "---------------------------------> best coeff  [-0.62699187  0.35178758  0.526342    0.90770148  1.01773339 -1.20572901]\n",
            "BR_si\n",
            "sd\n",
            "adv\n",
            "b\n",
            "----------------------------------------------> new method tested:  {'imp_method': 'BR_si', 'cov_strategy': 'sd', 'algo_superv_learn': 'ridge', 'color': 'k'}\n",
            "-------> ORACLE SD, std of the original dataset (with no missing) [0.2703752  0.25696074 0.28750158 0.29708937 0.28776028 0.28322079]\n",
            "A PREVIOUS IMPUTATION HAS BEEN DONE\n",
            "crush test------------------------------------------------->  -12.260463081095942\n",
            "[1 2 1 2 2 2 0 0 1 2 1 2 2 2 2 2 0 1 2 2 2 2 1 0 1 0 2 2 3 1 2 1 1 3 1 0 2\n",
            " 2 3 1 2 0 2 3 1 0 0 2 1 4 0 2 0 3 2 4 1 2 1 1 1 0 0 0 1 1 0 0 1 1 3 1 3 1\n",
            " 1 1 1 2 1 3 1 0 1 2 0 2 3 0 2 1 3 1 2 0 1 1 1 1 3 0]\n",
            "S dataset \n",
            " [[0.22955862 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.22529171 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.23698748 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.26179551 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.25309803 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.25265991]]\n",
            "shape oject in cov strategy missing  6\n",
            "shape oject in cov strategy missing  (20000, 6)\n",
            "S missing shape\n",
            "  (6, 6)\n",
            "S missing\n",
            "  [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "shape X_imputed in post_imputation  (100, 6)\n",
            "y_train length  100\n",
            "-------> size test:  20000  , size train:  100 nbr_full_seen (train):  21  nbr_at_least_one_miss :  79\n",
            "rid alphas  [1.e-05]\n",
            "S_dts_inv in get path, ridge regression \n",
            " [[0.22955862 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.22529171 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.23698748 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.26179551 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.25309803 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.25265991]]\n",
            "S_dts_inv in get path, ridge regression \n",
            " [[4.35618575 0.         0.         0.         0.         0.        ]\n",
            " [0.         4.43868981 0.         0.         0.         0.        ]\n",
            " [0.         0.         4.21963223 0.         0.         0.        ]\n",
            " [0.         0.         0.         3.81977526 0.         0.        ]\n",
            " [0.         0.         0.         0.         3.95103822 0.        ]\n",
            " [0.         0.         0.         0.         0.         3.95788946]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 684.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test shape,  (20000, 6) ,   y_test shape  (20000,)\n",
            "---------------------------------> best idx  0  best hyperp [best_alpha_delta_dst, best_alpha_delta_mis]:  [1.e-05 0.e+00] , min score  0.011485614540483044\n",
            "---------------------------------> best coeff  [-0.62699635  0.35178682  0.52634717  0.90770815  1.01773315 -1.20574003]\n",
            "BR_si\n",
            "sd\n",
            "ridge\n",
            "k\n",
            "----------------------------------------------> new method tested:  {'imp_method': 'oracle', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'adv', 'color': 'orange'}\n",
            "-------> ORACLE SD, std of the original dataset (with no missing) [0.2703752  0.25696074 0.28750158 0.29708937 0.28776028 0.28322079]\n",
            "NO PREVIOUS IMPUTATION HAS BEEN DONE\n",
            "crush test------------------------------------------------->  -11.68850518536391\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "DON'T USE std_nan with oracle and ld because you do not have any nan. Use sd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-4-29649289.py:374: RuntimeWarning: divide by zero encountered in log10\n",
            "  return best_coeff, min_score, -np.log10(best_hyper_p)  # -np.log10((best_alpha_delta_dts + best_alpha_delta_mis)/2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'S' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-4034831012.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0mnbr_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m67\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mmean_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_multiple_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_x_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLOT OF THE MEANS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_infer_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'seed: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', nbr_exp: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_exp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', data: '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', dim: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ', cov: ' + str(cov_var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-147721701.py\u001b[0m in \u001b[0;36mrun_multiple_experiments\u001b[0;34m(nbr_exp, rdm_seed, dictio, info_x_axis)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;31m#rdm_seed = 4654321\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdm_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_methods_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m   \u001b[0mplot_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_x_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   '''\n",
            "\u001b[0;32m/tmp/ipython-input-5-147721701.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(dictio, methods_strategy)\u001b[0m\n\u001b[1;32m     95\u001b[0m           \u001b[0mcoeff_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_p_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_2d_ext_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_obser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_imp_cov_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnbr_ima\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnbr_ima\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# == 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m           \u001b[0mcoeff_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_p_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_2d_ext_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_obser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_imp_cov_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff_round\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta_gt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0ml2_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-29649289.py\u001b[0m in \u001b[0;36mexperiment_2d_ext_dataset\u001b[0;34m(dict_obs, dict_imp, ax)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mdict_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_obs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'X_imputed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_imputed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_from_X_imputed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_from_X_imputed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'masks_after_imputation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmask_from_X_imputed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m#  print(dict_obs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mS_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_imp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#* dict_imp['multip_dataset']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"S dataset \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;31m#  dict_obs = dict_obs | {'cov_within': S_within}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-29649289.py\u001b[0m in \u001b[0;36mcov_strategy\u001b[0;34m(info, dict_observations)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0;31m#mu = np.mean(X_imputed, axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;31m#sigma = np.cov(X_imputed, rowvar=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'S' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DJHHiKFqIUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FhKy2v5qIb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "840TDfGaqIei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SijOuiZYgmon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TG_Fsg4Mgmrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRfmkoYlgmuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vbxlxxNgmyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zJgEeongm1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkEtCuWEgm3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjbzANmcg_cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JsQXdlOg_fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4FH2Zo7g_j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxm1VFuIg_q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cxU4-RZg_uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktlqpLJ9g_ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doiemfrVg_zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bzoCpqTg_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwoz_Lp2g_4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-Kc_0IPg_7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMqppMFSgm5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSeaLOr-qLYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azRmYEueqLbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4l2vYHzqLd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7X3AQIZqLgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iv34YkpnqLju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCRKDvv-qLmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJ6b5Zd_vZ8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7swocNpvZ_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cxt5tWj1vaCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCvPWMOtvaE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9kxssOuRMv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWcLfNW2RM1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S5O3hTLTvaIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fE8VHJ90vaMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZnN2xpSvaOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze5Q9M4Tycsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzWhOlCoycwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZMXiJW-ycz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAINCofcyc20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uORnN_wO05n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQ441A0hO09K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iF_8aBrWO1Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHLiMt5wO1Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gB_XEEFvaQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFwxXhyWefs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Kw8bLBTefv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXN7C2Jfefzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeFvCalref4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "An113TsYef7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kF5dEO1zef-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zuC2em6egA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MM7Zk7OWegDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVL97vbaegFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA3cOcmeegJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FpYA6ClegNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3MxLnLIqLol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DgSOhmwgm7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSdrq6HmqIhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4nfPNbTqIjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4xXWwHeeMR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oonp7YBzeMUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnVLZhvbeMXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9Yk_s6leMZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5asNezNqImF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwGCYcYWqIrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6dLlbTgqIt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lu0iNCNHc_0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "031VAAc5c_4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quVErgChc_-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVqmuefndAEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7M3O9KqdAL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LN_xYsFMdAQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wo4YT1OODeeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wl-gtIlyDeh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnKMsLPgDemY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKCZUoDYDesF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1vpjYZ9dAVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1Fx16kedAX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randint(2, 5, size=(2, 2, 2))\n",
        "print(X)\n",
        "\n",
        "XX = np.concatenate(X)\n",
        "print(XX)\n",
        "\n",
        "\n",
        "Y = np.random.randint(2, 5, size=(1, 3, 2))\n",
        "print(Y)\n",
        "\n",
        "YY = np.concatenate(Y)\n",
        "print(YY)\n",
        "\n",
        "\n",
        "Z = np.random.randint(2, 5, size=(5, 2))\n",
        "print(Z)\n",
        "\n",
        "ZZ = np.concatenate(Z)\n",
        "print(ZZ)\n",
        "\n",
        "print(\"other\")\n",
        "s = np.random.randint(2, 4, 5)\n",
        "print(s)\n",
        "z = np.tile(s, reps=3)  # np.array([s] * 2)\n",
        "print(z)\n",
        "\n",
        "\n",
        "print(\"other mult\")\n",
        "s = np.random.randint(2, 8, size=(3, 2))\n",
        "print(s)\n",
        "z = np.tile(s, reps=(3, 1))  # np.array([s] * 2)\n",
        "print(z)\n"
      ],
      "metadata": {
        "id": "J-KLwpDqTkGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBkG1_lacBfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBILRQvDuI4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dPT52NAbKyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2ShK9JYb6c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fk7A_5N_c1gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgcEt2LBhEBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FiP-uNujLRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6uENE-JShLaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JeqAEblFooY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXByx8OrjqZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4TCzI5siopUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4pGls4IpDT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWpiTpQ5lVg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ASCmfdEnvjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jq6GembmmAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nb6YB8DbvKVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9pv_OW7pJtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgNt4tVYQk7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlebwxRZ1_QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1dmiXA-FcI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDRrQMKGU3Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnNTv-mXVIB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBlyABpt0-qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQgiJb-V-hIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCuJj9HPb2cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VM2QQJkmcsdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## random forest imputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_estimator = RandomForestRegressor(n_estimators=4, max_depth=10, bootstrap=True, max_samples=0.5, n_jobs=2, random_state=0)\n",
        "\n",
        "X_rf = single_imputation(X_nan, rf_estimator)\n",
        "print(X_rf.shape)\n",
        "sd_rf = np.std(X_rf, axis=0)\n",
        "S_inv_rf = np.diag(1 / sd_rf)\n",
        "print(\"std_orig: \\n\", np.std(X_orig, axis=0))\n",
        "print(\"std rf\\n \", sd_rf)\n",
        "fig, ax = plt.subplots(num='advtrain_linf_rf')\n",
        "linfadvtrain_rf = AdversarialTraining(X_rf, y, S_inv_rf, p=np.inf)\n",
        "estimator_rf = lambda X, y, a:  linfadvtrain_rf(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_rf  = get_path(X_rf, y, estimator_rf, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_rf, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "uSgnV3aVXL1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## iterative imputer Bayesian Ridge\n",
        "\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "br_estimator = BayesianRidge()\n",
        "\n",
        "X_br = single_imputation(X_nan, br_estimator)\n",
        "sd_br = np.std(X_br, axis=0)\n",
        "S_inv_br = np.diag(1 / sd_br)\n",
        "print(\"std_orig: \\n\", np.std(X_orig, axis=0))\n",
        "print(\"std  br\\n \", sd_br)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_br')\n",
        "linfadvtrain_br = AdversarialTraining(X_br, y, S_inv_br, p=np.inf)\n",
        "estimator_br = lambda X, y, a:  linfadvtrain_br(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_br  = get_path(X_br, y, estimator_br, 1e4)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_br, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "pgNaP74gWAga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## mean imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X_mean = imp_mean.fit_transform(X_nan)\n",
        "sd_mean = np.std(X_mean, axis=0)\n",
        "print(sd_mean)\n",
        "S_inv_mean = np.diag(1 / sd_mean)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_mean')\n",
        "linfadvtrain_mean = AdversarialTraining(X_mean, y, S_inv_mean, p=np.inf)\n",
        "estimator_mean = lambda X, y, a:  linfadvtrain_mean(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_mean  = get_path(X_mean, y, estimator_mean, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_mean, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "u0kpCJCkFbcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# imputation elliptic\n",
        "\n",
        "mu = np.nanmean(X_nan, axis=0)\n",
        "print(\"means \", mu)\n",
        "delta = np.mean(masks) # parameter missingness\n",
        "print(\"delta \", delta)\n",
        "X_0 = np.nan_to_num(X_nan)\n",
        "print(\"nbr obs\", X_0.shape[0])\n",
        "S_ellp =  X_0.T @ X_0 / X_0.shape[0]\n",
        "S_ellp = (1/delta - 1/(delta**2)) * np.diag(np.diag(S_ellp)) + 1/(delta**2) * S_ellp\n",
        "print(\"eig cov \", np.linalg.eigvalsh(S_ellp))\n",
        "X_ellp = imputation_elliptic(mu, S_ellp, X_nan, masks)\n",
        "#S_inv_ellp = np.linalg.inv(S_ellp)  # other variance\n",
        "sd_inv_ellp = np.std(X_ellp, axis=0)\n",
        "print(\"sd ellp\", sd_inv_ellp)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_ellp')\n",
        "linfadvtrain_ellp = AdversarialTraining(X_ellp, y, S_ellp, p=np.inf)\n",
        "estimator_ellp = lambda X, y, a:  linfadvtrain_ellp(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_ellp  = get_path(X_ellp, y, estimator_ellp, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_ellp, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "2RYR4_BJhXjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mlM-FR-OfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgxEbR071wT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pwDiPU0D_ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BdM7Mk_mjf0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYa8pmuMk4jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMwgzXI1_rEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Example data\n",
        "x_test_rect = np.random.rand(10)\n",
        "y_test_rect = np.random.rand(10)\n",
        "\n",
        "# Plot the points\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x_test_rect, y_test_rect)\n",
        "\n",
        "width = 0.1\n",
        "height = 0.1\n",
        "\n",
        "#add_rectangles(x_test_rect, y_test_rect, width, height, ax)\n",
        "\n",
        "# Add the rectangle to the plot\n"
      ],
      "metadata": {
        "id": "-9qaVcwZUB6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WZeO2EOWHwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell for some tests\n",
        "\n",
        "def test_clear_dataset(n, d):\n",
        "  print(\"test clear dataset\")\n",
        "  X = np.random.randint(1, 3, size=(n, d))\n",
        "  y = np.random.randint(1, 3, size=n)\n",
        "  masks = np.random.binomial(1, 0.3, size=(n, d))\n",
        "  print(\"X \\n\", X)\n",
        "  print(\"y\\n\", y)\n",
        "  print(\"masks \\n\", masks)\n",
        "  masks[:, 0] = np.ones(n)\n",
        "  masks[0, :] = np.ones(d)\n",
        "  X_res, y_res, masks_res = clear_dataset(X, y, masks)\n",
        "  print(\"X_res \\n\", X_res)\n",
        "  print(\"y\\n\", y_res)\n",
        "  print(\"masks \\n\", masks_res)\n",
        "  print(\"test clear dataset ended successfully\")\n",
        "\n",
        "def test_generate_X():\n",
        "    print(\"test generate_X started\")\n",
        "    fig, ax = plt.subplots(3, 1, figsize=(10, 8), num='advtrain_linf')\n",
        "    gen = generate_X('circles', 2)\n",
        "    data = gen(1000)\n",
        "    print(data.shape)\n",
        "    ax[0].scatter(data[:, 0], data[:, 1])\n",
        "    print(\"test generate passed syccessfully\")\n",
        "\n",
        "def test_preparation_dataset(n, d):\n",
        "      print(\"\\ntest preparation dataset started\")\n",
        "      X_train = np.random.rand(n, d)\n",
        "      print(\"X_train \\n\", X_train)\n",
        "      mask = np.random.binomial(1, 0.5, (n, d))\n",
        "      print(\"mask, 0 seen, 1 missing \\n \", mask)\n",
        "      X_masked = X_train * (1 - mask)\n",
        "      print(\"X_masked \\n\", X_masked)\n",
        "      X_nan_train = X_train.copy()\n",
        "      X_nan_train[mask == 1] = np.nan\n",
        "      print(\"X_nan_train \\n\", X_nan_train)\n",
        "      X_br_train = single_imputation(X_nan_train, BayesianRidge())\n",
        "      print(\"X_br_train\\n \", X_br_train)\n",
        "\n",
        "      print(\"what happens if we run single_imputation of full dataset\")\n",
        "      X_br_full = single_imputation(X_train, BayesianRidge())\n",
        "      print(\"X_br_full\\n \", X_br_full)\n",
        "      np.testing.assert_allclose(X_train, X_br_full)  # shuold be untouched\n",
        "      print(\"test preparation dataset ended successfully\")\n",
        "\n",
        "def test_listwise_delection(n, d):\n",
        "    print(\"\\n test list_wise delection started\")\n",
        "    X = np.random.rand(n, d)\n",
        "    print(\"data\\n\", X)\n",
        "    mask = np.random.binomial(1, 0.2, (n, d))\n",
        "    print(\"mask \\n\", mask)\n",
        "    X_ld = listwise_delection(X, mask)\n",
        "    print(\"after calling function, X_ld \\n\", X_ld)\n",
        "\n",
        "    print(\"edge cases, all missing\")\n",
        "    mask_1 = np.ones_like(X)  # all missing\n",
        "    X1 = listwise_delection(X, mask_1)\n",
        "    print(\"X1 \\n\", X1)  # should be empty\n",
        "    mask_0 = np.zeros_like(X)  # all seen\n",
        "    X0 = listwise_delection(X, mask_0)\n",
        "    print(\"X0 \\n\", X0)\n",
        "    np.testing.assert_allclose(X0, X)  # should be the original dataset\n",
        "\n",
        "    print(\"one dimnsional array\")\n",
        "    y = np.random.rand(n)\n",
        "    print(\"y before \", y)\n",
        "    y_ld = listwise_delection(y, mask)\n",
        "    print(\"y after ld \", y_ld)\n",
        "    print(\"test listwise_delection passed\")\n",
        "\n",
        "\n",
        "test_generate_X()\n",
        "test_preparation_dataset(3, 4)\n",
        "test_listwise_delection(3, 4)\n",
        "test_clear_dataset(6, 3)\n",
        "\n",
        "xxx = np.random.randint(2, 5, size=(3, 3)) * 1.0\n",
        "mmm = np.random.binomial(1, 0.5, size=(3, 3))\n",
        "print(xxx)\n",
        "print(mmm)\n",
        "print(mmm == 1)\n",
        "print(xxx[mmm == 1])\n",
        "xxx[mmm == 1] = np.nan\n",
        "print(xxx)\n",
        "mask_from_xxx = np.isnan(xxx).astype(int)\n",
        "print(\"mask from xxx \\n\", mask_from_xxx)\n"
      ],
      "metadata": {
        "id": "SDHMAeapZVgK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test best predictor\n",
        "\n",
        "def test_best_predictor(n, d, nb_coeff):\n",
        "  X_test = np.random.randint(1, 9, size=(n, d))\n",
        "  beta_gt_test = np.random.randint(1, 7, size=d)\n",
        "  y_test = X_test @ beta_gt_test\n",
        "  #print(\"X_test \\n\", X_test, \"\\n beta_gt\", beta_gt_test, \"\\n y_test = X_test @ beta_gt_test \", y_test)\n",
        "  coeff_test = np.random.randint(1, 5, size=(d, nb_coeff))\n",
        "  rdm_idx = np.random.randint(1, d+1, size=1)\n",
        "  print(rdm_idx)\n",
        "  #print(\"coeff test partial \", coeff_test[:, -1])\n",
        "  rng = np.arange(nb_coeff)\n",
        "  #print(rng != rdm_idx)\n",
        "  coeff_test[:, rng != rdm_idx] = coeff_test[:, rng != rdm_idx] + 1000  # increase artificially the value of the other coefficient, to induce the minimum index to be rdm_idx\n",
        "  #print(\"coeff_test \\n\", coeff_test)\n",
        "  best_coeff, best_score = best_predictor(X_test, coeff_test, y_test)\n",
        "  print(\"best coeff \", best_coeff)\n",
        "  print(\"best score \", best_score)\n",
        "  np.testing.assert_allclose(best_coeff, coeff_test[:,rdm_idx].squeeze())\n",
        "  print(\"test best predictor passed\")\n",
        "\n",
        "test_best_predictor(100, 5, 20)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YJk1Yaj1ReIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test train_and_plot\n",
        "\n",
        "X_diab, y_diab = datasets.load_diabetes(return_X_y=True)\n",
        "n, d = X_diab.shape\n",
        "print(\"n:  \", n, \", d: \", d)\n",
        "# Standardize data\n",
        "X_diab -= X_diab.mean(axis=0)\n",
        "X_diab /= X_diab.std(axis=0)\n",
        "\n",
        "## original lasso\n",
        "fig_l, ax_l = plt.subplots(num='lasso')\n",
        "alphas_lasso, coefs_lasso, _ = get_lasso_path(X_diab, y_diab)\n",
        "plot_coefs_l1norm(coefs_lasso, ax_l)\n",
        "\n",
        "## Antonio's algo, 1 matrix\n",
        "S_diab_eye = np.eye(X_diab.shape[1])\n",
        "fig, ax_1 = plt.subplots(1, 1, num='advtrain_linf_diab')\n",
        "fig, ax_2 = plt.subplots(1, 1, num='advtrain_linf_diab_2')\n",
        "train_and_plot(X_diab, y_diab, S_diab_eye, [ax_1, ax_2])\n",
        "\n",
        "## Antonio's algo, multiple diagonal matrix\n",
        "#S_diab = np.eye(X_diab.shape[1])\n",
        "#S_diab = np.random.randint(1, 3, size=(n, d))\n",
        "#print(S_diab)\n",
        "#fig, ax_5 = plt.subplots(1, 1, num='advtrain_linf_diab_5')\n",
        "#fig, ax_6 = plt.subplots(1, 1, num='advtrain_linf_diab_6')\n",
        "#train_and_plot(X_diab, y_diab, S_diab, [ax_5, ax_6])\n",
        "\n",
        "\n",
        "## Antonio's algo, multiple matrices (same matrix stacked multiple time)\n",
        "S_diab_stacked = np.array([S_diab_eye] * X_diab.shape[0])\n",
        "S_diab_stacked = np.concatenate(S_diab_stacked)\n",
        "fig, ax_3 = plt.subplots(1, 1, num='advtrain_linf_diab_3')\n",
        "fig, ax_4 = plt.subplots(1, 1, num='advtrain_linf_diab_4')\n",
        "train_and_plot(X_diab, y_diab, S_diab_stacked, [ax_3, ax_4])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KjpHk0mYdiFh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test imputations\n",
        "\n",
        "np.random.seed(45)\n",
        "\n",
        "\n",
        "def test_imputations(n, d):\n",
        "  X = np.random.randint(2, 5, size=(n, d)) * 1.0\n",
        "  y = X @ np.random.randint(1, 3, size=d)\n",
        "  m = np.random.binomial(1, 0.4, size=(n, d))  # 1 missing, 0 seen\n",
        "  print(\"m original\\n\", m)\n",
        "  X, y, m = clear_dataset(X, y, m)\n",
        "  print(m)\n",
        "  X_nan = X.copy()\n",
        "  X_nan[m == 1] = np.nan\n",
        "\n",
        "  #mask_from_xxx = np.isnan(xxx).astype(int)\n",
        "  print(\"X\\n \", X)\n",
        "  print(\"masks \\n\", m)\n",
        "  print(\"X_nan\\n \", X_nan)\n",
        "  methods = ['BR_si', 'mi', 'l_d']\n",
        "  nbr_mi = [1, 3]\n",
        "  #for method in methods:\n",
        "  #  dict_info = {'imp_method': method, 'mi_nbr':nbr_mi}\n",
        "  #dict_info = {'imp_method':methods, 'mi_nbr':nbr_mi}\n",
        "  for method in methods:\n",
        "    print(\"---------- method: \", method)\n",
        "    if method == 'mi':\n",
        "      for x in nbr_mi:\n",
        "        print(\"-------------------- nbr mi: \", x)\n",
        "        dict_info = {'imp_method':method, 'mi_nbr':x}\n",
        "        #print(\"XNANNANAN \", X_nan)\n",
        "        X_res, y_res, mask_res = imputations(dict_info, X_nan, y)\n",
        "        print(X_res, y_res, \"\\n\", mask_res)\n",
        "    else:\n",
        "      dict_info = {'imp_method': method}\n",
        "      X_res, y_res, mask_res = imputations(dict_info, X_nan, y)\n",
        "      print(X_res, y_res, \"\\n\", mask_res)\n",
        "    print(\"test imputations ended successfully\")\n",
        "\n",
        "test_imputations(6, 3)\n"
      ],
      "metadata": {
        "id": "z5crxb1usyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "y = np.array([1, 2])\n",
        "x.append(y)\n",
        "x.append(y)\n",
        "x.append(y)\n",
        "xx = np.stack(x)\n",
        "print(x)\n",
        "print(xx)\n",
        "print(type(xx))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sizes = [100, 1000, 10000, 100000]\n",
        "values = [0.8, 0.85, 0.9, 0.92]\n",
        "positions = range(len(sizes))\n",
        "\n",
        "plt.plot(positions, values, marker='o', label='Model Accuracy')  # Add label here\n",
        "plt.xticks(positions, sizes)\n",
        "\n",
        "plt.xlabel(\"Dataset Size (equispaced)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Performance vs Dataset Size (equispaced x-axis)\")\n",
        "#plt.legend()  # Show legend\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GU2RjW63SNaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dictio = {'a':1, 'b':2, 'c':3}\n",
        "vv = dictio.values()\n",
        "#print(vv)\n",
        "#print(vv[1])\n",
        "\n",
        "x1 = np.array([1, 2, 3])\n",
        "x2 = np.array([3, 2 ,1])\n",
        "v = np.maximum(x1, x2)\n",
        "print(v)\n"
      ],
      "metadata": {
        "id": "UE1NuR4D2h8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "648zfFp8ERD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n, d = 2, 3, 2\n",
        "x_int = np.random.randint(1, 9, (m, n, d))\n",
        "print(x_int)\n",
        "s = np.std(x_int, axis=0)\n",
        "print(s)\n",
        "\n",
        "# manual\n",
        "print(\"manual computation\")\n",
        "x = np.zeros((m, d))\n",
        "for i in range(n):\n",
        "  print(\"i -----> \", i)\n",
        "  x = x_int[:, i, :]\n",
        "  print(\"x\\n\", x)\n",
        "  ss = np.std(x, axis=0)\n",
        "  print(ss)\n",
        "\n",
        "\n",
        "print(\"little exp on squeeze\")\n",
        "sss = np.random.rand(1, 3, 3)\n",
        "print(sss)\n",
        "print(sss.squeeze())\n",
        "print(sss.squeeze())\n",
        "\n"
      ],
      "metadata": {
        "id": "vpBvPibeERnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int(34.99)\n",
        "\n",
        "xxxx = np.random.randint(2, 4, (5, 2))\n",
        "print(xxxx)\n",
        "xxxx[0:2, :] = 1\n",
        "print(xxxx)\n",
        "\n",
        "print(\"yyyy\\n\")\n",
        "yy = []\n",
        "yy.append([1, 2, 3])\n",
        "yy.append([4, 5, 6])\n",
        "print(yy)\n",
        "print(np.stack( yy ).T)\n",
        "print(\"\\n\\n\")\n",
        "yyy = np.random.randint(1, 10, size=(3 , 3))\n",
        "print(yyy)\n",
        "yyy_a = np.array([yyy] * 2)\n",
        "print(yyy_a.shape)\n",
        "print(np.concatenate([yyy] * 2))\n",
        "#print(np.tile(yyy_a, (2, 1, 1) ))\n",
        "\n",
        "zzz = np.zeros((2, 2))\n",
        "\n",
        "np.sum(np.zeros((2, 2)) == zzz)"
      ],
      "metadata": {
        "id": "et578OpzERsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def multiple_imputation1(nbr_mi, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    res = np.zeros((nbr_mi, n, d))\n",
        "    for i in range(nbr_mi):\n",
        "       n_i = np.random.randint(0, 1000)\n",
        "       ice = IterativeImputer(random_state=n_i, max_iter=50, sample_posterior=True)\n",
        "       res[i, :, :] = ice.fit_transform(X_nan)\n",
        "       #print(\"fin res shape\", res.shape)\n",
        "       #if nbr_mi == 1:\n",
        "        #res = res[0, :, :]\n",
        "        #print(\"fin res shape\", res.shape)\n",
        "    return res\n",
        "\n",
        "\n",
        "Xx = np.random.randint(1, 3, (4, 4)) * 1.0\n",
        "mm = np.random.binomial(1, 0.25, (4, 4))\n",
        "print(Xx)\n",
        "print(mm)\n",
        "Xx[mm == 1] = np.nan\n",
        "print(Xx)\n",
        "\n",
        "ice = IterativeImputer(random_state=18, max_iter=50, sample_posterior=True)\n",
        "ice.fit(Xx)\n",
        "XxX = np.random.randint(1, 3, (2, 4)) * 1.0\n",
        "mmM = np.random.binomial(1, 0.5, (2, 4))\n",
        "print(XxX)\n",
        "print(mmM)\n",
        "XxX[mmM == 1] = np.nan\n",
        "print(XxX)\n",
        "\n",
        "print(ice.transform(XxX))\n",
        "print(ice.transform(XxX))\n",
        "\n",
        "print(\"new\")\n",
        "\n",
        "ls = [[[]],[[]]]\n",
        "print(ls)\n",
        "ls[0][0]\n",
        "\n"
      ],
      "metadata": {
        "id": "_U_r_qaJ9pw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "XX = np.random.randint(1, 7, (2, 3, 3))\n",
        "print(XX)\n",
        "XXX = np.tile(XX, (2, 1, 1))\n",
        "print(XXX)\n",
        "\n",
        "print(np.zeros(2))\n",
        "\n",
        "y_o = np.array([1, 2])\n",
        "y_oo = np.tile(y_o, 3)\n",
        "print(y_oo)\n"
      ],
      "metadata": {
        "id": "PXfceAK8es4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def generate_masks_binomial_general(nbr_of_sample, p_missing):\n",
        "    # nbr_of_sample is the number of masks\n",
        "    # p_missing=[p00, p01, p10], where p00 is the probability of seeing both components,\n",
        "    # p10 is the probability of seeing the right component, p01 is the probability of seeing the left component\n",
        "    masks = np.zeros((nbr_of_sample, 2))\n",
        "    v = np.random.choice(a=3, size=nbr_of_sample, p=p_missing)\n",
        "    masks[v == 0, :] = np.array([0, 0])  # both seen\n",
        "    masks[v == 1, :] = np.array([0, 1])  # left seen\n",
        "    masks[v == 2, :] = np.array([1, 0])  # right seen\n",
        "    return masks\n",
        "\n",
        "\n",
        "#mm = np.random.binomial(1, [[0.2, 0.2, 0.2], [0.8, 0.8, 0.8]], (2, 3, 3))\n",
        "#print(mm)\n",
        "cc = np.array([np.random.binomial(1, x, (4, 4)) for x in [0.2, 0.2, 0.2]])\n",
        "print(cc)\n",
        "s_cc = np.cumsum(cc, axis=0)\n",
        "print(s_cc)\n",
        "s_cc[s_cc>1] = 1\n",
        "print(s_cc)\n",
        "\n",
        "s_v = np.random.randint(1, 4, (3, 4))\n",
        "print(s_v)\n",
        "s_vv = s_v[:, None, :] * np.eye(4)\n",
        "print(s_vv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "auugsvFPZ88A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import ridge_regression\n",
        "\n",
        "X = np.random.randn(100, 4) #rng.randn(100, 4)\n",
        "\n",
        "y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * np.random.randn(100)\n",
        "np.random.seed(4)\n",
        "alphas = [0.00001, 0.001, 0.1, 1]\n",
        "estim = lambda XX, yy, rad: ridge_regression(XX, yy, alpha=rad, return_intercept=False, random_state=0)\n",
        "for a in alphas:\n",
        "  #coef, intercept = estim(X, y, a)\n",
        "  coef = estim(X, y, a)\n",
        "  print(\"coef : \", coef)\n",
        "  #print(\"intercpt \", intercept)\n",
        "  coef, intercept = ridge_regression(X, y, alpha=a, return_intercept=True, random_state=0)\n",
        "  #print(\"coef : \", coef)\n",
        "  #print(\"intercpt \", intercept)\n",
        "\n",
        "\n",
        "lg = np.random.logistic(loc=0.0, scale=1.0, size=(4, 3))\n",
        "print(\"log res \", lg)\n",
        "\n"
      ],
      "metadata": {
        "id": "t3KaZyJKwnRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}