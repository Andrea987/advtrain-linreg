{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea987/advtrain-linreg/blob/main/notebooks/fig1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Sgo-CifolM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ggDSA-ktpXgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07918fc2-8027-4682-b8a1-954c7343c8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CLARABEL', 'CVXOPT', 'GLPK', 'GLPK_MI', 'HIGHS', 'OSQP', 'SCIPY', 'SCS']\n",
            "end block\n"
          ]
        }
      ],
      "source": [
        "from re import VERBOSE\n",
        "from itertools import cycle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.linear_model import lasso_path\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import tqdm\n",
        "import cvxpy as cp\n",
        "print(cp.installed_solvers())\n",
        "import numpy as np\n",
        "\n",
        "import traceback\n",
        "\n",
        "\n",
        "def compute_q(p):\n",
        "    if p != np.inf and p > 1:\n",
        "        q = p / (p - 1)\n",
        "    elif p == 1:\n",
        "        q = np.inf\n",
        "    else:\n",
        "        q = 1\n",
        "    return q\n",
        "\n",
        "\n",
        "class AdversarialTraining:\n",
        "    def __init__(self, X, y, S_dict, p):  # S is the matrix such that ||S^(-1) @ Dx||\\leq delta. As a consequence, S appears in the unconstrained problem\n",
        "        # S: (n, n) matrix, or S = np.concatenate(tS), with tS = [S1,..,S_m], so S is (n * m, n)\n",
        "        m, n = X.shape\n",
        "        q = compute_q(p)\n",
        "        #print(\"who is X\", X)\n",
        "        #print(\"who is y\", y)\n",
        "        #print(\"who is S\", S)\n",
        "        #print(\"who is q in AdversarialTraining: \", q)\n",
        "        #Formulate problem\n",
        "        param = cp.Variable(n)\n",
        "        #print(\"shape param \", param.shape)\n",
        "        #print(\"dim \", n)\n",
        "        print(\"X \", m, n)\n",
        "        #print(\"S_dict \", S_dict)\n",
        "        #print(\"S in adv training\", S)\n",
        "        print(\"nm \", n*m)\n",
        "        S_dts = S_dict['S_dts']\n",
        "        S_mis = S_dict['S_mis']\n",
        "        adv_radius_times_scale_dts = cp.Parameter(name='adv_radius_times_scale_dts', nonneg=True)\n",
        "        adv_radius_times_scale_mis = cp.Parameter(name='adv_radius_times_scale_mis', nonneg=True)\n",
        "        #scale_dts = cp.Parameter(name='scale_dts', nonneg=True)\n",
        "        #scale_mis = cp.Parameter(name='scale_mis', nonneg=True)\n",
        "        print(\"S_mis in Adbvt training \", S_mis)\n",
        "        if np.sum(S_mis * S_mis) == 0:\n",
        "          print(\"no missing part\")\n",
        "          S = S_dts * adv_radius_times_scale_dts\n",
        "        elif S_mis.shape == (m, n, n):\n",
        "          S_dts_tiled = np.concatenate([S_dts] * m)\n",
        "          S_mis_conc = np.concatenate(S_mis)\n",
        "          #np.concatenate([yyy] * 2)\n",
        "          S = S_dts_tiled * adv_radius_times_scale_dts + S_mis_conc * adv_radius_times_scale_mis\n",
        "          print(\"S type \", type(S))\n",
        "          #S = np.concatenate(S)\n",
        "          print(\"S is a tensor, concatenated\")\n",
        "          print(\"final S after conc \\n\", S)\n",
        "\n",
        "        if S.shape == (n, n):\n",
        "          print(\"one matrix in input, S.shape = (n, n)\")\n",
        "          partial = S @ param  # should be (m * n,)\n",
        "          param_norm = cp.pnorm(partial, p=q)\n",
        "        elif S.shape == (n * m, n):  # should be a stack of matrices\n",
        "          print(\"multiple matrices in input, S conc\")\n",
        "          partial = S @ param  # should be (m * n,)\n",
        "          partial = cp.reshape(partial, (m, n), order='C')\n",
        "          param_norm = cp.pnorm(partial, p=q, axis=1)\n",
        "        else:\n",
        "          print(\"--------> ERROR: NO MATRIX S FOUND IN ADVERSARIAL TRAINING\")\n",
        "        #elif S.shape == (m , n):  # stack of diagonal matrices\n",
        "        #  print(\"multiple matrices in input, S_i diag\")\n",
        "          #S_cvx = cp.Constant(S)\n",
        "        #  partial = cp.multiply(cp.Parameter(S), param)\n",
        "        #  param_norm = cp.pnorm(partial, p=q, axis=1)\n",
        "        abs_error = cp.abs(X @ param - y)\n",
        "        adv_loss = 1 / m * cp.sum((abs_error + param_norm) ** 2)\n",
        "        prob = cp.Problem(cp.Minimize(adv_loss))\n",
        "        self.prob = prob\n",
        "        self.adv_radius_times_scale_dts = adv_radius_times_scale_dts\n",
        "        self.adv_radius_times_scale_mis = adv_radius_times_scale_mis\n",
        "        #self.scale_dts = scale_dts\n",
        "        #self.scale_mis = scale_mis\n",
        "        self.param = param\n",
        "        self.warm_start = False\n",
        "\n",
        "\n",
        "    def __call__(self, dict_hyper_p, **kwargs):\n",
        "        try:\n",
        "            #print(\"dic thyper p \", dict_hyper_p)\n",
        "            self.adv_radius_times_scale_dts.value = dict_hyper_p['adv_radius_times_scale_dts']\n",
        "            self.adv_radius_times_scale_mis.value = dict_hyper_p['adv_radius_times_scale_mis']\n",
        "            #self.scale_dts.value = dict_hyper_p['scale_dts\n",
        "            #self.scale_mis.value = dict_hyper_p['scale_mis']\n",
        "            self.prob.solve(warm_start=self.warm_start, solver=cp.CLARABEL, max_iter=10000, **kwargs)\n",
        "            v = self.param.value\n",
        "        except Exception as e:\n",
        "          print(\"------------------> Error occurred:\")\n",
        "          traceback.print_exc()\n",
        "          v = np.zeros(self.param.shape)\n",
        "        #except:\n",
        "        #    print(\"----------------------> you are in except\")\n",
        "        #    v = np.zeros(self.param.shape)\n",
        "        return v\n",
        "\n",
        "'''\n",
        "    def __call__(self, adv_radius, **kwargs):\n",
        "        try:\n",
        "            self.adv_radius.value = adv_radius\n",
        "            self.prob.solve(warm_start=self.warm_start, solver=cp.CLARABEL, max_iter=10000, **kwargs)\n",
        "            v = self.param.value\n",
        "        except Exception as e:\n",
        "          print(\"------------------> Error occurred:\")\n",
        "          traceback.print_exc()\n",
        "          v = np.zeros(self.param.shape)\n",
        "        #except:\n",
        "        #    print(\"----------------------> you are in except\")\n",
        "        #    v = np.zeros(self.param.shape)\n",
        "        return v\n",
        "'''\n",
        "\n",
        "\n",
        "def get_lasso_path(X, y, eps_lasso=1e-5):\n",
        "    alphas, coefs, _ = lasso_path(X, y, eps=eps_lasso)\n",
        "    coefs= np.concatenate([np.zeros([X.shape[1], 1]), coefs], axis=1)\n",
        "    alphas = np.concatenate([1e2 * np.ones([1]), alphas], axis=0)\n",
        "    return alphas, coefs, []\n",
        "\n",
        "\n",
        "def get_path(X, y, estimator, amax, dts_max, mis_max, eps_amax=1e-5, eps_dts_max=1e-3, eps_mis_max=1e-1, n_alphas=150, n_deltas_dts=1, n_deltas_mis=1):\n",
        "    _, m = X.shape\n",
        "    amin = eps_amax * amax\n",
        "    dts_min = eps_dts_max * dts_max\n",
        "    mis_min = eps_mis_max * mis_max\n",
        "    alphas = np.logspace(np.log10(amin), np.log10(amax), n_alphas)\n",
        "    dts_deltas = np.logspace(np.log10(dts_min), np.log10(dts_max), n_deltas_dts)\n",
        "    mis_deltas = np.logspace(np.log10(mis_min), np.log10(mis_max), n_deltas_mis)\n",
        "    print(\"dts deltas \", dts_deltas)\n",
        "    #hyper_p = {'scale_dts': dts_deltas, 'scale_mis': mis_deltas}\n",
        "    hyper_p_ret_ = []\n",
        "    coefs_ = []\n",
        "    for scale_dts_value in tqdm.tqdm(dts_deltas):\n",
        "        for scale_mis_value in tqdm.tqdm(mis_deltas):\n",
        "          #tuple_key = (scale_dts_value, scale_mis_value)\n",
        "          #coefs_ = []\n",
        "          for a in tqdm.tqdm(alphas):\n",
        "              #dict_hyper_p_values = {'adv_radius': a, 'scale_dts': scale_dts_value, 'scale_mis': scale_mis_value}\n",
        "              dict_hyper_p_values = {'adv_radius_times_scale_dts': a * scale_dts_value, 'adv_radius_times_scale_mis': a * scale_mis_value}\n",
        "              coefs = estimator(X, y, dict_hyper_p_values)\n",
        "              #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "              coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "              hyper_p_ret_.append([a, scale_dts_value, scale_mis_value])\n",
        "          #res[tuple_key] = np.stack((coefs_)).T\n",
        "    return np.stack((hyper_p_ret_)).T, np.stack((coefs_)).T\n",
        "\n",
        "'''\n",
        "def get_path(X, y, estimator, amax, eps=1e-5, n_alphas=200):\n",
        "    _, m = X.shape\n",
        "    amin = eps * amax\n",
        "    alphas = np.logspace(np.log10(amin), np.log10(amax), n_alphas)\n",
        "    coefs_ = []\n",
        "    for a in tqdm.tqdm(alphas):\n",
        "        coefs = estimator(X, y, a)\n",
        "        #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "        coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "    return alphas, np.stack((coefs_)).T\n",
        "'''\n",
        "\n",
        "\n",
        "def plot_coefs(alphas, coefs, ax):\n",
        "    #print(\"you are printing coefs in function of 1/alphas\")\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    #l1norm = np.abs(coefs).sum(axis=0)\n",
        "    ax.set_xlabel(\"1/alphas\")\n",
        "    ax.set_ylabel(\"coef\")\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.semilogx(1/alphas, coef_l, c=c)\n",
        "        #ax.semilogx(1/alphas, l1norm, c=c)\n",
        "        #ax.plot(1/alphas, coef_l, c=c)\n",
        "\n",
        "\n",
        "def plot_coefs_l1norm(coefs, ax):\n",
        "    #print(\"you are printing coeff in function of l1 norm\")\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    #l1norm = np.abs(coefs).mean(axis=0)\n",
        "    l1norm = np.abs(coefs).sum(axis=0)\n",
        "    #print(\"coef \", coefs)\n",
        "    #print(\"l1norm \", l1norm)\n",
        "    ax.set_xlabel(\"l1norm\")\n",
        "    ax.set_ylabel(\"coef\")\n",
        "\n",
        "\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.plot(l1norm, coef_l, c=c)\n",
        "\n",
        "\n",
        "def train_and_plot(X, y, S_dict, list_ax):\n",
        "    linfadvtrain = AdversarialTraining(X, y, S_dict, p=np.inf)\n",
        "    estimator = lambda X, y, dic_h:  linfadvtrain(dict_hyper_p=dic_h)\n",
        "    hyper_p, coefs_advtrain_linf  = get_path(X, y, estimator, 1e3, 1e0, 1e0)\n",
        "    #print(\"hyper_p used\\n \", hyper_p)\n",
        "    if len(list_ax) > 0:\n",
        "      plot_coefs_l1norm(coefs_advtrain_linf, list_ax[0])\n",
        "      plot_coefs(alphas_adv, coefs_advtrain_linf, list_ax[1])\n",
        "    return hyper_p, coefs_advtrain_linf\n",
        "\n",
        "'''\n",
        "def add_rectangles_old(x, y, box_width, box_height, ax):\n",
        "  r_c = (np.random.binomial(1, 1, size=x.size) == 1)  # 1 taken, 0 not taken\n",
        "  #print(r_c)\n",
        "\n",
        "  for xi, yi in zip(x[r_c], y[r_c]):\n",
        "      rect = patches.Rectangle(\n",
        "        (xi-box_width/2, yi-box_height/2),\n",
        "        box_width, box_height,\n",
        "        linewidth=1, edgecolor='r', facecolor='none'\n",
        "      )\n",
        "      ax.add_patch(rect)\n",
        "'''\n",
        "\n",
        "def add_rectangles(x, y, S, ax):\n",
        "  r_c = (np.random.binomial(1, 1, size=x.size) == 1)  # 1 taken, 0 not taken\n",
        "  #print(r_c)\n",
        "  d = S.shape[-1]\n",
        "  #S = S * 100\n",
        "  if S.ndim == 2 or S.shape == (1, d, d):\n",
        "    S = S.squeeze()\n",
        "    print(\"------------------------> who is S in add_rectangles\\n\", S)\n",
        "    box_width = S[0, 0]\n",
        "    box_height = S[1, 1]\n",
        "    for xi, yi in zip(x[r_c], y[r_c]):\n",
        "        rect = patches.Rectangle(\n",
        "          (xi-box_width/2, yi-box_height/2),\n",
        "          box_width, box_height,\n",
        "          linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "  else:  # S is something like (n, d, d)\n",
        "    #print(\"---------------> who is S in add_rectangles (mult imp)\\n\", S)\n",
        "    box_width = S[:, 0, 0]\n",
        "    box_height = S[:, 1, 1]\n",
        "    #print(\"bw\\n \", box_width)\n",
        "    #print(\"bh\\n \", box_height)\n",
        "    #print(\"------------------------------> boxes printed\")\n",
        "    for xi, yi, bw, bh in zip(x[r_c], y[r_c], box_width[r_c], box_height[r_c]):\n",
        "        #print(\"bw, bh \", bw, \",   \", bh)\n",
        "        rect = patches.Rectangle(\n",
        "          (xi-bw/2, yi-bh/2),\n",
        "          bw, bh, linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "\n",
        "\n",
        "print(\"end block\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation's block\n",
        "\n",
        "def clear_dataset(X, y, masks):\n",
        "  # remove observations full NaN\n",
        "  # X is an (n, d) matrix, y is a (n,) vector,\n",
        "  # masks is an (n, d) binary matrix associated to X. 1 missing, 0 seen\n",
        "  M = np.sum(1 - masks, axis=1) > 0\n",
        "  M_col = np.sum(1 - masks, axis=0) > 0  # True if in the column there is at least one seen component\n",
        "  if np.sum(M_col) < masks.shape[1]:\n",
        "    print(\"Careful, there is one column full of nan\")\n",
        "  return X[M, :][:, M_col], y[M], masks[M, :][:, M_col]\n",
        "\n",
        "\n",
        "def single_imputation(X_nan, impute_estimator):\n",
        "    ice = IterativeImputer(estimator=impute_estimator)\n",
        "    return ice.fit_transform(X_nan)\n",
        "\n",
        "\n",
        "def multiple_imputation(nbr_mi, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    res = np.zeros((nbr_mi, n, d))\n",
        "    for i in range(nbr_mi):\n",
        "       n_i = np.random.randint(0, 1000)\n",
        "       ice = IterativeImputer(random_state=n_i, max_iter=50, sample_posterior=True)\n",
        "       res[i, :, :] = ice.fit_transform(X_nan)\n",
        "       #print(\"fin res shape\", res.shape)\n",
        "       #if nbr_mi == 1:\n",
        "        #res = res[0, :, :]\n",
        "        #print(\"fin res shape\", res.shape)\n",
        "    return res\n",
        "\n",
        "\n",
        "def imputation_elliptic(mu, sigma, x, masks):\n",
        "  # mu, mean elliptical distribution (,d)\n",
        "  # sigma, cov matrix elliptical distribution (d, d)\n",
        "  # x: dataset (n, d)\n",
        "  # masks: mask data, 0 seen, 1 missing\n",
        "  n, d = x.shape\n",
        "  print(n, d)\n",
        "  x_imp = x.copy()\n",
        "  #print(\"x_imp clean\", x_imp)\n",
        "  for i in range(n):\n",
        "    if not (masks[i, :] == 0).all():  # if we have at least one missing component\n",
        "      #print(\"nbr : \", i)\n",
        "      x_c = x[i, :]\n",
        "      m_bool = (masks[i, :] == 0)  # True seen, False missing\n",
        "      sigma_aa_inv = np.linalg.inv(sigma[m_bool, :][:, m_bool])\n",
        "      sigma_ma = sigma[~m_bool, :][:, m_bool]\n",
        "      mu_cond = mu[~m_bool] + sigma_ma @ sigma_aa_inv @ (x_c[m_bool] - mu[m_bool])\n",
        "      x_imp[i, ~m_bool] = mu_cond\n",
        "  return x_imp\n",
        "\n",
        "\n",
        "def listwise_delection(X, masks):\n",
        "  # masks: 1 missing, 0 seen\n",
        "    M = np.sum(masks, axis=1) == 0  # zeros components are the one with full entries\n",
        "    ret = X[M, :] if X.ndim == 2 else X[M]\n",
        "    return ret\n"
      ],
      "metadata": {
        "id": "qyWskXpdOW9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElCvHxBiO_2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA7J67yAuQM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.random.seed(42)\n",
        "\n",
        "#p_miss_2d = [0.2, 0.4, 0.4]\n",
        "#beta_2d = np.array([0.5, 2])  # ground truth\n",
        "\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def generate_masks_2d(nbr_of_sample, p_missing):\n",
        "    # nbr_of_sample is the number of masks\n",
        "    # p_missing=[p00, p01, p10], where p00 is the probability of seeing both components,\n",
        "    # p10 is the probability of seeing the right component, p01 is the probability of seeing the left component\n",
        "    masks = np.zeros((nbr_of_sample, 2))\n",
        "    v = np.random.choice(a=3, size=nbr_of_sample, p=p_missing)\n",
        "    masks[v == 0, :] = np.array([0, 0])  # both seen\n",
        "    masks[v == 1, :] = np.array([0, 1])  # left seen\n",
        "    masks[v == 2, :] = np.array([1, 0])  # right seen\n",
        "    return masks\n",
        "\n",
        "def best_predictor(X, coeff, y):\n",
        "  hat_y = (X @ coeff).T  # (n, d) @ (d, m) = (n, m)\n",
        "  r = hat_y - y  # residual\n",
        "  score = np.mean(r * r, axis=1)\n",
        "  print(\"scores:  \", score)\n",
        "  i_min = np.argmin(score)\n",
        "  return coeff[:, i_min], score[i_min]\n",
        "\n",
        "def best_idx_predictor(X, coeff, y):\n",
        "  hat_y = (X @ coeff).T  # (n, d) @ (d, m) = (n, m)\n",
        "  r = hat_y - y  # residual\n",
        "  #score = np.mean(r * r, axis=1)\n",
        "  score = np.mean(r * r, axis=1)\n",
        "  #print(\"score in best idx\", score)\n",
        "  i_min = np.argmin(score)\n",
        "  #### find the minimum value with a threshold, so we get bigger uncertainty set that are visible\n",
        "  min = np.min(score)\n",
        "  max = np.max(score)\n",
        "  score[ score < min + 1e-2 ] = max\n",
        "  ####\n",
        "  #print(\"score after \", score)\n",
        "  i_min = np.argmin(score)\n",
        "  return i_min, score[i_min]\n",
        "\n",
        "\n",
        "\n",
        "def generate_X(data, dim):\n",
        "    if data == 'Gaussian':\n",
        "      def generator(n):\n",
        "        return np.random.randn(n, dim)\n",
        "    elif data == 'Uniform':\n",
        "      def generator(n):\n",
        "        return np.random.rand(n, dim)\n",
        "    elif data == 'moons':\n",
        "      def generator(n):\n",
        "        return make_moons(n, noise=0.1)[0]\n",
        "    elif data == 'circles':\n",
        "      def generator(n):\n",
        "        return make_circles(n, noise=0.1, factor=0.4)[0]\n",
        "    return generator\n"
      ],
      "metadata": {
        "id": "AN61ok0A_Mbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jB0J9uh-dJBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwSkM31ztfUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment 2d with dataset generated externally\n",
        "\n",
        "def imputations(info, dict_obs_for_imp):  # X_nan, y):\n",
        "  # info contains the method and possible extra information\n",
        "  # X_nan is the dataset with nan in place of the missing components\n",
        "  # y is return as it is, unless the method require to change it, like in\n",
        "  # listwise deletion\n",
        "    #print(info)\n",
        "    X_nan = dict_obs_for_imp['X_nan']\n",
        "    y = dict_obs_for_imp['y_train']\n",
        "    mask_from_X_nan = np.isnan(X_nan).astype(int)\n",
        "    if info['imp_method'] == 'BR_si':  # Baeysian_Ridge_single_imputation\n",
        "        X = single_imputation(X_nan, BayesianRidge())\n",
        "    elif info['imp_method'] in  ['mi', 'mi_pure']:\n",
        "        X = multiple_imputation(info['mi_nbr'], X_nan)\n",
        "    elif info['imp_method'] == 'l_d':  # listwise_deletion\n",
        "        #mask_from_X_nan = np.isnan(X_nan).astype(int)\n",
        "        X = listwise_delection(X_nan, mask_from_X_nan)\n",
        "        y = listwise_delection(y, mask_from_X_nan)\n",
        "        if len(X) == 0:  # no elements left, add an artificial element\n",
        "            X = np.zeros((1, X_nan.shape[-1]))\n",
        "            y = np.zeros(1)\n",
        "        mask_from_X_nan = np.zeros_like(X)\n",
        "    elif info['imp_method'] == 'oracle':\n",
        "        X = dict_obs_for_imp['X_train_masked'][0]\n",
        "        mask_from_X_nan = np.zeros_like(X)\n",
        "    else:\n",
        "      print(\"-------------------> ERROR: WRONG KEYWORD (in imputations)\")\n",
        "    return X, y, mask_from_X_nan\n",
        "\n",
        "\n",
        "def cov_strategy(info, dict_observations):\n",
        "    X_imputed = dict_observations['X_imputed']\n",
        "    X_nan = dict_observations['X_nan']\n",
        "    masks = dict_observations['masks_after_imputation']\n",
        "    print(np.sum(masks, axis=-1))\n",
        "    if info['cov_strategy'] == 'sd':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      #print(\"sd in cov strategy \", sd)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(sd)\n",
        "    elif info['cov_strategy'] == 'inv_sd':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(1 / sd)\n",
        "    elif info['cov_strategy'] == 'zero':\n",
        "      #sd = np.std(X_imputed, axis=0)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.zeros((X_imputed.shape[-1], X_imputed.shape[-1]))\n",
        "    elif info['cov_strategy'] == 'eye':\n",
        "      S = np.eye(X_imputed.shape[-1])\n",
        "    elif info['cov_strategy'] == 'threshold':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      sd[sd < info['threshold']] = info['threshold']\n",
        "      #S = np.diag(sd) The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(sd)\n",
        "    elif info['cov_strategy'] == 'std_nan':\n",
        "      if info['imp_method'] in ['oracle']:\n",
        "        print(\"DON'T USE std_nan with oracle and ld because you do not have any nan. Use sd\")\n",
        "      else:\n",
        "        std_columnwise = np.nanstd(X_nan, axis=0)\n",
        "        S = np.diag(std_columnwise)\n",
        "    elif info['imp_method'] in ['mi_pure', 'mi']:\n",
        "      if info['cov_strategy'] == 'std_mi':   # std of the imputed dataset, then the mean\n",
        "        std_vectors = np.std(X_imputed, axis=-2)  # shape: (m, d)\n",
        "        #print(\"std vectors \", std_vectors)\n",
        "        s_within = np.mean(std_vectors, axis=0)  # within imputation variance  # shape : d\n",
        "        S = np.diag(s_within)\n",
        "        print(\"final S in cov strategy std_mi \", S)\n",
        "      elif info['cov_strategy'] == 'RR':\n",
        "        #if info['mi_nbr'] == 1:\n",
        "        #  X_imputed = np.array([X_imputed])\n",
        "        # X shape = (m, n, d)\n",
        "        std_vectors = np.std(X_imputed, axis=-2)  # shape: (m, d)\n",
        "        #print(\"std vectors \", std_vectors)\n",
        "        s_within = np.mean(std_vectors, axis=0)  # within imputation variance  # shape : d\n",
        "        print(\"s_within \", s_within)\n",
        "        print(\"cov computed\")\n",
        "        #print(s_mean)\n",
        "        s_between = np.std(std_vectors, axis=0) # between imputation variance  # shape: d. That's already scaled because we are computing the std\n",
        "        print(\"s_between \", s_between)\n",
        "        S = np.diag(s_within + s_between * (1 + 1 / info['mi_nbr']))\n",
        "        print(\"final S in cov strategy RR \", S)\n",
        "        #mu = np.mean(X_imputed, axis=0)\n",
        "        #sigma = np.cov(X_imputed, rowvar=False)\n",
        "      elif info['cov_strategy'] == 'RR_scaled (to check)':\n",
        "        print(\"Rub Rule right scaled\")\n",
        "        #if info['mi_nbr'] == 1:\n",
        "        #  X_imputed = np.array([X_imputed])\n",
        "        # X shape = (m, n, d)\n",
        "        std_vectors = np.std(X_imputed, axis=-2) # shape: (m, d)\n",
        "        #print(\"std vectors \", std_vectors)\n",
        "        s_within = np.mean(std_vectors, axis=0)  # within imputation variance  # shape : d\n",
        "        print(\"s_within \", s_within)\n",
        "        print(\"cov computed\")\n",
        "        #print(s_mean)\n",
        "        s_between = np.std(std_vectors, axis=0) # between imputation variance  # shape: d\n",
        "        #s_between = np.sqrt(s_between)\n",
        "        print(\"s_between \", s_between)\n",
        "        S = np.diag(s_within + s_between * (1 + 1 / info['mi_nbr']))\n",
        "        #S = np.sqrt(S)\n",
        "        print(\"final S in cov strategy RR \", S)\n",
        "      #elif info['cov_strategy'] == 'cond_var':\n",
        "        # we have imputed [X1,..,X_m]\n",
        "        #s = np.std(X_imputed, axis=0)\n",
        "        #print(\"s\\n \", s)\n",
        "        #eye = np.array([np.eye(X_imputed.shape[-1])] * X_imputed.shape[-2])\n",
        "        #S = eye * s[:, None, :]\n",
        "        #S = np.concatenate(S, axis=0)\n",
        "        #print(\"S in cond variance \", S)\n",
        "    elif info['cov_strategy'] == 'lounici':\n",
        "      mu = np.nanmean(X_nan, axis=0)\n",
        "      print(\"means \", mu)\n",
        "      delta = 1 - np.mean(masks) # parameter missingness\n",
        "      print(\"delta \", delta)\n",
        "      X_0 = np.nan_to_num(X_nan - mu)  # check if this is correct\n",
        "      print(\"nbr obs\", X_0.shape[0])\n",
        "      S =  X_0.T @ X_0 / X_0.shape[0]\n",
        "      S = (1/delta - 1/(delta**2)) * np.diag(np.diag(S)) + 1/(delta**2) * S\n",
        "    else:\n",
        "      raise ValueError(\"-------------> ERROR: NO COVARIANCE METHOD HAS BEEN CHOSEN\")\n",
        "      #print(\"-------------> ERROR: NO COVARIANCE METHOD HAS BEEN CHOSEN\")\n",
        "      #S = np.diag(S)\n",
        "      #mu = np.mean(X_imputed, axis=0)\n",
        "      #sigma = np.cov(X_imputed, rowvar=False)\n",
        "    return S\n",
        "\n",
        "\n",
        "def cov_strategy_missing(info, dict_observations):\n",
        "    # undertainty that come from the imputed part. It is zero\n",
        "    X_imputed = dict_observations['X_imputed']\n",
        "    if info['imp_method'] in ['mi', 'mi_pure']:\n",
        "      if info['cov_strategy_between'] == 'cond_var':\n",
        "        # we have imputed [X1,..,X_m], so shape (m, n, d)\n",
        "        s = np.std(X_imputed, axis=0)\n",
        "        s[s<1e-14] = 0  # set to zero values that are basically zero\n",
        "        #print(\"var \", s)\n",
        "        eye = np.array([np.eye(X_imputed.shape[-1])] * X_imputed.shape[-2])\n",
        "        S_mis = eye * s[:, None, :]\n",
        "    else:\n",
        "      print(\"shape oject in cov strategy missing \", dict_observations['X_test'].shape[-1])\n",
        "      print(\"shape oject in cov strategy missing \", dict_observations['X_test'].shape)\n",
        "      d = dict_observations['X_test'].shape[-1]\n",
        "      S_mis = np.zeros((d, d))\n",
        "    return S_mis\n",
        "\n",
        "\n",
        "def post_imputation(info_imp, dict_dataset):\n",
        "  # X_imptued should be a matrix (n, d) or tensor (m, d, n) (in multiple imputations methods)\n",
        "    X_imputed = dict_dataset['X_imputed']\n",
        "    y_train = dict_dataset['y_from_X_imputed']\n",
        "    #print(\"info imp in post_imp\", info_imp)\n",
        "    print(\"shape X_imputed in post_imputation \", X_imputed.shape)\n",
        "    mask_train = dict_dataset['masks_after_imputation']\n",
        "    if 'post_imp' not in info_imp.keys():\n",
        "      X_train = X_imputed\n",
        "    elif info_imp['post_imp'] == 'mean':\n",
        "      #print(\"entered in pst_iputation, in mi_mean\")\n",
        "      X_train = np.mean(X_imputed, axis=0)\n",
        "    elif info_imp['post_imp'] == 'conc':\n",
        "      X_train = np.concatenate(X_imputed)\n",
        "    else:\n",
        "      X_train = X_imputed\n",
        "    return X_train, y_train, mask_train\n",
        "\n",
        "\n",
        "def generate_dataset(data, n_tot, dim, beta_gt, perc_test, p_miss, err):\n",
        "    print(data)\n",
        "    if data['data'] == 'Gaussian':\n",
        "      X_complete = np.random.randn(n_tot, dim)\n",
        "    elif data['data'] == 'Normal':\n",
        "      #print(\"you are here\")\n",
        "      X_complete = np.random.multivariate_normal(mean=data['mean'], cov=data['cov'], size=n_tot)\n",
        "    elif data['data'] == 'Uniform':\n",
        "      X_complete = np.random.rand(n_tot, dim)\n",
        "    elif data['data'] == 'moons':\n",
        "      X_complete = make_moons(n_tot, noise=0.1)[0]\n",
        "    elif data['data'] == 'circles':\n",
        "      X_complete = make_circles(n_tot, noise=0.1, factor=0.4)[0]\n",
        "\n",
        "    if err['type'] == 'Gaussian_on_y':\n",
        "      #print(\"---> you have entered in GAUSSIAN ERROR \", \"scaling : \", err['scaling'])\n",
        "      error = np.random.randn(n_tot) * err['scaling']\n",
        "    elif err['type'] == 'Uniform_on_y':\n",
        "      error = (np.random.rand(n_tot)-0.5) * err['scaling']\n",
        "    elif err['type'] == 'Gaussian_on_X':\n",
        "      error = (np.random.randn(n_tot, dim) @ beta_gt) * err['scaling']  # error is of the form DX@beta_gt + error\n",
        "    #elif err['type'] == 'Gaussian':\n",
        "    #  error = np.random.randn(n_tot) * err['scaling']\n",
        "\n",
        "    print(X_complete.shape)\n",
        "\n",
        "    y_complete = X_complete @ beta_gt + error  #np.random.randn(n_tot) * err  # (np.random.rand(n_tot) - 0.5) * err\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_complete, y_complete, test_size=perc_test)\n",
        "    n_train = X_train.shape[0]\n",
        "    masks_train = generate_masks_2d(n_train, p_miss)  # 1 missing, 0 observed\n",
        "    #M = np.sum(masks, axis=1)  # M[i] > 0 iff i has missing component\n",
        "    #dict_obs = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test, 'masks_train': masks_train}\n",
        "    dict_obs = {'X_train_masked': (X_train, masks_train), 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}#, 'masks_train': masks_train}\n",
        "    return dict_obs\n",
        "\n",
        "\n",
        "def experiment_2d_ext_dataset(dict_obs, dict_imp, ax):\n",
        "    # dict_obs contains info on the observations, i.e. train, test, masks\n",
        "    # dict_imp contains info on the imputation an covariance methods used,\n",
        "    # dict_imp = {'imp_method': , 'cov_strategy': , .... }\n",
        "    # ax contains info for the plots\n",
        "\n",
        "    X_test = dict_obs['X_test']\n",
        "    y_test = dict_obs['y_test']\n",
        "    mask = dict_obs['X_train_masked'][1]\n",
        "\n",
        "    M = np.sum(mask, axis=1)  # M[i] > 0 iff i has missing component\n",
        "\n",
        "    X_nan_train = dict_obs['X_train_masked'][0].copy()\n",
        "    oracle_sd = np.std(X_nan_train, axis=0)\n",
        "    print(\"-------> ORACLE SD, std of the original dataset (with no missing)\", oracle_sd)\n",
        "    X_nan_train[mask == 1] = np.nan\n",
        "    #print(\"dict imp -----> \", dict_imp)\n",
        "    dict_obs = dict_obs | {'X_nan': X_nan_train} #, 'y_from_X_imputed': y_from_X_imputed, 'masks_after_imputation': mask_from_X_imputed}\n",
        "    if len(dict_obs['imp_ds'][dict_imp['imp_method']]) == 0:  # no previous imputation has been done\n",
        "      #results = imputations(dict_imp, X_nan_train, dict_obs['y_train'])\n",
        "      results = imputations(dict_imp, dict_obs)\n",
        "      X_imputed, y_from_X_imputed, mask_from_X_imputed = results  # imputations(dict_imp, X_nan_train, dict_obs['y_train'])\n",
        "      dict_obs['imp_ds'][dict_imp['imp_method']].append(results)\n",
        "      print(\"crush test-------------------------------------------------> \", np.sum(X_imputed))\n",
        "    else:\n",
        "      X_imputed, y_from_X_imputed, mask_from_X_imputed = dict_obs['imp_ds'][dict_imp['imp_method']][0]\n",
        "      print(\"crush test-------------------------------------------------> \", np.sum(X_imputed))\n",
        "    #print(\"X_imputed \", X_imputed)\n",
        "    n_imputed, n_test = X_imputed.shape[-2], X_test.shape[-2]\n",
        "    #print(\"X_train\\n \", X_train)\n",
        "    M = np.sum(mask_from_X_imputed, axis=1)  # M[i] > 0 iff i has missing component\n",
        "\n",
        "    dict_obs = dict_obs | {'X_imputed': X_imputed, 'y_from_X_imputed': y_from_X_imputed, 'masks_after_imputation': mask_from_X_imputed}\n",
        "    #  print(dict_obs)\n",
        "    S_dataset = cov_strategy(dict_imp, dict_obs) * dict_imp['multip_dataset']\n",
        "    print(\"S dataset \\n\", S_dataset)\n",
        "    #  dict_obs = dict_obs | {'cov_within': S_within}\n",
        "    S_missing = cov_strategy_missing(dict_imp, dict_obs)  * dict_imp['multip_missing']\n",
        "    print(\"S missing shape\\n \", S_missing.shape)\n",
        "    print(\"S missing\\n \", S_missing)\n",
        "    #  dict_obs = dict_obs | {'cov_between': S_between}\n",
        "    S_dict = {'S_dts': S_dataset, 'S_mis': S_missing}  #, 'multipliers_dts': dict_imp['multip_dataset'], 'multipliers_mis': dict_imp['multip_missing']}\n",
        "    #if True:  # check what to do of this part later\n",
        "      #S = S_dataset * dict_imp['multip_dataset'] + S_missing * dict_imp['multip_missing']\n",
        "      #if S.ndim == 2:\n",
        "      #  print(\"final S \\n\", S)\n",
        "\n",
        "\n",
        "    #print(\"matrices S \\n\", S)\n",
        "    #print(\"---....---....----....--> diag matrix: \", np.diag(S))\n",
        "\n",
        "    #if dict_imp['imp_method'] == 'mi':  # prepare the training set in case of multiple imputation\n",
        "    #  X_train = np.concatenate(X_train)  # X_train, if the method is mi, should be (mi_nbr, n, dim)\n",
        "    #  y_train = np.tile(y_train, reps=dict_imp['mi_nbr'])\n",
        "    #  mask_train = np.tile(mask_train, reps=(dict_imp['mi_nbr'], 1))\n",
        "    #  M = np.sum(mask_train, axis=1)\n",
        "    #print(\"final matrices (exp 2d ext run)\\n \", S)\n",
        "    X_train, y_train, mask_train = post_imputation(dict_imp, dict_obs)\n",
        "    n_train = X_train.shape[-2]\n",
        "    print(\"y_train length \", y_train.shape[0])\n",
        "    print(\"-------> size test: \", n_test, \" , size train: \", n_train, \"nbr_seen (train): \", np.sum(M == 0), \" nbr_miss : \", np.sum(M > 0))\n",
        "\n",
        "#    plt.tight_layout()\n",
        "    #S_between = S.copy()\n",
        "    if dict_imp['imp_method'] == 'mi_pure':\n",
        "      best_coeff = np.zeros(X_train.shape[-1])\n",
        "      best_alpha = 0\n",
        "      for i in range(dict_imp['mi_nbr']):\n",
        "        print(\"i .-------------> \", i)\n",
        "        dict_obs_i = {'X_imputed': X_train[i, :, :], 'X_nan': X_nan_train, 'masks': mask_train}\n",
        "        dict_imp_new = {'imp_method': dict_imp['imp_method'], 'cov_strategy': dict_imp['cov_strategy_within']}\n",
        "        S_within = cov_strategy(dict_imp_new, dict_obs_i)  # within the dataset\n",
        "        #print(\"S_within \", S_within)\n",
        "        S = S_within[None, :, :] + S_between\n",
        "        S = np.concatenate(S, axis=0)\n",
        "        #print(S)\n",
        "        alphas_used, coeff_results = train_and_plot(X_train[i, :, :], y_train, S, [ax[1], ax[2]])\n",
        "        idx_best, min_score = best_idx_predictor(X_test, coeff_results, y_test)\n",
        "        best_coeff_partial, best_alpha_partial = coeff_results[:, idx_best], alphas_used[idx_best]\n",
        "        print(\"best coeff partial \", best_coeff_partial)\n",
        "        best_coeff += best_coeff_partial\n",
        "        best_alpha += best_alpha_partial\n",
        "        if len(ax) > 0:\n",
        "          ax[0].scatter(X_train[i, M == 0, 0], X_train[i, M == 0, 1])\n",
        "          ax[0].scatter(X_train[i, M == 1, 0], X_train[i, M == 1, 1])\n",
        "          ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', n_s: ' + str(np.sum(M == 0)) + \" n_m: \" + str(np.sum(M > 0)))  # n_s = nbr seen, n_m = nbr missing\n",
        "          add_rectangles(X_train[i, :, 0], X_train[i, :, 1], S[0, 0] * best_alpha_partial, S[1, 1] * best_alpha_partial, ax[0])\n",
        "        best_coeff /= dict_imp['mi_nbr']\n",
        "      best_alpha /= dict_imp['mi_nbr']\n",
        "    else:\n",
        "      #alphas_used, coeff_results = train_and_plot(X_train, y_train, S_dict, [ax[1], ax[2]])\n",
        "      hyper_p_used, coeff_results = train_and_plot(X_train, y_train, S_dict, [])\n",
        "      idx_best, min_score = best_idx_predictor(X_test, coeff_results, y_test)\n",
        "      #best_coeff, best_alpha = coeff_results[:, idx_best], alphas_used[idx_best]\n",
        "      #print(\"-----------------> shape hyper_p used \", hyper_p_used.shape)\n",
        "      best_coeff, best_hyper_p = coeff_results[:, idx_best], hyper_p_used[:, idx_best]\n",
        "      #print(\"hyper_p_used \", hyper_p_used.T)\n",
        "      #input()\n",
        "      #print(X_br_train[M == 0, 0])\n",
        "      best_alpha, best_delta_dts, best_delta_mis = best_hyper_p[0], best_hyper_p[1], best_hyper_p[2]\n",
        "      if len(ax) > 0:\n",
        "        ax[0].scatter(X_train[M == 0, 0], X_train[M == 0, 1])\n",
        "        ax[0].scatter(X_train[M == 1, 0], X_train[M == 1, 1])\n",
        "        #ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', n_s: ' + str(np.sum(M == 0)) + \" n_m: \" + str(np.sum(M > 0)))  # n_s = nbr seen, n_m = nbr missing\n",
        "        # 'multip_betw': 1, 'multip_with':1\n",
        "        ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', dts:'+str(dict_imp['multip_dataset']) + ', mis:' + str(dict_imp['multip_missing']) )  # n_s = nbr seen, n_m = nbr missing\n",
        "        S_plot = (S_dict['S_dts'] * best_delta_dts + S_dict['S_mis'] * best_delta_mis) * best_alpha\n",
        "        #print(\"S_plot \", S_plot)\n",
        "        add_rectangles(X_train[:, 0], X_train[:, 1], S_plot, ax[0])\n",
        "        ax[0].set_aspect('equal')  # equal proportion of the axis\n",
        "    #print(\"X_train \", X_train)\n",
        "    #print(\"y_train \", y_train)\n",
        "    #print(\"mask_train \", mask_train)\n",
        "    #print(\"M \", M)\n",
        "\n",
        "\n",
        "    print(\"X_test shape, \", X_test.shape, \",   y_test shape \", y_test.shape)\n",
        "    #print(\"X_test shape, \", X_test.shape)\n",
        "    print(\"---------------------------------> best idx \", idx_best, \" best hyperp [best_alpha, coef_dts, coef_mis]: \", best_hyper_p, \", min score \", min_score)\n",
        "    print(\"---------------------------------> best coeff \", best_coeff)\n",
        "    #input()\n",
        "    #print(\"best 1/alpha \", 1 / best_alpha)\n",
        "#    print(\"min score \", min_score)\n",
        "\n",
        "    #\n",
        "    #add_rectangles(X_train[:, 0], X_train[:, 1], S[0, 0] * best_alpha, S[1, 1] * best_alpha, ax[0])\n",
        "\n",
        "    return best_coeff, min_score, -np.log10(best_alpha)\n",
        "\n"
      ],
      "metadata": {
        "id": "OhNXUBahJgBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_experiments(dictio, methods_strategy):  # ---------------------> new\n",
        "  # dictio: dictionary of lists that contains the parameters of generate_dataset.\n",
        "  # Each list should have the same length\n",
        "  # methods_strategy = list of dictionary, each one of the form\n",
        "  # {'imp_method': .., 'cov_strategy':.., extra info}\n",
        "\n",
        "    l = len(dictio['data'])\n",
        "    m = len(methods_strategy)\n",
        "    nbr_iter = len(methods_strategy)\n",
        "    coeff_fin = np.zeros((nbr_iter, 2, l))\n",
        "    scores_fin = np.zeros((nbr_iter, l))\n",
        "\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(3 * l , 9 *l), num='advtrain_linf_')\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(6 * l , 9 *l), num='advtrain_linf_')\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(6 * l / 2, 9 *l / 2), num='advtrain_linf_')\n",
        "    print(dictio['plots'])\n",
        "    print(dictio['plots'][0])\n",
        "    nbr_ima = len(dictio['plots'][0])\n",
        "    if nbr_ima == 1:\n",
        "      #nbr_ima = 1\n",
        "      fig, ax = plt.subplots(nbr_ima * nbr_iter, l, figsize=(3 * l, 8/3 * m), squeeze=False)#, num='advtrain_linf_')\n",
        "    elif nbr_ima == 3:  # == 3, one day should be more general\n",
        "      #nbr_ima = 3\n",
        "      fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(3 * l, 8 * m), squeeze=False)#, num='advtrain_linf_')\n",
        "\n",
        "    res = {}\n",
        "    for info_imp_cov_dict in methods_strategy:\n",
        "      key_list = []\n",
        "      for value in info_imp_cov_dict.values():\n",
        "        print(value)\n",
        "        key_list.append(value)\n",
        "      key_tuple = tuple(key_list)\n",
        "      res[key_tuple] = {'best_coeff':[], 'l2_dist_best_coeff_gt':[], 'best_score':[], 'best_alpha':[]}\n",
        "      #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])] = {'best_coeff':[], 'l2_dist_best_coeff_gt':[], 'best_score':[], 'best_alpha':[]}\n",
        "\n",
        "    if dictio['generation'] == 'fixed':  # use this if you want to fix the generated data, and not change at every iteartion\n",
        "      dictio_obser_fixed = dict_obser = generate_dataset(data=dictio['data'][0],\n",
        "                                    n_tot=dictio['n_tot'][0],\n",
        "                                    dim=dictio['dim'][0],\n",
        "                                    beta_gt=dictio['beta_gt'][0],\n",
        "                                    perc_test=dictio['perc_test'][0],\n",
        "                                    p_miss=dictio['p_miss'][0],\n",
        "                                    err=dictio['err'][0])\n",
        "      mask_no_both_seen = generate_masks_2d(dictio['n_train'][0], [0, 0.5, 0.5]) # generate a mask where there are no entries both seen. The idea then will be to consider percentage of this mask seen\n",
        "\n",
        "    for i in range(l):\n",
        "      print(\"---------------------------------------------------------------------------------------------------------------------------> iteration \", i)\n",
        "      #  dict_obs = {'X_train_masked': (X_train, masks_train), 'X_test': ....., 'y_train': ....., 'y_test': ....}\n",
        "      dict_obser_partial = generate_dataset(data=dictio['data'][i],\n",
        "                                    n_tot=dictio['n_tot'][i],\n",
        "                                    dim=dictio['dim'][i],\n",
        "                                    beta_gt=dictio['beta_gt'][i],\n",
        "                                    perc_test=dictio['perc_test'][i],\n",
        "                                    p_miss=dictio['p_miss'][i],\n",
        "                                    err=dictio['err'][i])\n",
        "      if dictio['generation'] == 'fixed':\n",
        "        dict_obser = dictio_obser_fixed\n",
        "        #mask_partial = dict_obser_partial['X_train_masked'][1]\n",
        "        p_i = dictio['p_miss'][i][0]  # probability of seen both component at round i\n",
        "        n_train = mask_no_both_seen.shape[0]\n",
        "        mask_partial = mask_no_both_seen.copy()\n",
        "        mask_partial[0:int(n_train * p_i), :] = 0\n",
        "        #mask_partial = mask_no_both_seen[0:int(n_train * p_i), :]\n",
        "        tuple_partial = (dictio_obser_fixed['X_train_masked'][0], mask_partial)\n",
        "        dict_obser['X_train_masked'] = tuple_partial\n",
        "      else:\n",
        "        dict_obser = dict_obser_partial\n",
        "\n",
        "      #print(\"dict obser \", dict_obser)\n",
        "      dict_obser = dict_obser | {'imp_ds':{'BR_si':[], 'l_d':[], 'oracle':[], 'mi':[]}}  # add an entry for imputed dataset\n",
        "      for idx, info_imp_cov_dict in enumerate(methods_strategy):\n",
        "        print(\"----------------------------------------------> new method tested: \", info_imp_cov_dict)\n",
        "        if nbr_ima > 0:\n",
        "          coeff_round, score_round, alpha_round = experiment_2d_ext_dataset(dict_obser, info_imp_cov_dict, ax[(idx * nbr_ima):((idx+1) * nbr_ima), i])\n",
        "        else:  # == 0\n",
        "          coeff_round, score_round, alpha_round = experiment_2d_ext_dataset(dict_obser, info_imp_cov_dict, [])\n",
        "        r = coeff_round - dictio['beta_gt'][i]\n",
        "        l2_dist = np.linalg.norm(r)\n",
        "        key_list = []\n",
        "        for value in info_imp_cov_dict.values():\n",
        "          print(value)\n",
        "          key_list.append(value)\n",
        "        key_tuple = tuple(key_list)\n",
        "        res[key_tuple]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "        res[key_tuple]['best_coeff'].append(coeff_round)\n",
        "        res[key_tuple]['best_score'].append(score_round)\n",
        "        res[key_tuple]['best_alpha'].append(alpha_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_coeff'].append(coeff_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_score'].append(score_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_alpha'].append(alpha_round)\n",
        "    plt.tight_layout()\n",
        "    return res\n",
        "\n",
        "\n",
        "def plot_res(x_axis_info, res, extra_info):\n",
        "  x_axis = x_axis_info['vector']\n",
        "  print(\"x_axis for print in plot_res----> \", x_axis)\n",
        "  l = len(x_axis)\n",
        "  fig_res, ax_res = plt.subplots(1, 3, figsize=(25, 5))#, num='advtrain_linf_res')\n",
        "  positions = range(l)\n",
        "\n",
        "  for key, values in res.items():\n",
        "    print(\"key \", key, \": \", values)\n",
        "    #print(\"values \", values)\n",
        "  #print(\"res\\n \", res)\n",
        "\n",
        "  ch = ['o', 'x', '+', '*', '<', '>', 'p', 'D', 'd', 'v']\n",
        "  lb = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha']\n",
        "  for i in range(3):\n",
        "    for idx, (key, dictio) in enumerate(res.items()):\n",
        "      #print(dictio)\n",
        "      ax_res[i].plot(positions, dictio[lb[i]], marker=ch[idx], label=str(key))  # the marker is linked to the key (= method), different key correspond to different marker\n",
        "      #ax_res[1].plot(positions, dictio[lb[idx]], marker=ch[idx], label=str(key))\n",
        "      #ax_res[2].plot(positions, -np.log(dictio['best_alpha']), marker=ch[idx], label=str(key))\n",
        "      #ax_res[0].xticks(positions, n_tot)  # Set custom labels for the x-axis\n",
        "    ax_res[i].set_xticks(positions)         # Set the tick positions\n",
        "    ax_res[i].set_xticklabels(x_axis)        # Set the labels at those positions\n",
        "    ax_res[i].set_xlabel(x_axis_info['name'])\n",
        "    #ax_res[i].legend(loc='upper center', bbox_to_anchor=(1, 1))\n",
        "    ax_res[i].legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
        "  ax_res[0].set_ylabel(\"||hat_Beta - Beta^*||_2\")\n",
        "  ax_res[1].set_ylabel(\"||hat_y - y||_2^2 / n_test\")\n",
        "  dict_err = extra_info['err'][0]\n",
        "  #size_train = extra_info['n_tot'][0]\n",
        "  #ax_res[0].set_title(\"\")\n",
        "  n_test = extra_info['n_test'][0]\n",
        "  #ax_res[1].set_title(\"err: \" + dict_err['type'] + \", scale: \" + str(dict_err['scaling'])  + \", n_test: \" + str(n_test))\n",
        "  #ax_res[0].set_title('n_test: ' + str(n_test) + extra_info['title_infer_error'])\n",
        "  #ax_res[1].set_title('n_test: ' + str(n_test) + extra_info['title_test_error'])\n",
        "  ax_res[0].set_title(extra_info['title_infer_error'])\n",
        "  ax_res[1].set_title(extra_info['title_test_error'])\n",
        "  ax_res[2].set_ylabel(\"-log10(alpha)\")\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "def make_dictionary_data(nbr_experiments, n_train, n_test, data, beta_gt, p_miss, err_vector, plots):\n",
        "  # make a dictionary where each element is a list of nbr_experiments element made by the other element of the function\n",
        "  if isinstance(n_train, int):  # in case n_train is just a number\n",
        "    n_train = [n_train] * nbr_experiments\n",
        "  else:  # should be a list of integer\n",
        "    print(\"change nbr_experiments to match the size of n_train\")\n",
        "    nbr_experiments = len(n_train)\n",
        "  if isinstance(n_test, int):  # in case n_test is just a number\n",
        "    n_test = [n_test] * nbr_experiments\n",
        "  n_tot = [x + y for x, y in zip(n_train, n_test)]\n",
        "  perc_test = [x / (x+y) for x, y in zip(n_test, n_train)]\n",
        "  dim = beta_gt.size\n",
        "\n",
        "  list_errors = []\n",
        "  for i in range(nbr_experiments):\n",
        "    err_dic_app = {'type': err_vector[0], 'scaling': err_vector[1][i]}\n",
        "    list_errors.append(err_dic_app)\n",
        "\n",
        "  dictio = {'data':[data] * nbr_experiments,\n",
        "        'n_tot': n_tot,\n",
        "        'n_train': n_train,\n",
        "        'n_test': n_test,\n",
        "        'dim': [dim] * nbr_experiments,\n",
        "        'beta_gt': [beta_gt] * nbr_experiments,\n",
        "        'perc_test': perc_test,\n",
        "        #'p_miss': [p_miss] * nbr_experiments,\n",
        "        'err': list_errors,\n",
        "        'plots': [plots] * nbr_experiments\n",
        "        }\n",
        "  dictio['p_miss'] = p_miss\n",
        "\n",
        "  return dictio\n",
        "\n",
        "def make_probabilities(list_prob):\n",
        "  l = []\n",
        "  for x in list_prob:\n",
        "    l.append([x, 0.5 - x/2, 0.5 - x/2])\n",
        "  return l\n",
        "\n",
        "def make_info_axis(vector, name):\n",
        "  if name == 'train':\n",
        "    dictio = {'name': 'size train set', 'vector': vector}\n",
        "  elif name == 'p_seen':\n",
        "    dictio = {'name': 'probability seen full entries', 'vector': vector}\n",
        "  elif name == 'error':\n",
        "    dictio = {'name': 'error', 'vector': vector}\n",
        "  else:\n",
        "    print(\"wrong info_axis\")\n",
        "  return dictio\n",
        "\n",
        "def make_dictionary_method(list_meth):\n",
        "  # make a dictionary where each element is a list of nbr_experiments element made by the other element of the function\n",
        "  list_dictio=[]\n",
        "  list_key = ['imp_method', 'cov_strategy', 'mi_nbr']\n",
        "  for meth in list_meth:\n",
        "    dictio_imp = {}\n",
        "    for i in range(len(meth)):\n",
        "      dictio_imp[list_key[i]] = meth[i] #= {list_key[i]: meth[i]}\n",
        "      #print(dictio_imp)\n",
        "    list_dictio.append(dictio_imp)\n",
        "  return list_dictio\n",
        "\n",
        "\n",
        "def run_multiple_experiments(nbr_exp, rdm_seed, dictio, info_x_axis):\n",
        "  rdm_seed = 4654321\n",
        "  np.random.seed(rdm_seed)\n",
        "  res = run_experiments(dicc, list_methods_strategy)\n",
        "  plot_res(info_x_axis, res, dicc)\n",
        "  if nbr_exp > 1:\n",
        "    for k in res:\n",
        "      for h in res[k]:\n",
        "        res[k][h] = [res[k][h]]\n",
        "    for i in range(nbr_exp-1):\n",
        "      print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "      #np.random.seed(rdm_seed * (i+2))\n",
        "      res_partial = run_experiments(dictio, list_methods_strategy)\n",
        "      plot_res(info_x_axis, res_partial, dictio)\n",
        "      print(res)\n",
        "      for k in res:\n",
        "        res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "        res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "        res[k]['best_alpha'].append(res_partial[k]['best_alpha'])\n",
        "        #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "        #res.append(res['l2_dist_best_coeff_gt'])\n",
        "\n",
        "  print(\"final step, let's take the mean of the results\")\n",
        "  #print(\"res, after all the experimetns \", res)\n",
        "  for k in res:\n",
        "    print(\"key in res \", k)\n",
        "    print(np.array(res[k]['l2_dist_best_coeff_gt']))\n",
        "    print(\"mean l2_dist              \", np.mean(np.array(res[k]['l2_dist_best_coeff_gt']), axis=0))\n",
        "    print(\"mean_l2_dist diff method: \", np.mean(res[k]['l2_dist_best_coeff_gt'], axis=0))\n",
        "  #mean_res = {k: np.mean(v, axis=0) for k, v in res.items()}\n",
        "  mean_res = {k: {v: np.mean(w, axis=0) for v, w in res[k].items()} for k in res}\n",
        "  print(\"final dictionary, dictionary of the means:\")\n",
        "  for k, v in mean_res.items():\n",
        "    print(\"k:   \", k)\n",
        "    for s, t in v.items():\n",
        "      print(s, \": \", t)\n",
        "  return mean_res\n",
        "  #print(np.mean(res, axis=0))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_2LB5UnMpgCC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#info_axis = 'train'\n",
        "#n_train = [400, 800, 1200, 1600, 2000]\n",
        "#p_seen = make_probabilities([0.8, 0.8, 0.8, 0.8, 0.8])\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen\n",
        "#info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "\n",
        "gen = 'fixed'\n",
        "info_axis = 'p_seen'  # train or p_seen\n",
        "#p_seen_both = [1, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.02]\n",
        "#p_seen_both = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "p_seen_both = [0.9, 0.8, 0.7, 0.6, 0.5]\n",
        "#p_seen_both = [0.1]\n",
        "#p_seen_both = [1, 0.9, 0.8]\n",
        "length_vec = len(p_seen_both)\n",
        "#n_train = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
        "n_train = [100] * length_vec\n",
        "error_vec =  [0.1] * length_vec\n",
        "p_seen = make_probabilities(p_seen_both)\n",
        "if info_axis == 'train':\n",
        "  main_vec = n_train\n",
        "elif info_axis == 'p_seen':\n",
        "  main_vec = p_seen_both\n",
        "elif info_axis == 'error':\n",
        "  main_vec = error_vec\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen_both\n",
        "info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "number_test = 20000\n",
        "cov_var = 0.4\n",
        "\n",
        "dicc = make_dictionary_data(\n",
        "    nbr_experiments= len(main_vec), n_train = n_train, n_test=number_test,\n",
        "    data = {'data': 'Normal', 'mean': np.array([0, 0]), 'cov': np.array([[1, cov_var], [cov_var, 1]])},\n",
        "    beta_gt = np.array([-0.5, 2]),\n",
        "    p_miss = p_seen,\n",
        "    err_vector = ['Gaussian_on_y', error_vec],\n",
        "    plots = []#['points', 'l1_vs_coef', '1/alpha_vs_coef']\n",
        ")\n",
        "#dicc = dicc | {'generation':gen}\n",
        "dicc = dicc | {'generation':gen, 'title_infer_error':'  inference_error', 'title_test_error':'  test_error'}\n",
        "for key, value in dicc.items():\n",
        "  print(key,\": \" , value[0])\n",
        "\n",
        "# (imp method, cov strategy, mi_nbr)\n",
        "#list_imp_cov_methods = [('BR_si', 'sd'), ('l_d', 'sd'), ('mi', 'sd', 1)]\n",
        "\n",
        "#list_methods_strategy = make_dictionary_method(list_imp_cov_methods)\n",
        "mi_nbr = 30\n",
        "list_methods_strategy = [{'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'multip_dataset': 3, 'multip_missing':0},\n",
        "                        #{'imp_method': 'l_d', 'cov_strategy': 'std_nan', 'multip_dataset': 1, 'multip_missing':1},\n",
        "                        {'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 3, 'multip_missing': 0},\n",
        "                        {'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 0, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 1},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 3},\n",
        "                        #{'imp_method': 'mi_pure', 'cov_strategy': 'eye', 'mi_nbr': 2},\n",
        "                        #{'imp_method': 'mi_pure', 'cov_strategy': 'cond_var', 'cov_strategy_within': 'sd', 'mi_nbr': 5},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 1},\n",
        "                        #{'imp_method': 'mi_mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'eye', 'mi_nbr': 5},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'multip_betw': 0, 'multip_with': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.2},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.4},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.6},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 0, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 0, 'multip_missing': 1},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 3, 'multip_missing': 0},\n",
        "                        {'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'sd', 'mi_nbr': mi_nbr, 'multip_dataset': 3, 'multip_missing': 1},\n",
        "                        {'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 3, 'multip_missing': 1},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 5},\n",
        "                        ]\n",
        "print(list_methods_strategy)\n",
        "for el in list_methods_strategy:\n",
        "  for key, value in el.items():\n",
        "    print(key,\": \" , value)\n",
        "\n",
        "print(\"----> Starting experiments\")\n",
        "\n",
        "'''\n",
        "nbr_exp = 2\n",
        "#res[key_tuple]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "#res[key_tuple]['best_coeff'].append(coeff_round)\n",
        "#res[key_tuple]['best_score'].append(score_round)\n",
        "#res[key_tuple]['best_alpha'].append(alpha_round)\n",
        "res_l2 = []\n",
        "\n",
        "rdm_seed = 4654321\n",
        "np.random.seed(rdm_seed)\n",
        "res = run_experiments(dicc, list_methods_strategy)\n",
        "plot_res(info_x_axis, res, dicc)\n",
        "if nbr_exp > 1:\n",
        "  for k in res:\n",
        "    for h in res[k]:\n",
        "      res[k][h] = [res[k][h]]\n",
        "  for i in range(nbr_exp-1):\n",
        "    print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "    #np.random.seed(rdm_seed * (i+2))\n",
        "    res_partial = run_experiments(dicc, list_methods_strategy)\n",
        "    plot_res(info_x_axis, res_partial, dicc)\n",
        "    print(res)\n",
        "    for k in res:\n",
        "      res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "      res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "      res[k]['best_alpha'].append(res_partial[k]['best_alpha'])\n",
        "      #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "    #res.append(res['l2_dist_best_coeff_gt'])\n",
        "\n",
        "print(\"final \")\n",
        "print(res)\n",
        "for k in res:\n",
        "  print(k)\n",
        "  print(np.array(res[k]['l2_dist_best_coeff_gt']))\n",
        "  print(np.mean(np.array(res[k]['l2_dist_best_coeff_gt']), axis=0))\n",
        "  print(np.mean(res[k]['l2_dist_best_coeff_gt'], axis=0))\n",
        "#mean_res = {k: np.mean(v, axis=0) for k, v in res.items()}\n",
        "mean_res = {k: {v: np.mean(w, axis=0) for v, w in res[k].items()} for k in res}\n",
        "for k, v in mean_res.items():\n",
        "  print(\"k:   \", k)\n",
        "  for s, t in v.items():\n",
        "    print(s, \": \", t)\n",
        "#print(np.mean(res, axis=0))\n",
        "'''\n",
        "nbr_exp = 2\n",
        "seed = 432198\n",
        "mean_res = run_multiple_experiments(nbr_exp, seed, dicc, info_x_axis)\n",
        "print(\"PLOT OF THE MEANS\")\n",
        "dicc['title_infer_error'] = 'seed: ' + str(seed) + ', nbr_exp: ' + str(nbr_exp) + ', cov: ' + str(cov_var)\n",
        "dicc['title_test_error'] = 'sigma_err: ' + str(error_vec[0]) + ', n_train: ' + str(n_train[0]) + ', n_test: ' + str(number_test)\n",
        "#dicc = dicc | {'generation':gen, 'title_infer_error':'mean_infer_error, rep: ' + str(nbr_exp), 'title_mean_error':'mean_test_error'}\n",
        "plot_res(info_x_axis, mean_res, dicc)\n",
        "\n",
        "## you can see if you manage to take the index i that maximize alpha\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bXcjBX8GqIAF",
        "outputId": "4693eed8-e3ed-4eea-c64c-02dda5885e1b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change nbr_experiments to match the size of n_train\n",
            "data :  {'data': 'Normal', 'mean': array([0, 0]), 'cov': array([[1. , 0.4],\n",
            "       [0.4, 1. ]])}\n",
            "n_tot :  20100\n",
            "n_train :  100\n",
            "n_test :  20000\n",
            "dim :  2\n",
            "beta_gt :  [-0.5  2. ]\n",
            "perc_test :  0.9950248756218906\n",
            "err :  {'type': 'Gaussian_on_y', 'scaling': 0.1}\n",
            "plots :  []\n",
            "p_miss :  [0.9, 0.04999999999999999, 0.04999999999999999]\n",
            "generation :  f\n",
            "title_infer_error :   \n",
            "title_test_error :   \n",
            "[{'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'multip_dataset': 3, 'multip_missing': 0}, {'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 3, 'multip_missing': 0}, {'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 0, 'multip_missing': 0}, {'imp_method': 'mi', 'post_imp': 'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'sd', 'mi_nbr': 30, 'multip_dataset': 3, 'multip_missing': 1}, {'imp_method': 'mi', 'post_imp': 'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': 30, 'multip_dataset': 3, 'multip_missing': 1}]\n",
            "imp_method :  BR_si\n",
            "cov_strategy :  std_nan\n",
            "multip_dataset :  3\n",
            "multip_missing :  0\n",
            "imp_method :  oracle\n",
            "cov_strategy :  sd\n",
            "multip_dataset :  3\n",
            "multip_missing :  0\n",
            "imp_method :  oracle\n",
            "cov_strategy :  sd\n",
            "multip_dataset :  0\n",
            "multip_missing :  0\n",
            "imp_method :  mi\n",
            "post_imp :  mean\n",
            "cov_strategy_between :  cond_var\n",
            "cov_strategy :  sd\n",
            "mi_nbr :  30\n",
            "multip_dataset :  3\n",
            "multip_missing :  1\n",
            "imp_method :  mi\n",
            "post_imp :  mean\n",
            "cov_strategy_between :  cond_var\n",
            "cov_strategy :  std_nan\n",
            "mi_nbr :  30\n",
            "multip_dataset :  3\n",
            "multip_missing :  1\n",
            "----> Starting experiments\n",
            "[[], [], [], [], []]\n",
            "[]\n",
            "BR_si\n",
            "std_nan\n",
            "3\n",
            "0\n",
            "oracle\n",
            "sd\n",
            "3\n",
            "0\n",
            "oracle\n",
            "sd\n",
            "0\n",
            "0\n",
            "mi\n",
            "mean\n",
            "cond_var\n",
            "sd\n",
            "30\n",
            "3\n",
            "1\n",
            "mi\n",
            "mean\n",
            "cond_var\n",
            "std_nan\n",
            "30\n",
            "3\n",
            "1\n",
            "{'data': 'Normal', 'mean': array([0, 0]), 'cov': array([[1. , 0.4],\n",
            "       [0.4, 1. ]])}\n",
            "(20100, 2)\n",
            "---------------------------------------------------------------------------------------------------------------------------> iteration  0\n",
            "{'data': 'Normal', 'mean': array([0, 0]), 'cov': array([[1. , 0.4],\n",
            "       [0.4, 1. ]])}\n",
            "(20100, 2)\n",
            "----------------------------------------------> new method tested:  {'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'multip_dataset': 3, 'multip_missing': 0}\n",
            "-------> ORACLE SD, std of the original dataset (with no missing) [0.9874573  1.00592884]\n",
            "crush test------------------------------------------------->  -18.14241612467283\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
            "S dataset \n",
            " [[3.01607112 0.        ]\n",
            " [0.         3.02904089]]\n",
            "shape oject in cov strategy missing  2\n",
            "shape oject in cov strategy missing  (20000, 2)\n",
            "S missing shape\n",
            "  (2, 2)\n",
            "S missing\n",
            "  [[0. 0.]\n",
            " [0. 0.]]\n",
            "shape X_imputed in post_imputation  (100, 2)\n",
            "y_train length  100\n",
            "-------> size test:  20000  , size train:  100 nbr_seen (train):  90  nbr_miss :  10\n",
            "X  100 2\n",
            "nm  200\n",
            "S_mis in Adbvt training  [[0. 0.]\n",
            " [0. 0.]]\n",
            "no missing part\n",
            "one matrix in input, S.shape = (n, n)\n",
            "dts deltas  [0.001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 15/150 [00:00<00:00, 149.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██▏       | 32/150 [00:00<00:00, 156.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 49/150 [00:00<00:00, 160.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 66/150 [00:00<00:00, 159.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▌    | 83/150 [00:00<00:00, 162.30it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 100/150 [00:00<00:00, 164.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▊  | 118/150 [00:00<00:00, 167.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 150/150 [00:00<00:00, 158.61it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test shape,  (20000, 2) ,   y_test shape  (20000,)\n",
            "---------------------------------> best idx  120  best hyperp [best_alpha, coef_dts, coef_mis]:  [1.06376485e+02 1.00000000e-03 1.00000000e-01] , min score  0.02542487897755192\n",
            "---------------------------------> best coeff  [-0.37785978  1.89292479]\n",
            "BR_si\n",
            "std_nan\n",
            "3\n",
            "0\n",
            "----------------------------------------------> new method tested:  {'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 3, 'multip_missing': 0}\n",
            "-------> ORACLE SD, std of the original dataset (with no missing) [0.9874573  1.00592884]\n",
            "crush test------------------------------------------------->  -16.051666402274748\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "S dataset \n",
            " [[2.96237191 0.        ]\n",
            " [0.         3.01778651]]\n",
            "shape oject in cov strategy missing  2\n",
            "shape oject in cov strategy missing  (20000, 2)\n",
            "S missing shape\n",
            "  (2, 2)\n",
            "S missing\n",
            "  [[0. 0.]\n",
            " [0. 0.]]\n",
            "shape X_imputed in post_imputation  (100, 2)\n",
            "y_train length  100\n",
            "-------> size test:  20000  , size train:  100 nbr_seen (train):  100  nbr_miss :  0\n",
            "X  100 2\n",
            "nm  200\n",
            "S_mis in Adbvt training  [[0. 0.]\n",
            " [0. 0.]]\n",
            "no missing part\n",
            "one matrix in input, S.shape = (n, n)\n",
            "dts deltas  [0.001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|▋         | 11/150 [00:00<00:01, 108.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 29/150 [00:00<00:00, 146.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31%|███       | 46/150 [00:00<00:00, 155.73it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 64/150 [00:00<00:00, 164.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▍    | 82/150 [00:00<00:00, 168.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 100/150 [00:00<00:00, 170.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▊  | 118/150 [00:00<00:00, 172.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 150/150 [00:00<00:00, 158.83it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test shape,  (20000, 2) ,   y_test shape  (20000,)\n",
            "---------------------------------> best idx  122  best hyperp [best_alpha, coef_dts, coef_mis]:  [1.24153719e+02 1.00000000e-03 1.00000000e-01] , min score  0.02422461537962698\n",
            "---------------------------------> best coeff  [-0.3838534   1.89557225]\n",
            "oracle\n",
            "sd\n",
            "3\n",
            "0\n",
            "----------------------------------------------> new method tested:  {'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 0, 'multip_missing': 0}\n",
            "-------> ORACLE SD, std of the original dataset (with no missing) [0.9874573  1.00592884]\n",
            "crush test------------------------------------------------->  -16.051666402274748\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "S dataset \n",
            " [[0. 0.]\n",
            " [0. 0.]]\n",
            "shape oject in cov strategy missing  2\n",
            "shape oject in cov strategy missing  (20000, 2)\n",
            "S missing shape\n",
            "  (2, 2)\n",
            "S missing\n",
            "  [[0. 0.]\n",
            " [0. 0.]]\n",
            "shape X_imputed in post_imputation  (100, 2)\n",
            "y_train length  100\n",
            "-------> size test:  20000  , size train:  100 nbr_seen (train):  100  nbr_miss :  0\n",
            "X  100 2\n",
            "nm  200\n",
            "S_mis in Adbvt training  [[0. 0.]\n",
            " [0. 0.]]\n",
            "no missing part\n",
            "one matrix in input, S.shape = (n, n)\n",
            "dts deltas  [0.001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 12/150 [00:00<00:01, 117.27it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 30/150 [00:00<00:00, 151.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 49/150 [00:00<00:00, 167.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 45%|████▍     | 67/150 [00:00<00:00, 171.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 86/150 [00:00<00:00, 176.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 105/150 [00:00<00:00, 179.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 125/150 [00:00<00:00, 183.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 150/150 [00:00<00:00, 171.41it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test shape,  (20000, 2) ,   y_test shape  (20000,)\n",
            "---------------------------------> best idx  0  best hyperp [best_alpha, coef_dts, coef_mis]:  [0.01  0.001 0.1  ] , min score  0.010429785434824817\n",
            "---------------------------------> best coeff  [-0.52339783  2.01241619]\n",
            "oracle\n",
            "sd\n",
            "0\n",
            "0\n",
            "----------------------------------------------> new method tested:  {'imp_method': 'mi', 'post_imp': 'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'sd', 'mi_nbr': 30, 'multip_dataset': 3, 'multip_missing': 1}\n",
            "-------> ORACLE SD, std of the original dataset (with no missing) [0.9874573  1.00592884]\n",
            "crush test------------------------------------------------->  -539.6954494038825\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
            "S dataset \n",
            " [1.66533454e-16 0.00000000e+00]\n",
            "S missing shape\n",
            "  (100, 2, 2)\n",
            "S missing\n",
            "  [[[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[1.01637287 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[1.02712232 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.90253101]]\n",
            "\n",
            " [[0.8241865  0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.74097532 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.81348201]]\n",
            "\n",
            " [[0.98619124 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.74629629 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.91820402 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.87738544]]]\n",
            "shape X_imputed in post_imputation  (30, 100, 2)\n",
            "y_train length  100\n",
            "-------> size test:  20000  , size train:  100 nbr_seen (train):  90  nbr_miss :  10\n",
            "X  100 2\n",
            "nm  200\n",
            "S_mis in Adbvt training  [[[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[1.01637287 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[1.02712232 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.90253101]]\n",
            "\n",
            " [[0.8241865  0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.74097532 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.81348201]]\n",
            "\n",
            " [[0.98619124 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.74629629 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.91820402 0.        ]\n",
            "  [0.         0.        ]]\n",
            "\n",
            " [[0.         0.        ]\n",
            "  [0.         0.87738544]]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (200,) and arg 1 with shape (200, 2).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-1359756561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mnbr_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m432198\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mmean_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_multiple_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_x_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLOT OF THE MEANS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_infer_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'seed: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', nbr_exp: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_exp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', cov: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-2353569204.py\u001b[0m in \u001b[0;36mrun_multiple_experiments\u001b[0;34m(nbr_exp, rdm_seed, dictio, info_x_axis)\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0mrdm_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4654321\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdm_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_methods_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0mplot_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_x_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnbr_exp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-2353569204.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(dictio, methods_strategy)\u001b[0m\n\u001b[1;32m     74\u001b[0m           \u001b[0mcoeff_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_2d_ext_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_obser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_imp_cov_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnbr_ima\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnbr_ima\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# == 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m           \u001b[0mcoeff_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_2d_ext_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_obser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_imp_cov_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff_round\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta_gt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0ml2_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-3685170564.py\u001b[0m in \u001b[0;36mexperiment_2d_ext_dataset\u001b[0;34m(dict_obs, dict_imp, ax)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0;31m#alphas_used, coeff_results = train_and_plot(X_train, y_train, S_dict, [ax[1], ax[2]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m       \u001b[0mhyper_p_used\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m       \u001b[0midx_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_idx_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m       \u001b[0;31m#best_coeff, best_alpha = coeff_results[:, idx_best], alphas_used[idx_best]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2275679722.py\u001b[0m in \u001b[0;36mtrain_and_plot\u001b[0;34m(X, y, S_dict, list_ax)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_ax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mlinfadvtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdversarialTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic_h\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mlinfadvtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_hyper_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdic_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mhyper_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs_advtrain_linf\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2275679722.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y, S_dict, p)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0mS_mis_conc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_mis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;31m#np.concatenate([yyy] * 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m           \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_dts_tiled\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madv_radius_times_scale_dts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mS_mis_conc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madv_radius_times_scale_mis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"S type \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0;31m#S = np.concatenate(S)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cvxpy/expressions/expression.py\u001b[0m in \u001b[0;36mcast_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_to_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cvxpy/expressions/expression.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvxtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcvxtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cvxpy/expressions/expression.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(lh_expr, rh_expr)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;31m# Broadcasting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlh_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrh_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlh_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrh_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlh_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlh_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                 \u001b[0mlh_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlh_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_stride_tricks_impl.py\u001b[0m in \u001b[0;36mbroadcast_shapes\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \"\"\"\n\u001b[1;32m    481\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_stride_tricks_impl.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (200,) and arg 1 with shape (200, 2)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DJHHiKFqIUF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "np.random.seed(456789)\n",
        "\n",
        "#info_axis = 'train'\n",
        "#n_train = [400, 800, 1200, 1600, 2000]\n",
        "#p_seen = make_probabilities([0.8, 0.8, 0.8, 0.8, 0.8])\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen\n",
        "#info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "\n",
        "gen = 'fixed'\n",
        "info_axis = 'p_seen'  # train or p_seen\n",
        "#p_seen_both = [1, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.02]\n",
        "#p_seen_both = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "p_seen_both = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "#p_seen_both = [1, 0.9, 0.8]\n",
        "length_vec = len(p_seen_both)\n",
        "#n_train = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
        "n_train = [50] * length_vec\n",
        "error_vec =  [1] * length_vec\n",
        "p_seen = make_probabilities(p_seen_both)\n",
        "if info_axis == 'train':\n",
        "  main_vec = n_train\n",
        "elif info_axis == 'p_seen':\n",
        "  main_vec = p_seen_both\n",
        "elif info_axis == 'error':\n",
        "  main_vec = error_vec\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen_both\n",
        "info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "\n",
        "\n",
        "dicc = make_dictionary_data(\n",
        "    nbr_experiments= len(main_vec), n_train = n_train, n_test=20000,\n",
        "    data = {'data': 'Normal', 'mean': np.array([0, 0]), 'cov': np.array([[1, -0.1], [-0.1, 1]])},\n",
        "    beta_gt = np.array([-0.5, 2]),\n",
        "    p_miss = p_seen,\n",
        "    err_vector = ['Gaussian_on_y', error_vec],\n",
        ")\n",
        "dicc = dicc | {'generation':gen}\n",
        "for key, value in dicc.items():\n",
        "  print(key,\": \" , value[0])\n",
        "\n",
        "# (imp method, cov strategy, mi_nbr)\n",
        "#list_imp_cov_methods = [('BR_si', 'sd'), ('l_d', 'sd'), ('mi', 'sd', 1)]\n",
        "\n",
        "#list_methods_strategy = make_dictionary_method(list_imp_cov_methods)\n",
        "mi_nbr = 30\n",
        "list_methods_strategy = [{'imp_method': 'BR_si', 'cov_strategy': 'eye', 'multip_betw': 1, 'multip_with':1},\n",
        "                        {'imp_method': 'l_d', 'cov_strategy': 'eye', 'multip_betw': 1, 'multip_with':1},\n",
        "                        {'imp_method': 'oracle', 'cov_strategy': 'eye', 'multip_betw': 1, 'multip_with':1},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 1},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 3},\n",
        "                        #{'imp_method': 'mi_pure', 'cov_strategy': 'eye', 'mi_nbr': 2},\n",
        "                        #{'imp_method': 'mi_pure', 'cov_strategy': 'cond_var', 'cov_strategy_within': 'sd', 'mi_nbr': 5},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 1},\n",
        "                        #{'imp_method': 'mi_mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'eye', 'mi_nbr': 5},\n",
        "                        {'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 0, 'multip_with': 0},\n",
        "                        {'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 0, 'multip_with': 1},\n",
        "                        {'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 3, 'multip_with': 0},\n",
        "                        {'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 3, 'multip_with': 1},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 5},\n",
        "                        ]\n",
        "print(list_methods_strategy)\n",
        "for el in list_methods_strategy:\n",
        "  for key, value in el.items():\n",
        "    print(key,\": \" , value)\n",
        "\n",
        "print(\"----> Starting experiments\")\n",
        "res = run_experiments(dicc, list_methods_strategy)\n",
        "plot_res(info_x_axis, res, dicc)\n",
        "\n",
        "\n",
        "\n",
        "## you can see if you manage to take the index i that maximize alpha\n",
        "'''\n"
      ],
      "metadata": {
        "id": "6FhKy2v5qIb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ab49ab0c-c675-4a48-fd8d-f78505d861ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnp.random.seed(456789)\\n\\n#info_axis = \\'train\\'\\n#n_train = [400, 800, 1200, 1600, 2000]\\n#p_seen = make_probabilities([0.8, 0.8, 0.8, 0.8, 0.8])\\n#main_vec = n_train if info_axis == \\'train\\' else p_seen\\n#info_x_axis = make_info_axis(main_vec, info_axis)\\n\\ngen = \\'fixed\\'\\ninfo_axis = \\'p_seen\\'  # train or p_seen\\n#p_seen_both = [1, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.02]\\n#p_seen_both = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\\np_seen_both = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\\n#p_seen_both = [1, 0.9, 0.8]\\nlength_vec = len(p_seen_both)\\n#n_train = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\\nn_train = [50] * length_vec\\nerror_vec =  [1] * length_vec\\np_seen = make_probabilities(p_seen_both)\\nif info_axis == \\'train\\':\\n  main_vec = n_train\\nelif info_axis == \\'p_seen\\':\\n  main_vec = p_seen_both\\nelif info_axis == \\'error\\':\\n  main_vec = error_vec\\n#main_vec = n_train if info_axis == \\'train\\' else p_seen_both\\ninfo_x_axis = make_info_axis(main_vec, info_axis)\\n\\n\\ndicc = make_dictionary_data(\\n    nbr_experiments= len(main_vec), n_train = n_train, n_test=20000,\\n    data = {\\'data\\': \\'Normal\\', \\'mean\\': np.array([0, 0]), \\'cov\\': np.array([[1, -0.1], [-0.1, 1]])},\\n    beta_gt = np.array([-0.5, 2]),\\n    p_miss = p_seen,\\n    err_vector = [\\'Gaussian_on_y\\', error_vec],\\n)\\ndicc = dicc | {\\'generation\\':gen}\\nfor key, value in dicc.items():\\n  print(key,\": \" , value[0])\\n\\n# (imp method, cov strategy, mi_nbr)\\n#list_imp_cov_methods = [(\\'BR_si\\', \\'sd\\'), (\\'l_d\\', \\'sd\\'), (\\'mi\\', \\'sd\\', 1)]\\n\\n#list_methods_strategy = make_dictionary_method(list_imp_cov_methods)\\nmi_nbr = 30\\nlist_methods_strategy = [{\\'imp_method\\': \\'BR_si\\', \\'cov_strategy\\': \\'eye\\', \\'multip_betw\\': 1, \\'multip_with\\':1},\\n                        {\\'imp_method\\': \\'l_d\\', \\'cov_strategy\\': \\'eye\\', \\'multip_betw\\': 1, \\'multip_with\\':1},\\n                        {\\'imp_method\\': \\'oracle\\', \\'cov_strategy\\': \\'eye\\', \\'multip_betw\\': 1, \\'multip_with\\':1},\\n                        #{\\'imp_method\\': \\'mi\\', \\'cov_strategy\\': \\'RR\\', \\'mi_nbr\\': 1},\\n                        #{\\'imp_method\\': \\'mi\\', \\'cov_strategy\\': \\'RR\\', \\'mi_nbr\\': 3},\\n                        #{\\'imp_method\\': \\'mi_pure\\', \\'cov_strategy\\': \\'eye\\', \\'mi_nbr\\': 2},\\n                        #{\\'imp_method\\': \\'mi_pure\\', \\'cov_strategy\\': \\'cond_var\\', \\'cov_strategy_within\\': \\'sd\\', \\'mi_nbr\\': 5},\\n                        #{\\'imp_method\\': \\'mi\\', \\'post_imp\\':\\'mean\\', \\'cov_strategy_between\\': \\'cond_var\\', \\'cov_strategy\\': \\'zero\\', \\'mi_nbr\\': mi_nbr, \\'multip_betw\\': 1, \\'multip_with\\': 1},\\n                        #{\\'imp_method\\': \\'mi_mean\\', \\'cov_strategy_between\\': \\'cond_var\\', \\'cov_strategy\\': \\'eye\\', \\'mi_nbr\\': 5},\\n                        {\\'imp_method\\': \\'mi\\', \\'post_imp\\':\\'mean\\', \\'cov_strategy_between\\': \\'cond_var\\', \\'cov_strategy\\': \\'RR\\', \\'mi_nbr\\': mi_nbr, \\'multip_betw\\': 0, \\'multip_with\\': 0},\\n                        {\\'imp_method\\': \\'mi\\', \\'post_imp\\':\\'mean\\', \\'cov_strategy_between\\': \\'cond_var\\', \\'cov_strategy\\': \\'RR\\', \\'mi_nbr\\': mi_nbr, \\'multip_betw\\': 0, \\'multip_with\\': 1},\\n                        {\\'imp_method\\': \\'mi\\', \\'post_imp\\':\\'mean\\', \\'cov_strategy_between\\': \\'cond_var\\', \\'cov_strategy\\': \\'RR\\', \\'mi_nbr\\': mi_nbr, \\'multip_betw\\': 3, \\'multip_with\\': 0},\\n                        {\\'imp_method\\': \\'mi\\', \\'post_imp\\':\\'mean\\', \\'cov_strategy_between\\': \\'cond_var\\', \\'cov_strategy\\': \\'RR\\', \\'mi_nbr\\': mi_nbr, \\'multip_betw\\': 3, \\'multip_with\\': 1},\\n                        #{\\'imp_method\\': \\'mi\\', \\'cov_strategy\\': \\'RR\\', \\'mi_nbr\\': 5},\\n                        ]\\nprint(list_methods_strategy)\\nfor el in list_methods_strategy:\\n  for key, value in el.items():\\n    print(key,\": \" , value)\\n\\nprint(\"----> Starting experiments\")\\nres = run_experiments(dicc, list_methods_strategy)\\nplot_res(info_x_axis, res, dicc)\\n\\n\\n\\n## you can see if you manage to take the index i that maximize alpha\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "840TDfGaqIei"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SijOuiZYgmon"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TG_Fsg4Mgmrz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRfmkoYlgmuk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vbxlxxNgmyD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zJgEeongm1F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkEtCuWEgm3H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjbzANmcg_cT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JsQXdlOg_fX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4FH2Zo7g_j9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxm1VFuIg_q_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cxU4-RZg_uA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktlqpLJ9g_ws"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doiemfrVg_zR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bzoCpqTg_10"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwoz_Lp2g_4Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-Kc_0IPg_7I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMqppMFSgm5F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSeaLOr-qLYh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azRmYEueqLbT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4l2vYHzqLd5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7X3AQIZqLgz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iv34YkpnqLju"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCRKDvv-qLmZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJ6b5Zd_vZ8U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7swocNpvZ_f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cxt5tWj1vaCJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCvPWMOtvaE7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9kxssOuRMv5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWcLfNW2RM1F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S5O3hTLTvaIU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fE8VHJ90vaMP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZnN2xpSvaOs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze5Q9M4Tycsm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzWhOlCoycwe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZMXiJW-ycz-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAINCofcyc20"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uORnN_wO05n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQ441A0hO09K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iF_8aBrWO1Aa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHLiMt5wO1Dh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gB_XEEFvaQy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFwxXhyWefs2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Kw8bLBTefv7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXN7C2Jfefzt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeFvCalref4n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "An113TsYef7-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kF5dEO1zef-e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zuC2em6egA6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MM7Zk7OWegDa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVL97vbaegFz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA3cOcmeegJ6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FpYA6ClegNV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3MxLnLIqLol"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DgSOhmwgm7O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSdrq6HmqIhD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4nfPNbTqIjf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4xXWwHeeMR6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oonp7YBzeMUh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnVLZhvbeMXR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9Yk_s6leMZ_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5asNezNqImF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwGCYcYWqIrC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6dLlbTgqIt4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lu0iNCNHc_0N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "031VAAc5c_4Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quVErgChc_-h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVqmuefndAEJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7M3O9KqdAL-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LN_xYsFMdAQx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wo4YT1OODeeU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wl-gtIlyDeh2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnKMsLPgDemY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKCZUoDYDesF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1vpjYZ9dAVM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1Fx16kedAX8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randint(2, 5, size=(2, 2, 2))\n",
        "print(X)\n",
        "\n",
        "XX = np.concatenate(X)\n",
        "print(XX)\n",
        "\n",
        "\n",
        "Y = np.random.randint(2, 5, size=(1, 3, 2))\n",
        "print(Y)\n",
        "\n",
        "YY = np.concatenate(Y)\n",
        "print(YY)\n",
        "\n",
        "\n",
        "Z = np.random.randint(2, 5, size=(5, 2))\n",
        "print(Z)\n",
        "\n",
        "ZZ = np.concatenate(Z)\n",
        "print(ZZ)\n",
        "\n",
        "print(\"other\")\n",
        "s = np.random.randint(2, 4, 5)\n",
        "print(s)\n",
        "z = np.tile(s, reps=3)  # np.array([s] * 2)\n",
        "print(z)\n",
        "\n",
        "\n",
        "print(\"other mult\")\n",
        "s = np.random.randint(2, 8, size=(3, 2))\n",
        "print(s)\n",
        "z = np.tile(s, reps=(3, 1))  # np.array([s] * 2)\n",
        "print(z)\n"
      ],
      "metadata": {
        "id": "J-KLwpDqTkGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc40ce8-bb9c-416d-d73e-d7207f21cd00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[4 4]\n",
            "  [2 2]]\n",
            "\n",
            " [[4 2]\n",
            "  [2 4]]]\n",
            "[[4 4]\n",
            " [2 2]\n",
            " [4 2]\n",
            " [2 4]]\n",
            "[[[3 2]\n",
            "  [3 2]\n",
            "  [4 4]]]\n",
            "[[3 2]\n",
            " [3 2]\n",
            " [4 4]]\n",
            "[[2 4]\n",
            " [4 2]\n",
            " [3 3]\n",
            " [4 2]\n",
            " [4 3]]\n",
            "[2 4 4 2 3 3 4 2 4 3]\n",
            "other\n",
            "[3 2 3 2 2]\n",
            "[3 2 3 2 2 3 2 3 2 2 3 2 3 2 2]\n",
            "other mult\n",
            "[[2 5]\n",
            " [2 2]\n",
            " [6 5]]\n",
            "[[2 5]\n",
            " [2 2]\n",
            " [6 5]\n",
            " [2 5]\n",
            " [2 2]\n",
            " [6 5]\n",
            " [2 5]\n",
            " [2 2]\n",
            " [6 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBkG1_lacBfs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBILRQvDuI4y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dPT52NAbKyT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2ShK9JYb6c_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fk7A_5N_c1gV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgcEt2LBhEBK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FiP-uNujLRK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6uENE-JShLaC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JeqAEblFooY_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXByx8OrjqZe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4TCzI5siopUy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4pGls4IpDT2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWpiTpQ5lVg1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ASCmfdEnvjn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jq6GembmmAV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nb6YB8DbvKVN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9pv_OW7pJtp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgNt4tVYQk7-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlebwxRZ1_QW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1dmiXA-FcI_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDRrQMKGU3Ji"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnNTv-mXVIB6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBlyABpt0-qa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQgiJb-V-hIH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCuJj9HPb2cx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VM2QQJkmcsdX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## random forest imputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_estimator = RandomForestRegressor(n_estimators=4, max_depth=10, bootstrap=True, max_samples=0.5, n_jobs=2, random_state=0)\n",
        "\n",
        "X_rf = single_imputation(X_nan, rf_estimator)\n",
        "print(X_rf.shape)\n",
        "sd_rf = np.std(X_rf, axis=0)\n",
        "S_inv_rf = np.diag(1 / sd_rf)\n",
        "print(\"std_orig: \\n\", np.std(X_orig, axis=0))\n",
        "print(\"std rf\\n \", sd_rf)\n",
        "fig, ax = plt.subplots(num='advtrain_linf_rf')\n",
        "linfadvtrain_rf = AdversarialTraining(X_rf, y, S_inv_rf, p=np.inf)\n",
        "estimator_rf = lambda X, y, a:  linfadvtrain_rf(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_rf  = get_path(X_rf, y, estimator_rf, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_rf, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "uSgnV3aVXL1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9fe486e3-823c-4c98-dccb-eb5edf5598c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## random forest imputer\\nfrom sklearn.ensemble import RandomForestRegressor\\nrf_estimator = RandomForestRegressor(n_estimators=4, max_depth=10, bootstrap=True, max_samples=0.5, n_jobs=2, random_state=0)\\n\\nX_rf = single_imputation(X_nan, rf_estimator)\\nprint(X_rf.shape)\\nsd_rf = np.std(X_rf, axis=0)\\nS_inv_rf = np.diag(1 / sd_rf)\\nprint(\"std_orig: \\n\", np.std(X_orig, axis=0))\\nprint(\"std rf\\n \", sd_rf)\\nfig, ax = plt.subplots(num=\\'advtrain_linf_rf\\')\\nlinfadvtrain_rf = AdversarialTraining(X_rf, y, S_inv_rf, p=np.inf)\\nestimator_rf = lambda X, y, a:  linfadvtrain_rf(adv_radius=a)\\nalphas_adv, coefs_advtrain_linf_rf  = get_path(X_rf, y, estimator_rf, 1e1)\\nplot_coefs_l1norm(coefs_advtrain_linf_rf, ax)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## iterative imputer Bayesian Ridge\n",
        "\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "br_estimator = BayesianRidge()\n",
        "\n",
        "X_br = single_imputation(X_nan, br_estimator)\n",
        "sd_br = np.std(X_br, axis=0)\n",
        "S_inv_br = np.diag(1 / sd_br)\n",
        "print(\"std_orig: \\n\", np.std(X_orig, axis=0))\n",
        "print(\"std  br\\n \", sd_br)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_br')\n",
        "linfadvtrain_br = AdversarialTraining(X_br, y, S_inv_br, p=np.inf)\n",
        "estimator_br = lambda X, y, a:  linfadvtrain_br(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_br  = get_path(X_br, y, estimator_br, 1e4)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_br, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "pgNaP74gWAga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7d310efd-799b-4a7b-8bb0-2c0b4388391c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## iterative imputer Bayesian Ridge\\n\\nfrom sklearn.linear_model import BayesianRidge\\nbr_estimator = BayesianRidge()\\n\\nX_br = single_imputation(X_nan, br_estimator)\\nsd_br = np.std(X_br, axis=0)\\nS_inv_br = np.diag(1 / sd_br)\\nprint(\"std_orig: \\n\", np.std(X_orig, axis=0))\\nprint(\"std  br\\n \", sd_br)\\n\\nfig, ax = plt.subplots(num=\\'advtrain_linf_br\\')\\nlinfadvtrain_br = AdversarialTraining(X_br, y, S_inv_br, p=np.inf)\\nestimator_br = lambda X, y, a:  linfadvtrain_br(adv_radius=a)\\nalphas_adv, coefs_advtrain_linf_br  = get_path(X_br, y, estimator_br, 1e4)\\nplot_coefs_l1norm(coefs_advtrain_linf_br, ax)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## mean imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X_mean = imp_mean.fit_transform(X_nan)\n",
        "sd_mean = np.std(X_mean, axis=0)\n",
        "print(sd_mean)\n",
        "S_inv_mean = np.diag(1 / sd_mean)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_mean')\n",
        "linfadvtrain_mean = AdversarialTraining(X_mean, y, S_inv_mean, p=np.inf)\n",
        "estimator_mean = lambda X, y, a:  linfadvtrain_mean(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_mean  = get_path(X_mean, y, estimator_mean, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_mean, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "u0kpCJCkFbcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e8300b1a-0310-45fc-a301-2295514c8250"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n## mean imputation\\nfrom sklearn.impute import SimpleImputer\\n\\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\\nX_mean = imp_mean.fit_transform(X_nan)\\nsd_mean = np.std(X_mean, axis=0)\\nprint(sd_mean)\\nS_inv_mean = np.diag(1 / sd_mean)\\n\\nfig, ax = plt.subplots(num='advtrain_linf_mean')\\nlinfadvtrain_mean = AdversarialTraining(X_mean, y, S_inv_mean, p=np.inf)\\nestimator_mean = lambda X, y, a:  linfadvtrain_mean(adv_radius=a)\\nalphas_adv, coefs_advtrain_linf_mean  = get_path(X_mean, y, estimator_mean, 1e1)\\nplot_coefs_l1norm(coefs_advtrain_linf_mean, ax)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# imputation elliptic\n",
        "\n",
        "mu = np.nanmean(X_nan, axis=0)\n",
        "print(\"means \", mu)\n",
        "delta = np.mean(masks) # parameter missingness\n",
        "print(\"delta \", delta)\n",
        "X_0 = np.nan_to_num(X_nan)\n",
        "print(\"nbr obs\", X_0.shape[0])\n",
        "S_ellp =  X_0.T @ X_0 / X_0.shape[0]\n",
        "S_ellp = (1/delta - 1/(delta**2)) * np.diag(np.diag(S_ellp)) + 1/(delta**2) * S_ellp\n",
        "print(\"eig cov \", np.linalg.eigvalsh(S_ellp))\n",
        "X_ellp = imputation_elliptic(mu, S_ellp, X_nan, masks)\n",
        "#S_inv_ellp = np.linalg.inv(S_ellp)  # other variance\n",
        "sd_inv_ellp = np.std(X_ellp, axis=0)\n",
        "print(\"sd ellp\", sd_inv_ellp)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_ellp')\n",
        "linfadvtrain_ellp = AdversarialTraining(X_ellp, y, S_ellp, p=np.inf)\n",
        "estimator_ellp = lambda X, y, a:  linfadvtrain_ellp(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_ellp  = get_path(X_ellp, y, estimator_ellp, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_ellp, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "2RYR4_BJhXjv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d56ad9f7-1d50-415e-9b39-6aff1e878a7e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# imputation elliptic\\n\\nmu = np.nanmean(X_nan, axis=0)\\nprint(\"means \", mu)\\ndelta = np.mean(masks) # parameter missingness\\nprint(\"delta \", delta)\\nX_0 = np.nan_to_num(X_nan)\\nprint(\"nbr obs\", X_0.shape[0])\\nS_ellp =  X_0.T @ X_0 / X_0.shape[0]\\nS_ellp = (1/delta - 1/(delta**2)) * np.diag(np.diag(S_ellp)) + 1/(delta**2) * S_ellp\\nprint(\"eig cov \", np.linalg.eigvalsh(S_ellp))\\nX_ellp = imputation_elliptic(mu, S_ellp, X_nan, masks)\\n#S_inv_ellp = np.linalg.inv(S_ellp)  # other variance\\nsd_inv_ellp = np.std(X_ellp, axis=0)\\nprint(\"sd ellp\", sd_inv_ellp)\\n\\nfig, ax = plt.subplots(num=\\'advtrain_linf_ellp\\')\\nlinfadvtrain_ellp = AdversarialTraining(X_ellp, y, S_ellp, p=np.inf)\\nestimator_ellp = lambda X, y, a:  linfadvtrain_ellp(adv_radius=a)\\nalphas_adv, coefs_advtrain_linf_ellp  = get_path(X_ellp, y, estimator_ellp, 1e1)\\nplot_coefs_l1norm(coefs_advtrain_linf_ellp, ax)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mlM-FR-OfL4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgxEbR071wT-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pwDiPU0D_ks"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BdM7Mk_mjf0t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYa8pmuMk4jq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMwgzXI1_rEu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Example data\n",
        "x_test_rect = np.random.rand(10)\n",
        "y_test_rect = np.random.rand(10)\n",
        "\n",
        "# Plot the points\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x_test_rect, y_test_rect)\n",
        "\n",
        "width = 0.1\n",
        "height = 0.1\n",
        "\n",
        "add_rectangles(x_test_rect, y_test_rect, width, height, ax)\n",
        "\n",
        "# Add the rectangle to the plot\n"
      ],
      "metadata": {
        "id": "-9qaVcwZUB6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "8aa7b6ef-06e1-462f-e139-de3cc300f983"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "add_rectangles() takes 4 positional arguments but 5 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-3084510147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0madd_rectangles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_rect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_rect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Add the rectangle to the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: add_rectangles() takes 4 positional arguments but 5 were given"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WZeO2EOWHwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell for some tests\n",
        "\n",
        "def test_clear_dataset(n, d):\n",
        "  print(\"test clear dataset\")\n",
        "  X = np.random.randint(1, 3, size=(n, d))\n",
        "  y = np.random.randint(1, 3, size=n)\n",
        "  masks = np.random.binomial(1, 0.3, size=(n, d))\n",
        "  print(\"X \\n\", X)\n",
        "  print(\"y\\n\", y)\n",
        "  print(\"masks \\n\", masks)\n",
        "  masks[:, 0] = np.ones(n)\n",
        "  masks[0, :] = np.ones(d)\n",
        "  X_res, y_res, masks_res = clear_dataset(X, y, masks)\n",
        "  print(\"X_res \\n\", X_res)\n",
        "  print(\"y\\n\", y_res)\n",
        "  print(\"masks \\n\", masks_res)\n",
        "  print(\"test clear dataset ended successfully\")\n",
        "\n",
        "def test_generate_X():\n",
        "    print(\"test generate_X started\")\n",
        "    fig, ax = plt.subplots(3, 1, figsize=(10, 8), num='advtrain_linf')\n",
        "    gen = generate_X('circles', 2)\n",
        "    data = gen(1000)\n",
        "    print(data.shape)\n",
        "    ax[0].scatter(data[:, 0], data[:, 1])\n",
        "    print(\"test generate passed syccessfully\")\n",
        "\n",
        "def test_preparation_dataset(n, d):\n",
        "      print(\"\\ntest preparation dataset started\")\n",
        "      X_train = np.random.rand(n, d)\n",
        "      print(\"X_train \\n\", X_train)\n",
        "      mask = np.random.binomial(1, 0.5, (n, d))\n",
        "      print(\"mask, 0 seen, 1 missing \\n \", mask)\n",
        "      X_masked = X_train * (1 - mask)\n",
        "      print(\"X_masked \\n\", X_masked)\n",
        "      X_nan_train = X_train.copy()\n",
        "      X_nan_train[mask == 1] = np.nan\n",
        "      print(\"X_nan_train \\n\", X_nan_train)\n",
        "      X_br_train = single_imputation(X_nan_train, BayesianRidge())\n",
        "      print(\"X_br_train\\n \", X_br_train)\n",
        "\n",
        "      print(\"what happens if we run single_imputation of full dataset\")\n",
        "      X_br_full = single_imputation(X_train, BayesianRidge())\n",
        "      print(\"X_br_full\\n \", X_br_full)\n",
        "      np.testing.assert_allclose(X_train, X_br_full)  # shuold be untouched\n",
        "      print(\"test preparation dataset ended successfully\")\n",
        "\n",
        "def test_listwise_delection(n, d):\n",
        "    print(\"\\n test list_wise delection started\")\n",
        "    X = np.random.rand(n, d)\n",
        "    print(\"data\\n\", X)\n",
        "    mask = np.random.binomial(1, 0.2, (n, d))\n",
        "    print(\"mask \\n\", mask)\n",
        "    X_ld = listwise_delection(X, mask)\n",
        "    print(\"after calling function, X_ld \\n\", X_ld)\n",
        "\n",
        "    print(\"edge cases, all missing\")\n",
        "    mask_1 = np.ones_like(X)  # all missing\n",
        "    X1 = listwise_delection(X, mask_1)\n",
        "    print(\"X1 \\n\", X1)  # should be empty\n",
        "    mask_0 = np.zeros_like(X)  # all seen\n",
        "    X0 = listwise_delection(X, mask_0)\n",
        "    print(\"X0 \\n\", X0)\n",
        "    np.testing.assert_allclose(X0, X)  # should be the original dataset\n",
        "\n",
        "    print(\"one dimnsional array\")\n",
        "    y = np.random.rand(n)\n",
        "    print(\"y before \", y)\n",
        "    y_ld = listwise_delection(y, mask)\n",
        "    print(\"y after ld \", y_ld)\n",
        "    print(\"test listwise_delection passed\")\n",
        "\n",
        "\n",
        "test_generate_X()\n",
        "test_preparation_dataset(3, 4)\n",
        "test_listwise_delection(3, 4)\n",
        "test_clear_dataset(6, 3)\n",
        "\n",
        "xxx = np.random.randint(2, 5, size=(3, 3)) * 1.0\n",
        "mmm = np.random.binomial(1, 0.5, size=(3, 3))\n",
        "print(xxx)\n",
        "print(mmm)\n",
        "print(mmm == 1)\n",
        "print(xxx[mmm == 1])\n",
        "xxx[mmm == 1] = np.nan\n",
        "print(xxx)\n",
        "mask_from_xxx = np.isnan(xxx).astype(int)\n",
        "print(\"mask from xxx \\n\", mask_from_xxx)\n"
      ],
      "metadata": {
        "id": "SDHMAeapZVgK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test best predictor\n",
        "\n",
        "def test_best_predictor(n, d, nb_coeff):\n",
        "  X_test = np.random.randint(1, 9, size=(n, d))\n",
        "  beta_gt_test = np.random.randint(1, 7, size=d)\n",
        "  y_test = X_test @ beta_gt_test\n",
        "  #print(\"X_test \\n\", X_test, \"\\n beta_gt\", beta_gt_test, \"\\n y_test = X_test @ beta_gt_test \", y_test)\n",
        "  coeff_test = np.random.randint(1, 5, size=(d, nb_coeff))\n",
        "  rdm_idx = np.random.randint(1, d+1, size=1)\n",
        "  print(rdm_idx)\n",
        "  #print(\"coeff test partial \", coeff_test[:, -1])\n",
        "  rng = np.arange(nb_coeff)\n",
        "  #print(rng != rdm_idx)\n",
        "  coeff_test[:, rng != rdm_idx] = coeff_test[:, rng != rdm_idx] + 1000  # increase artificially the value of the other coefficient, to induce the minimum index to be rdm_idx\n",
        "  #print(\"coeff_test \\n\", coeff_test)\n",
        "  best_coeff, best_score = best_predictor(X_test, coeff_test, y_test)\n",
        "  print(\"best coeff \", best_coeff)\n",
        "  print(\"best score \", best_score)\n",
        "  np.testing.assert_allclose(best_coeff, coeff_test[:,rdm_idx].squeeze())\n",
        "  print(\"test best predictor passed\")\n",
        "\n",
        "test_best_predictor(100, 5, 20)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YJk1Yaj1ReIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test train_and_plot\n",
        "\n",
        "X_diab, y_diab = datasets.load_diabetes(return_X_y=True)\n",
        "n, d = X_diab.shape\n",
        "print(\"n:  \", n, \", d: \", d)\n",
        "# Standardize data\n",
        "X_diab -= X_diab.mean(axis=0)\n",
        "X_diab /= X_diab.std(axis=0)\n",
        "\n",
        "## original lasso\n",
        "fig_l, ax_l = plt.subplots(num='lasso')\n",
        "alphas_lasso, coefs_lasso, _ = get_lasso_path(X_diab, y_diab)\n",
        "plot_coefs_l1norm(coefs_lasso, ax_l)\n",
        "\n",
        "## Antonio's algo, 1 matrix\n",
        "S_diab_eye = np.eye(X_diab.shape[1])\n",
        "fig, ax_1 = plt.subplots(1, 1, num='advtrain_linf_diab')\n",
        "fig, ax_2 = plt.subplots(1, 1, num='advtrain_linf_diab_2')\n",
        "train_and_plot(X_diab, y_diab, S_diab_eye, [ax_1, ax_2])\n",
        "\n",
        "## Antonio's algo, multiple diagonal matrix\n",
        "#S_diab = np.eye(X_diab.shape[1])\n",
        "#S_diab = np.random.randint(1, 3, size=(n, d))\n",
        "#print(S_diab)\n",
        "#fig, ax_5 = plt.subplots(1, 1, num='advtrain_linf_diab_5')\n",
        "#fig, ax_6 = plt.subplots(1, 1, num='advtrain_linf_diab_6')\n",
        "#train_and_plot(X_diab, y_diab, S_diab, [ax_5, ax_6])\n",
        "\n",
        "\n",
        "## Antonio's algo, multiple matrices (same matrix stacked multiple time)\n",
        "S_diab_stacked = np.array([S_diab_eye] * X_diab.shape[0])\n",
        "S_diab_stacked = np.concatenate(S_diab_stacked)\n",
        "fig, ax_3 = plt.subplots(1, 1, num='advtrain_linf_diab_3')\n",
        "fig, ax_4 = plt.subplots(1, 1, num='advtrain_linf_diab_4')\n",
        "train_and_plot(X_diab, y_diab, S_diab_stacked, [ax_3, ax_4])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KjpHk0mYdiFh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test imputations\n",
        "\n",
        "np.random.seed(45)\n",
        "\n",
        "\n",
        "def test_imputations(n, d):\n",
        "  X = np.random.randint(2, 5, size=(n, d)) * 1.0\n",
        "  y = X @ np.random.randint(1, 3, size=d)\n",
        "  m = np.random.binomial(1, 0.4, size=(n, d))  # 1 missing, 0 seen\n",
        "  print(\"m original\\n\", m)\n",
        "  X, y, m = clear_dataset(X, y, m)\n",
        "  print(m)\n",
        "  X_nan = X.copy()\n",
        "  X_nan[m == 1] = np.nan\n",
        "\n",
        "  #mask_from_xxx = np.isnan(xxx).astype(int)\n",
        "  print(\"X\\n \", X)\n",
        "  print(\"masks \\n\", m)\n",
        "  print(\"X_nan\\n \", X_nan)\n",
        "  methods = ['BR_si', 'mi', 'l_d']\n",
        "  nbr_mi = [1, 3]\n",
        "  #for method in methods:\n",
        "  #  dict_info = {'imp_method': method, 'mi_nbr':nbr_mi}\n",
        "  #dict_info = {'imp_method':methods, 'mi_nbr':nbr_mi}\n",
        "  for method in methods:\n",
        "    print(\"---------- method: \", method)\n",
        "    if method == 'mi':\n",
        "      for x in nbr_mi:\n",
        "        print(\"-------------------- nbr mi: \", x)\n",
        "        dict_info = {'imp_method':method, 'mi_nbr':x}\n",
        "        #print(\"XNANNANAN \", X_nan)\n",
        "        X_res, y_res, mask_res = imputations(dict_info, X_nan, y)\n",
        "        print(X_res, y_res, \"\\n\", mask_res)\n",
        "    else:\n",
        "      dict_info = {'imp_method': method}\n",
        "      X_res, y_res, mask_res = imputations(dict_info, X_nan, y)\n",
        "      print(X_res, y_res, \"\\n\", mask_res)\n",
        "    print(\"test imputations ended successfully\")\n",
        "\n",
        "test_imputations(6, 3)\n"
      ],
      "metadata": {
        "id": "z5crxb1usyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "y = np.array([1, 2])\n",
        "x.append(y)\n",
        "x.append(y)\n",
        "x.append(y)\n",
        "xx = np.stack(x)\n",
        "print(x)\n",
        "print(xx)\n",
        "print(type(xx))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sizes = [100, 1000, 10000, 100000]\n",
        "values = [0.8, 0.85, 0.9, 0.92]\n",
        "positions = range(len(sizes))\n",
        "\n",
        "plt.plot(positions, values, marker='o', label='Model Accuracy')  # Add label here\n",
        "plt.xticks(positions, sizes)\n",
        "\n",
        "plt.xlabel(\"Dataset Size (equispaced)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Performance vs Dataset Size (equispaced x-axis)\")\n",
        "#plt.legend()  # Show legend\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GU2RjW63SNaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dictio = {'a':1, 'b':2, 'c':3}\n",
        "vv = dictio.values()\n",
        "#print(vv)\n",
        "#print(vv[1])\n",
        "\n",
        "x1 = np.array([1, 2, 3])\n",
        "x2 = np.array([3, 2 ,1])\n",
        "v = np.maximum(x1, x2)\n",
        "print(v)\n"
      ],
      "metadata": {
        "id": "UE1NuR4D2h8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "648zfFp8ERD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n, d = 2, 3, 2\n",
        "x_int = np.random.randint(1, 9, (m, n, d))\n",
        "print(x_int)\n",
        "s = np.std(x_int, axis=0)\n",
        "print(s)\n",
        "\n",
        "# manual\n",
        "print(\"manual computation\")\n",
        "x = np.zeros((m, d))\n",
        "for i in range(n):\n",
        "  print(\"i -----> \", i)\n",
        "  x = x_int[:, i, :]\n",
        "  print(\"x\\n\", x)\n",
        "  ss = np.std(x, axis=0)\n",
        "  print(ss)\n",
        "\n",
        "\n",
        "print(\"little exp on squeeze\")\n",
        "sss = np.random.rand(1, 3, 3)\n",
        "print(sss)\n",
        "print(sss.squeeze())\n",
        "print(sss.squeeze())\n",
        "\n"
      ],
      "metadata": {
        "id": "vpBvPibeERnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int(34.99)\n",
        "\n",
        "xxxx = np.random.randint(2, 4, (5, 2))\n",
        "print(xxxx)\n",
        "xxxx[0:2, :] = 1\n",
        "print(xxxx)\n",
        "\n",
        "print(\"yyyy\\n\")\n",
        "yy = []\n",
        "yy.append([1, 2, 3])\n",
        "yy.append([4, 5, 6])\n",
        "print(yy)\n",
        "print(np.stack( yy ).T)\n",
        "print(\"\\n\\n\")\n",
        "yyy = np.random.randint(1, 10, size=(3 , 3))\n",
        "print(yyy)\n",
        "yyy_a = np.array([yyy] * 2)\n",
        "print(yyy_a.shape)\n",
        "print(np.concatenate([yyy] * 2))\n",
        "#print(np.tile(yyy_a, (2, 1, 1) ))\n",
        "\n",
        "zzz = np.zeros((2, 2))\n",
        "\n",
        "np.sum(np.zeros((2, 2)) == zzz)"
      ],
      "metadata": {
        "id": "et578OpzERsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def multiple_imputation1(nbr_mi, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    res = np.zeros((nbr_mi, n, d))\n",
        "    for i in range(nbr_mi):\n",
        "       n_i = np.random.randint(0, 1000)\n",
        "       ice = IterativeImputer(random_state=n_i, max_iter=50, sample_posterior=True)\n",
        "       res[i, :, :] = ice.fit_transform(X_nan)\n",
        "       #print(\"fin res shape\", res.shape)\n",
        "       #if nbr_mi == 1:\n",
        "        #res = res[0, :, :]\n",
        "        #print(\"fin res shape\", res.shape)\n",
        "    return res\n",
        "\n",
        "\n",
        "Xx = np.random.randint(1, 3, (4, 4)) * 1.0\n",
        "mm = np.random.binomial(1, 0.25, (4, 4))\n",
        "print(Xx)\n",
        "print(mm)\n",
        "Xx[mm == 1] = np.nan\n",
        "print(Xx)\n",
        "\n",
        "ice = IterativeImputer(random_state=18, max_iter=50, sample_posterior=True)\n",
        "ice.fit(Xx)\n",
        "XxX = np.random.randint(1, 3, (2, 4)) * 1.0\n",
        "mmM = np.random.binomial(1, 0.5, (2, 4))\n",
        "print(XxX)\n",
        "print(mmM)\n",
        "XxX[mmM == 1] = np.nan\n",
        "print(XxX)\n",
        "\n",
        "print(ice.transform(XxX))\n",
        "print(ice.transform(XxX))\n",
        "\n",
        "print(\"new\")\n",
        "\n",
        "ls = [[[]],[[]]]\n",
        "print(ls)\n",
        "ls[0][0]\n",
        "\n"
      ],
      "metadata": {
        "id": "_U_r_qaJ9pw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}