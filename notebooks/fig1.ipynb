{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea987/advtrain-linreg/blob/main/notebooks/fig1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install miceforest"
      ],
      "metadata": {
        "id": "6Sgo-CifolM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fdf83c-f744-476c-df85-fd1da699aefb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: miceforest in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: lightgbm>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from miceforest) (4.6.0)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from miceforest) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from miceforest) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from miceforest) (1.16.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from miceforest) (18.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->miceforest) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->miceforest) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.0->miceforest) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->miceforest) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --force-reinstall tensorflow"
      ],
      "metadata": {
        "id": "7-UDWIhewLY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gcimpute"
      ],
      "metadata": {
        "id": "5dNp2M8GkRLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e9aa74-bc81-49a8-88c1-62ed42f6e27a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gcimpute in /usr/local/lib/python3.12/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.12/dist-packages (from gcimpute) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from gcimpute) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from gcimpute) (1.16.1)\n",
            "Requirement already satisfied: statsmodels>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from gcimpute) (0.14.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from gcimpute) (4.67.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from gcimpute) (6.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->gcimpute) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->gcimpute) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->gcimpute) (2025.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.11.0->gcimpute) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.11.0->gcimpute) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->gcimpute) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install \"numpy<2.0\""
      ],
      "metadata": {
        "id": "fKBNvxtp1rG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gcimpute.gaussian_copula import GaussianCopula\n",
        "\n",
        "n=4\n",
        "d=3\n",
        "p_miss=0.2\n",
        "xt = np.random.randint(low=0, high=99, size=(n, d)) + 0.0\n",
        "mt = np.random.binomial(1, p_miss, size=(n, d))\n",
        "xt[mt == 1] = np.nan\n",
        "\n",
        "print(xt)\n",
        "\n",
        "#model = GaussianCopula()\n",
        "#Ximp = model.fit_transform(X=xt)\n",
        "#model.fit_transform(X=xt)\n",
        "#Xmul = model.sample_imputation(X=xt, num=5).swapaxes(0, 2)  #.reshape((5, 4, 3), order='F')\n",
        "#Xmul = Xmul.swapaxes(-1, -2)\n",
        "#print(Xmul)"
      ],
      "metadata": {
        "id": "ahPyLc7tfNVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQ9K3f57kWSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xf9BEU4Ds3jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDRZK-Eqs-ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_NXEAOWmC8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_JUPo3StFpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tK4o4xNtKqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tm-S8izbszNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ggDSA-ktpXgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053384e3-c49e-42bf-f487-89c57808cf61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CLARABEL', 'CVXOPT', 'GLPK', 'GLPK_MI', 'HIGHS', 'OSQP', 'SCIPY', 'SCS']\n",
            "coef :  [ 2.00716199 -0.99761761  0.00323481  0.00468057]\n",
            "intercpt  -0.013228723902766722\n",
            "coef :  [ 2.00716199 -0.99761761  0.00323481  0.00468057]\n",
            "intercpt  -0.013228723902766722\n",
            "coef :  [ 2.00714661 -0.9976091   0.00323747  0.00468148]\n",
            "intercpt  -0.013229063736840354\n",
            "coef :  [ 2.00714661 -0.9976091   0.00323747  0.00468148]\n",
            "intercpt  -0.013229063736840354\n",
            "coef :  [ 2.00560974 -0.99675872  0.00350253  0.00477174]\n",
            "intercpt  -0.013263022974302346\n",
            "coef :  [ 2.00560974 -0.99675872  0.00350253  0.00477174]\n",
            "intercpt  -0.013263022974302346\n",
            "coef :  [ 1.99184975 -0.989014    0.005955    0.00570065]\n",
            "intercpt  -0.013588241879571871\n",
            "coef :  [ 1.99184975 -0.989014    0.005955    0.00570065]\n",
            "intercpt  -0.013588241879571871\n",
            "end block\n"
          ]
        }
      ],
      "source": [
        "from re import VERBOSE\n",
        "from itertools import cycle\n",
        "import miceforest as mf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.linear_model import lasso_path\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import ridge_regression\n",
        "import tqdm\n",
        "import cvxpy as cp\n",
        "print(cp.installed_solvers())\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import traceback\n",
        "\n",
        "\n",
        "def compute_q(p):\n",
        "    if p != np.inf and p > 1:\n",
        "        q = p / (p - 1)\n",
        "    elif p == 1:\n",
        "        q = np.inf\n",
        "    else:\n",
        "        q = 1\n",
        "    return q\n",
        "\n",
        "\n",
        "class AdversarialTraining:\n",
        "    def __init__(self, X, y, S_dict, p):  # S is the matrix such that ||S^(-1) @ Dx||\\leq delta. As a consequence, S appears in the unconstrained problem\n",
        "        # S: (d, d) matrix, or S = np.concatenate(tS), with tS = [S1,..,S_m], so S is (d * n, d)\n",
        "        n, d = X.shape\n",
        "        q = compute_q(p)\n",
        "\n",
        "        #print(\"who is X\", X)\n",
        "        #print(\"who is y\", y)\n",
        "        #print(\"who is S\", S)\n",
        "        #print(\"who is q in AdversarialTraining: \", q)\n",
        "        #Formulate problem\n",
        "        param = cp.Variable(d)\n",
        "        #print(\"shape param \", param.shape)\n",
        "        #print(\"dim \", n)\n",
        "        print(\"X \", n,\" \", d)\n",
        "        print(\"y shape\", y.shape)\n",
        "        #print(\"S_dict \", S_dict)\n",
        "        #print(\"S in adv training\", S)\n",
        "        print(\"nm \", d*n)\n",
        "        S_dts = S_dict['S_dts']\n",
        "        S_mis = S_dict['S_mis']\n",
        "        adv_radius_times_scale_dts = cp.Parameter(name='adv_radius_times_dts', nonneg=True)\n",
        "        adv_radius_times_scale_mis = cp.Parameter(name='adv_radius_times_mis', nonneg=True)\n",
        "        #scale_dts = cp.Parameter(name='scale_dts', nonneg=True)\n",
        "        #scale_mis = cp.Parameter(name='scale_mis', nonneg=True)\n",
        "        print(\"S_mis in Adbvt training \", S_mis)\n",
        "        #if np.sum(S_mis * S_mis) == 0:\n",
        "        if np.all(S_dict['S_mis'] == 0):\n",
        "          print(\"no missing part\")\n",
        "          S = S_dts * adv_radius_times_scale_dts\n",
        "        else:  # S_mis.shape == (n, d, d):\n",
        "          S_dts_tiled = np.concatenate([S_dts] * n)\n",
        "          S_mis_conc = np.concatenate(S_mis)\n",
        "          #np.concatenate([yyy] * 2)\n",
        "          S = S_dts_tiled * adv_radius_times_scale_dts + S_mis_conc * adv_radius_times_scale_mis\n",
        "          print(\"S type \", type(S))\n",
        "          #S = np.concatenate(S)\n",
        "          print(\"S is a tensor, concatenated\")\n",
        "          print(\"final S after conc \\n\", S)\n",
        "\n",
        "        if S.shape == (d, d):\n",
        "          print(\"one matrix in input, S.shape = (n, n)\")\n",
        "          partial = S @ param  # should be (m * n,)\n",
        "          param_norm = cp.pnorm(partial, p=q)\n",
        "        elif S.shape == (d * n, d):  # should be a stack of matrices\n",
        "          print(\"multiple matrices in input, S conc\")\n",
        "          partial = S @ param  # should be (m * n,)\n",
        "          partial = cp.reshape(partial, (n, d), order='C')\n",
        "          param_norm = cp.pnorm(partial, p=q, axis=1)\n",
        "        else:\n",
        "          print(\"--------> ERROR: NO MATRIX S FOUND IN ADVERSARIAL TRAINING\")\n",
        "        #elif S.shape == (m , n):  # stack of diagonal matrices\n",
        "        #  print(\"multiple matrices in input, S_i diag\")\n",
        "          #S_cvx = cp.Constant(S)\n",
        "        #  partial = cp.multiply(cp.Parameter(S), param)\n",
        "        #  param_norm = cp.pnorm(partial, p=q, axis=1)\n",
        "        abs_error = cp.abs(X @ param - y)\n",
        "        adv_loss = 1 / n * cp.sum((abs_error + param_norm) ** 2)\n",
        "        prob = cp.Problem(cp.Minimize(adv_loss))\n",
        "        self.prob = prob\n",
        "        self.adv_radius_times_scale_dts = adv_radius_times_scale_dts\n",
        "        self.adv_radius_times_scale_mis = adv_radius_times_scale_mis\n",
        "        #self.scale_dts = scale_dts\n",
        "        #self.scale_mis = scale_mis\n",
        "        self.param = param\n",
        "        self.warm_start = False\n",
        "\n",
        "\n",
        "    def __call__(self, dict_hyper_p, **kwargs):\n",
        "        try:\n",
        "            #print(\"dic thyper p \", dict_hyper_p)\n",
        "            self.adv_radius_times_scale_dts.value = dict_hyper_p['adv_radius_times_dts']\n",
        "            self.adv_radius_times_scale_mis.value = dict_hyper_p['adv_radius_times_mis']\n",
        "            #self.scale_dts.value = dict_hyper_p['scale_dts\n",
        "            #self.scale_mis.value = dict_hyper_p['scale_mis']\n",
        "            self.prob.solve(warm_start=self.warm_start, solver=cp.CLARABEL, max_iter=10000, **kwargs)\n",
        "            v = self.param.value\n",
        "        except Exception as e:\n",
        "          print(\"------------------> Error occurred:\")\n",
        "          traceback.print_exc()\n",
        "          v = np.zeros(self.param.shape)\n",
        "        #except:\n",
        "        #    print(\"----------------------> you are in except\")\n",
        "        #    v = np.zeros(self.param.shape)\n",
        "        return v\n",
        "\n",
        "'''\n",
        "    def __call__(self, adv_radius, **kwargs):\n",
        "        try:\n",
        "            self.adv_radius.value = adv_radius\n",
        "            self.prob.solve(warm_start=self.warm_start, solver=cp.CLARABEL, max_iter=10000, **kwargs)\n",
        "            v = self.param.value\n",
        "        except Exception as e:\n",
        "          print(\"------------------> Error occurred:\")\n",
        "          traceback.print_exc()\n",
        "          v = np.zeros(self.param.shape)\n",
        "        #except:\n",
        "        #    print(\"----------------------> you are in except\")\n",
        "        #    v = np.zeros(self.param.shape)\n",
        "        return v\n",
        "'''\n",
        "\n",
        "\n",
        "def get_lasso_path(X, y, eps_lasso=1e-5):\n",
        "    alphas, coefs, _ = lasso_path(X, y, eps=eps_lasso)\n",
        "    coefs= np.concatenate([np.zeros([X.shape[1], 1]), coefs], axis=1)\n",
        "    alphas = np.concatenate([1e2 * np.ones([1]), alphas], axis=0)\n",
        "    return alphas, coefs, []\n",
        "\n",
        "# dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'eps_adv_rad_times_delta_dts': 1e-4 'eps_adv_rad_times_delta_dts': 1e-4}}\n",
        "def get_path(X, y, estimator, S_dict): #eps_amax=1e-4, eps_dts_max=1e-3, eps_mis_max=1e-3, n_alphas=100, n_deltas_dts=2, n_deltas_mis=3):\n",
        "    _, m = X.shape\n",
        "\n",
        "    if S_dict['algo_superv_learn'] == 'adv':\n",
        "      #n_a_dts = S_dict['n_a_dts']\n",
        "      #a_d_dts_max = S_dict['adv_rad_times_delta_dts_max']\n",
        "      #a_d_dts_min = a_d_dts_max * S_dict['eps_adv_rad_times_delta_dts']\n",
        "\n",
        "      if np.all(S_dict['S_dts'] == 0):\n",
        "        n_a_dts, a_d_dts_max, a_d_dts_min = 1, 0, 0\n",
        "      else:\n",
        "        n_a_dts = S_dict['n_a_dts']\n",
        "        a_d_dts_max = S_dict['adv_rad_times_delta_dts_max']\n",
        "        a_d_dts_min = a_d_dts_max * S_dict['eps_adv_rad_times_delta_dts']\n",
        "\n",
        "      if np.all(S_dict['S_mis'] == 0):\n",
        "        n_a_mis, a_d_mis_max, a_d_mis_min = 1, 0, 0\n",
        "      else:\n",
        "        n_a_mis = S_dict['n_a_mis']\n",
        "        a_d_mis_max = S_dict['adv_rad_times_delta_mis_max']\n",
        "        a_d_mis_min = a_d_mis_max * S_dict['eps_adv_rad_times_delta_mis']\n",
        "\n",
        "\n",
        "      if a_d_dts_max < 0 or a_d_mis_max < 0 or n_a_dts < 1 or n_a_mis <1:\n",
        "        print(\"WARNING: some bad values for the grid of cross validation, the number of grid point should be strictly potive, the radius strictly positive\")\n",
        "      alphas_dts = np.logspace(np.log10(a_d_dts_min), np.log10(a_d_dts_max), n_a_dts) if a_d_dts_max > 0 else np.zeros(1)\n",
        "      alphas_mis = np.logspace(np.log10(a_d_mis_min), np.log10(a_d_mis_max), n_a_mis) if a_d_mis_max > 0 else np.zeros(1)\n",
        "      #alphas_dts = np.logspace(np.log10(a_d_dts_min), np.log10(a_d_dts_max), n_a_dts)\n",
        "      #alphas_mis = np.logspace(np.log10(a_d_mis_min), np.log10(a_d_mis_max), n_a_mis)\n",
        "      print(\"dts deltas \", alphas_dts)\n",
        "      print(\"mis deltas \", alphas_mis)\n",
        "      #hyper_p = {'scale_dts': dts_deltas, 'scale_mis': mis_deltas}\n",
        "      hyper_p_ret_ = []\n",
        "      coefs_ = []\n",
        "      for a_mis_value in tqdm.tqdm(alphas_mis):\n",
        "        for a_dts_value in tqdm.tqdm(alphas_dts):\n",
        "            #tuple_key = (scale_dts_value, scale_mis_value)\n",
        "            #coefs_ = []\n",
        "            #for a in tqdm.tqdm(alphas):\n",
        "              #dict_hyper_p_values = {'adv_radius': a, 'scale_dts': scale_dts_value, 'scale_mis': scale_mis_value}\n",
        "              dict_hyper_p_values = {'adv_radius_times_dts': a_dts_value, 'adv_radius_times_mis': a_mis_value}\n",
        "              #print(\"dict hyper in get path \", dict_hyper_p_values)\n",
        "              coefs = estimator(X, y, dict_hyper_p_values)\n",
        "              #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "              coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "              hyper_p_ret_.append([a_dts_value, a_mis_value])\n",
        "            #res[tuple_key] = np.stack((coefs_)).T\n",
        "    elif S_dict['algo_superv_learn'] == 'ridge':\n",
        "      n_a_rid = S_dict['n_a_rid']\n",
        "      a_rid_max = S_dict['alpha_ridge_reg_max']\n",
        "      a_rid_min = a_rid_max * S_dict['eps_alpha_ridge_reg']\n",
        "      alphas_rid = np.logspace(np.log10(a_rid_min), np.log10(a_rid_max), n_a_rid) if a_rid_max > 0 else np.zeros(1)\n",
        "      print(\"rid alphas \", alphas_rid)\n",
        "      hyper_p_ret_ = []\n",
        "      coefs_ = []\n",
        "      print(\"S_dts_inv in get path, ridge regression \\n\", S_dict['S_dts'])\n",
        "      S_dts_inv = np.linalg.inv(S_dict['S_dts'])  # (d, d)\n",
        "      print(\"S_dts_inv in get path, ridge regression \\n\", S_dts_inv)\n",
        "      for a_rid in tqdm.tqdm(alphas_rid):\n",
        "            #dict_hyper_p_values = {'adv_radius_times_dts': a_dts_value, 'adv_radius_times_mis': a_mis_value}\n",
        "            #print(\"dict hyper in get path \", dict_hyper_p_values)\n",
        "            coefs = estimator(X @ S_dts_inv, y, a_rid)\n",
        "            coefs = S_dts_inv @ coefs\n",
        "            print(\"alpha  \", a_rid, \"coef_ridge: \", coefs)\n",
        "            coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "            hyper_p_ret_.append([a_rid, 0])  #([a_dts_value, a_mis_value])\n",
        "\n",
        "\n",
        "    '''\n",
        "    for scale_dts_value in tqdm.tqdm(dts_deltas):\n",
        "        for scale_mis_value in tqdm.tqdm(mis_deltas):\n",
        "          #tuple_key = (scale_dts_value, scale_mis_value)\n",
        "          #coefs_ = []\n",
        "          for a in tqdm.tqdm(alphas):\n",
        "              #dict_hyper_p_values = {'adv_radius': a, 'scale_dts': scale_dts_value, 'scale_mis': scale_mis_value}\n",
        "              dict_hyper_p_values = {'adv_radius_times_scale_dts': a * scale_dts_value, 'adv_radius_times_scale_mis': a * scale_mis_value}\n",
        "              coefs = estimator(X, y, dict_hyper_p_values)\n",
        "              #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "              coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "              hyper_p_ret_.append([a, scale_dts_value, scale_mis_value])\n",
        "          #res[tuple_key] = np.stack((coefs_)).T\n",
        "    '''\n",
        "    return np.stack((hyper_p_ret_)).T, np.stack((coefs_)).T\n",
        "\n",
        "#dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg': 1,\n",
        "#                             'eps_adv_rad_times_delta_dts': 1e-4, 'eps_adv_rad_times_delta_mis': 1e-4, 'eps_alpha_ridge_reg': 1e-4,\n",
        "#                             'n_a_dts': 25, 'n_a_mis':4, 'n_a_rid': 25}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "def get_path(X, y, estimator, amax, eps=1e-5, n_alphas=200):\n",
        "    _, m = X.shape\n",
        "    amin = eps * amax\n",
        "    alphas = np.logspace(np.log10(amin), np.log10(amax), n_alphas)\n",
        "    coefs_ = []\n",
        "    for a in tqdm.tqdm(alphas):\n",
        "        coefs = estimator(X, y, a)\n",
        "        #print(\"alpha  \", a, \"coef: \", coefs)\n",
        "        coefs_.append(coefs if coefs is not None else np.zeros(m))\n",
        "    return alphas, np.stack((coefs_)).T\n",
        "'''\n",
        "\n",
        "\n",
        "def plot_coefs(alphas, coefs, ax):\n",
        "    #print(\"you are printing coefs in function of 1/alphas\")\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    #l1norm = np.abs(coefs).sum(axis=0)\n",
        "    ax.set_xlabel(\"1/alphas\")\n",
        "    ax.set_ylabel(\"coef\")\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.semilogx(1/alphas, coef_l, c=c)\n",
        "        #ax.semilogx(1/alphas, l1norm, c=c)\n",
        "        #ax.plot(1/alphas, coef_l, c=c)\n",
        "\n",
        "\n",
        "def plot_coefs_l1norm(coefs, ax):\n",
        "    #print(\"you are printing coeff in function of l1 norm\")\n",
        "    colors = cycle([\"b\", \"r\", \"g\", \"c\", \"k\"])\n",
        "    #l1norm = np.abs(coefs).mean(axis=0)\n",
        "    l1norm = np.abs(coefs).sum(axis=0)\n",
        "    #print(\"coef \", coefs)\n",
        "    #print(\"l1norm \", l1norm)\n",
        "    ax.set_xlabel(\"l1norm\")\n",
        "    ax.set_ylabel(\"coef\")\n",
        "\n",
        "\n",
        "    for coef_l, c in zip(coefs, colors):\n",
        "        ax.plot(l1norm, coef_l, c=c)\n",
        "\n",
        "\n",
        "def train_and_plot(X, y, S_dict, list_ax):\n",
        "\n",
        "    if S_dict['algo_superv_learn'] == 'adv':\n",
        "      linfadvtrain = AdversarialTraining(X, y, S_dict, p=np.inf)\n",
        "      estimator = lambda X, y, dic_h:  linfadvtrain(dict_hyper_p=dic_h)\n",
        "      hyper_p, coefs_advtrain_linf  = get_path(X, y, estimator, S_dict)\n",
        "    elif S_dict['algo_superv_learn'] == 'ridge':\n",
        "      estimator = lambda XX, yy, rad: ridge_regression(XX, yy, alpha=rad, return_intercept=False)#, random_state=0)\n",
        "      hyper_p, coefs_advtrain_linf  = get_path(X, y, estimator, S_dict)\n",
        "      #estimator = lambda X, y, a: linear_model.Ridge(alpha=a).fit(X, y).coef_\n",
        "    #print(\"hyper_p used\\n \", hyper_p)\n",
        "    if len(list_ax) > 0:\n",
        "      plot_coefs_l1norm(coefs_advtrain_linf, list_ax[0])\n",
        "      plot_coefs(alphas_adv, coefs_advtrain_linf, list_ax[1])\n",
        "    return hyper_p, coefs_advtrain_linf\n",
        "\n",
        "\n",
        "X = np.random.randn(100, 4) #rng.randn(100, 4)\n",
        "\n",
        "y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * np.random.randn(100)\n",
        "\n",
        "alphas = [0.00001, 0.001, 0.1, 1]\n",
        "estim = lambda XX, yy, rad: ridge_regression(XX, yy, alpha=rad, return_intercept=True, random_state=0)\n",
        "for a in alphas:\n",
        "  coef, intercept = estim(X, y, a)\n",
        "  print(\"coef : \", coef)\n",
        "  print(\"intercpt \", intercept)\n",
        "  coef, intercept = ridge_regression(X, y, alpha=a, return_intercept=True, random_state=0)\n",
        "  print(\"coef : \", coef)\n",
        "  print(\"intercpt \", intercept)\n",
        "\n",
        "\n",
        "'''\n",
        "def add_rectangles_old(x, y, box_width, box_height, ax):\n",
        "  r_c = (np.random.binomial(1, 1, size=x.size) == 1)  # 1 taken, 0 not taken\n",
        "  #print(r_c)\n",
        "\n",
        "  for xi, yi in zip(x[r_c], y[r_c]):\n",
        "      rect = patches.Rectangle(\n",
        "        (xi-box_width/2, yi-box_height/2),\n",
        "        box_width, box_height,\n",
        "        linewidth=1, edgecolor='r', facecolor='none'\n",
        "      )\n",
        "      ax.add_patch(rect)\n",
        "'''\n",
        "\n",
        "def add_rectangles(x, y, S, ax):\n",
        "  r_c = (np.random.binomial(1, 1, size=x.size) == 1)  # 1 taken, 0 not taken\n",
        "  #print(r_c)\n",
        "  d = S.shape[-1]\n",
        "  #S = S * 100\n",
        "  if S.ndim == 2 or S.shape == (1, d, d):\n",
        "    S = S.squeeze()\n",
        "    print(\"------------------------> who is S in add_rectangles\\n\", S)\n",
        "    box_width = S[0, 0]\n",
        "    box_height = S[1, 1]\n",
        "    for xi, yi in zip(x[r_c], y[r_c]):\n",
        "        rect = patches.Rectangle(\n",
        "          (xi-box_width/2, yi-box_height/2),\n",
        "          box_width, box_height,\n",
        "          linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "  else:  # S is something like (n, d, d)\n",
        "    #print(\"---------------> who is S in add_rectangles (mult imp)\\n\", S)\n",
        "    box_width = S[:, 0, 0]\n",
        "    box_height = S[:, 1, 1]\n",
        "    #print(\"bw\\n \", box_width)\n",
        "    #print(\"bh\\n \", box_height)\n",
        "    #print(\"------------------------------> boxes printed\")\n",
        "    for xi, yi, bw, bh in zip(x[r_c], y[r_c], box_width[r_c], box_height[r_c]):\n",
        "        #print(\"bw, bh \", bw, \",   \", bh)\n",
        "        rect = patches.Rectangle(\n",
        "          (xi-bw/2, yi-bh/2),\n",
        "          bw, bh, linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "\n",
        "\n",
        "print(\"end block\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZ3P4s7I9DTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation's block\n",
        "\n",
        "def clear_dataset(X, y, masks):\n",
        "  # remove observations full NaN\n",
        "  # X is an (n, d) matrix, y is a (n,) vector,\n",
        "  # masks is an (n, d) binary matrix associated to X. 1 missing, 0 seen\n",
        "  M = np.sum(1 - masks, axis=1) > 0\n",
        "  print(\"X shape in clear data \", X.shape)\n",
        "  print(\"y shape in clear data \", y.shape)\n",
        "  print(\"M shape in clear data \", M.shape)\n",
        "  M_col = np.sum(1 - masks, axis=0) > 0  # True if in the column there is at least one seen component\n",
        "  if np.sum(M_col) < masks.shape[1]:\n",
        "    print(\"Careful, there is one column full of nan\")\n",
        "  return X[M, :][:, M_col], y[M], masks[M, :][:, M_col]\n",
        "\n",
        "\n",
        "def single_imputation(X_nan, impute_estimator):\n",
        "    ice = IterativeImputer(estimator=impute_estimator)\n",
        "    return ice.fit_transform(X_nan)\n",
        "\n",
        "\n",
        "def multiple_imputation(info_mi, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    print(\"info mi\", info_mi)\n",
        "    nbr_mi = info_mi['mi_nbr']\n",
        "    nbr_feature = info_mi['nbr_feature']\n",
        "    res = np.zeros((nbr_mi, n, d))\n",
        "    for i in range(nbr_mi):\n",
        "       n_i = np.random.randint(0, 100000)\n",
        "       print(\"nbr features \", nbr_feature)\n",
        "       ice = IterativeImputer(random_state=n_i, max_iter=50, sample_posterior=True, n_nearest_features=nbr_feature)\n",
        "       res[i, :, :] = ice.fit_transform(X_nan)\n",
        "       #print(\"fin res shape\", res.shape)\n",
        "       #if nbr_mi == 1:\n",
        "        #res = res[0, :, :]\n",
        "        #print(\"fin res shape\", res.shape)\n",
        "    return res\n",
        "\n",
        "'''\n",
        "x = pd.DataFrame(x)\n",
        "x.columns = x.columns.astype(str)\n",
        "m = np.random.binomial(1, 0.3, size=(n, d))\n",
        "mm = (m==1)\n",
        "x[mm] = np.nan\n",
        "print(mm)\n",
        "print(x)\n",
        "\n",
        "\n",
        "# Create kernel.\n",
        "kernel = mf.ImputationKernel(\n",
        "  x,\n",
        "  num_datasets=4,\n",
        "  random_state=1,\n",
        "  mean_match_candidates=0\n",
        ")\n",
        "\n",
        "# Run the MICE algorithm for 2 iterations on each of the datasets\n",
        "\n",
        "%time kernel.mice(3)\n",
        "\n",
        "\n",
        "cd1 = kernel.complete_data(dataset=1)\n",
        "cd2 = kernel.complete_data(dataset=2)\n",
        "print(\"cd1\\n\", cd1, \"\\ncd2\\n\", cd2)\n",
        "\n",
        "# Printing the kernel will show you some high level information.\n",
        "print(kernel)\n",
        "\n",
        "print()\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def imputation_elliptic(mu, sigma, x, masks):\n",
        "  # mu, mean elliptical distribution (,d)\n",
        "  # sigma, cov matrix elliptical distribution (d, d)\n",
        "  # x: dataset (n, d)\n",
        "  # masks: mask data, 0 seen, 1 missing\n",
        "  n, d = x.shape\n",
        "  print(n, d)\n",
        "  x_imp = x.copy()\n",
        "  #print(\"x_imp clean\", x_imp)\n",
        "  for i in range(n):\n",
        "    if not (masks[i, :] == 0).all():  # if we have at least one missing component\n",
        "      #print(\"nbr : \", i)\n",
        "      x_c = x[i, :]\n",
        "      m_bool = (masks[i, :] == 0)  # True seen, False missing\n",
        "      sigma_aa_inv = np.linalg.inv(sigma[m_bool, :][:, m_bool])\n",
        "      sigma_ma = sigma[~m_bool, :][:, m_bool]\n",
        "      mu_cond = mu[~m_bool] + sigma_ma @ sigma_aa_inv @ (x_c[m_bool] - mu[m_bool])\n",
        "      x_imp[i, ~m_bool] = mu_cond\n",
        "  return x_imp\n",
        "\n",
        "\n",
        "def listwise_delection(X, masks):\n",
        "  # masks: 1 missing, 0 seen\n",
        "    M = np.sum(masks, axis=1) == 0  # zeros components are the one with full entries\n",
        "    ret = X[M, :] if X.ndim == 2 else X[M]\n",
        "    return ret\n",
        "\n",
        "def miceforest_imputation(info_mf, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    nbr_mi = info_mf['mi_nbr']\n",
        "\n",
        "def gc_imputation(info_gc, X_nan):\n",
        "  nbr_mi = info_gc['nbr_mi']\n",
        "  model = GaussianCopula()\n",
        "  model.fit_transform(X=X_nan)\n",
        "  Xmul = model.sample_imputation(X=X_nan, num=nbr_mi).swapaxes(0, 2)\n",
        "  Xmul = Xmul.swapaxes(-1, -2)\n",
        "  return Xmul\n",
        "\n",
        "def miceforest_imputation(info_mf, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    nbr_mi = info_mf['mi_nbr']\n",
        "    nbr_iterations_mc = info_mf['it_mc']  # number iteration Markov Chain\n",
        "    res = np.zeros((nbr_mi, n, d))\n",
        "    if np.isnan(X_nan).sum() == 0:  # no missing component\n",
        "      print(\"CAREFUL::: no missing component, so no multiple imputation\")\n",
        "      res = [X_nan] * nbr_mi\n",
        "    else:\n",
        "      x = pd.DataFrame(X_nan)\n",
        "      x.columns = x.columns.astype(str)\n",
        "      # Create kernel.\n",
        "      kernel = mf.ImputationKernel(\n",
        "      x,\n",
        "      num_datasets=nbr_mi,\n",
        "      #random_state=1,\n",
        "      mean_match_candidates=info_mf['nbr_candidates_mm']\n",
        "      )\n",
        "      # Run the MICE algorithm for 2 iterations on each of the datasets\n",
        "      variables = kernel.model_training_order\n",
        "      print(\"variab \", variables)\n",
        "      #dic_param = {key:{'random_state':42, 'n_estimator': 25, 'bagging_fraction': 0.9999999999, 'feature_fraction': 0.999999999999} for key in variables}\n",
        "      '''\n",
        "      dic_param = {\n",
        "        key: {\n",
        "          'random_state': 42,\n",
        "         'n_estimators': 25,\n",
        "         'bagging_fraction': 0.9999999999999999,\n",
        "         'feature_fraction': 0.9999999999999999,\n",
        "         'force_col_wise': True,\n",
        "         'num_threads': 1\n",
        "        } for key in variables\n",
        "      }'''\n",
        "      #kernel.mice(iterations=0,\n",
        "                  #variable_parameters=dic_param,\n",
        "      #            mean_match_candidates=0\n",
        "      #            )\n",
        "      %time kernel.mice(nbr_iterations_mc)\n",
        "      for i in range(nbr_mi):\n",
        "        res[i, :, :] = kernel.complete_data(dataset=i)\n",
        "    return res\n",
        "\n"
      ],
      "metadata": {
        "id": "qyWskXpdOW9e"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test miceforest\n",
        "\n",
        "np.random.seed(11)\n",
        "\n",
        "\n",
        "def test_miceforest(n, d, p_miss, nbr_c):\n",
        "    x_mf_test = np.random.randint(low=0, high=99, size=(n, d)) + 0.0\n",
        "    m_mf_test = np.random.binomial(1, p_miss, size=(n, d))\n",
        "    x_mf_test[m_mf_test == 1] = np.nan\n",
        "    print(x_mf_test)\n",
        "    info_mf_test = {'mi_nbr': 2, 'it_mc': 2, 'nbr_candidates_mm': nbr_c}\n",
        "\n",
        "    print(np.isnan(x_mf_test))\n",
        "    print(np.isnan(x_mf_test).sum() )\n",
        "\n",
        "\n",
        "    res = miceforest_imputation(info_mf_test, x_mf_test)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "\n",
        "test_miceforest(n=13, d=5, p_miss=0.2, nbr_c=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "ElCvHxBiO_2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d424ac-04ce-41d9-e4ed-7e33b7501931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[25. 63. 80. 91. 81.]\n",
            " [55. nan 76. 33. 71.]\n",
            " [82. 24. 92. 48. 32.]\n",
            " [45.  4. 34. 12.  1.]\n",
            " [nan 37. 84. nan 91.]\n",
            " [24. nan 67. nan 43.]\n",
            " [64. nan 96. 23. 27.]\n",
            " [70. 89. 10. 57. 27.]\n",
            " [44. 14.  7. 55. 79.]\n",
            " [64. nan 27. nan  5.]\n",
            " [53. 71. 50. 59.  1.]\n",
            " [ 3.  8. 87. 81. 68.]\n",
            " [72. nan 78. 19. 25.]]\n",
            "[[False False False False False]\n",
            " [False  True False False False]\n",
            " [False False False False False]\n",
            " [False False False False False]\n",
            " [ True False False  True False]\n",
            " [False  True False  True False]\n",
            " [False  True False False False]\n",
            " [False False False False False]\n",
            " [False False False False False]\n",
            " [False  True False  True False]\n",
            " [False False False False False]\n",
            " [False False False False False]\n",
            " [False  True False False False]]\n",
            "9\n",
            "variab  ['1', '3', '0']\n",
            "CPU times: user 317 ms, sys: 12.2 ms, total: 329 ms\n",
            "Wall time: 226 ms\n",
            "[[[25.         63.         80.         91.         81.        ]\n",
            "  [55.         17.85416667 76.         33.         71.        ]\n",
            "  [82.         24.         92.         48.         32.        ]\n",
            "  [45.          4.         34.         12.          1.        ]\n",
            "  [20.66666676 37.         84.         59.79166692 91.        ]\n",
            "  [24.         56.45833333 67.         52.45833345 43.        ]\n",
            "  [64.         16.75       96.         23.         27.        ]\n",
            "  [70.         89.         10.         57.         27.        ]\n",
            "  [44.         14.          7.         55.         79.        ]\n",
            "  [64.         10.5        27.         20.75000072  5.        ]\n",
            "  [53.         71.         50.         59.          1.        ]\n",
            "  [ 3.          8.         87.         81.         68.        ]\n",
            "  [72.         24.66666667 78.         19.         25.        ]]\n",
            "\n",
            " [[25.         63.         80.         91.         81.        ]\n",
            "  [55.                 nan 76.         33.         71.        ]\n",
            "  [82.         24.         92.         48.         32.        ]\n",
            "  [45.          4.         34.         12.          1.        ]\n",
            "  [46.83333308 37.         84.         49.52083349 91.        ]\n",
            "  [24.                 nan 67.         62.64583355 43.        ]\n",
            "  [64.                 nan 96.         23.         27.        ]\n",
            "  [70.         89.         10.         57.         27.        ]\n",
            "  [44.         14.          7.         55.         79.        ]\n",
            "  [64.                 nan 27.         46.68750004  5.        ]\n",
            "  [53.         71.         50.         59.          1.        ]\n",
            "  [ 3.          8.         87.         81.         68.        ]\n",
            "  [72.                 nan 78.         19.         25.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA7J67yAuQM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.random.seed(42)\n",
        "\n",
        "#p_miss_2d = [0.2, 0.4, 0.4]\n",
        "#beta_2d = np.array([0.5, 2])  # ground truth\n",
        "\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.model_selection import train_test_split\n",
        "'''\n",
        "def generate_masks_2d(nbr_of_sample, p_missing):\n",
        "    # nbr_of_sample is the number of masks\n",
        "    # p_missing=[p00, p01, p10], where p00 is the probability of seeing both components,\n",
        "    # p10 is the probability of seeing the right component, p01 is the probability of seeing the left component\n",
        "    masks = np.zeros((nbr_of_sample, 2))\n",
        "    v = np.random.choice(a=3, size=nbr_of_sample, p=p_missing)\n",
        "    masks[v == 0, :] = np.array([0, 0])  # both seen\n",
        "    masks[v == 1, :] = np.array([0, 1])  # left seen\n",
        "    masks[v == 2, :] = np.array([1, 0])  # right seen\n",
        "    return masks\n",
        "'''\n",
        "\n",
        "def generate_masks(dictio_data):#nbr_of_sample, dim, p_missing):\n",
        "    # nbr_of_sample is the number of masks\n",
        "    # p_missing=[p00, p01, p10], where p00 is the probability of seeing both components,\n",
        "    # p10 is the probability of seeing the right component, p01 is the probability of seeing the left component\n",
        "    dim = len(dictio_data['beta_gt'][0])\n",
        "    nbr_of_sample = dictio_data['n_train'][-1]  # last one should be the biggest one\n",
        "    p_missing = dictio_data['p_miss'][0]\n",
        "    print(\"p_missing in generate mask \", p_missing)\n",
        "    if dim == 2:\n",
        "      if len(p_missing) < 3:\n",
        "        print(\"WARNING: p_missing should be a list with a length of 3 if the dimension is 2\")\n",
        "      masks = np.zeros((nbr_of_sample, 2))\n",
        "      v = np.random.choice(a=3, size=nbr_of_sample, p=p_missing)\n",
        "      masks[v == 0, :] = np.array([0, 0])  # both seen\n",
        "      masks[v == 1, :] = np.array([0, 1])  # left seen\n",
        "      masks[v == 2, :] = np.array([1, 0])  # right seen\n",
        "    else:\n",
        "      # in this branch, p_missing = [p1,.., pl],\n",
        "      masks = np.array([np.random.binomial(1, 1-pr, (nbr_of_sample, dim)) for pr in p_missing])\n",
        "      masks = np.cumsum(masks, axis=0)  # each round\n",
        "      masks[masks>1] = 1\n",
        "    return masks\n",
        "\n",
        "def best_predictor(X, coeff, y):\n",
        "  hat_y = (X @ coeff).T  # (n, d) @ (d, m) = (n, m)\n",
        "  r = hat_y - y  # residual\n",
        "  score = np.mean(r * r, axis=1)\n",
        "  print(\"scores:  \", score)\n",
        "  i_min = np.argmin(score)\n",
        "  return coeff[:, i_min], score[i_min]\n",
        "\n",
        "def best_idx_predictor(X, coeff, y):\n",
        "  hat_y = (X @ coeff).T  # (n, d) @ (d, m) = (n, m)\n",
        "  r = hat_y - y  # residual\n",
        "  #score = np.mean(r * r, axis=1)\n",
        "  score = np.mean(r * r, axis=1)\n",
        "  #print(\"score in best idx\", score)\n",
        "  i_min = np.argmin(score)\n",
        "  #### find the minimum value with a threshold, so we get bigger uncertainty set that are visible\n",
        "  min = np.min(score)\n",
        "  max = np.max(score)\n",
        "  score[ score < min + -1 ] = max\n",
        "  ####\n",
        "  #print(\"score after \", score)\n",
        "  i_min = np.argmin(score)\n",
        "  return i_min, score[i_min]\n",
        "\n",
        "\n",
        "\n",
        "def generate_X(data, dim):\n",
        "    if data == 'Gaussian':\n",
        "      def generator(n):\n",
        "        return np.random.randn(n, dim)\n",
        "    elif data == 'Uniform':\n",
        "      def generator(n):\n",
        "        return np.random.rand(n, dim)\n",
        "    elif data == 'moons':\n",
        "      def generator(n):\n",
        "        return make_moons(n, noise=0.1)[0]\n",
        "    elif data == 'circles':\n",
        "      def generator(n):\n",
        "        return make_circles(n, noise=0.1, factor=0.4)[0]\n",
        "    return generator\n"
      ],
      "metadata": {
        "id": "AN61ok0A_Mbv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jB0J9uh-dJBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwSkM31ztfUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment 2d with dataset generated externally\n",
        "\n",
        "def imputations(info, dict_obs_for_imp):  # X_nan, y):\n",
        "  # info contains the method and possible extra information\n",
        "  # X_nan is the dataset with nan in place of the missing components\n",
        "  # y is return as it is, unless the method require to change it, like in\n",
        "  # listwise deletion\n",
        "    #print(info)\n",
        "    X_nan = dict_obs_for_imp['X_nan']\n",
        "    y = dict_obs_for_imp['y_train']\n",
        "    mask_from_X_nan = np.isnan(X_nan).astype(int)\n",
        "    if info['imp_method'] == 'BR_si':  # Baeysian_Ridge_single_imputation\n",
        "        X = single_imputation(X_nan, BayesianRidge())\n",
        "    elif info['imp_method'] in  ['mi', 'mi_pure']:\n",
        "        print(\"info in imputations \", info)\n",
        "        X = multiple_imputation(info, X_nan)  # size (info['mi_nbr], n, d)\n",
        "    elif info['imp_method'] in ['mf_imp','mf_imp1']:\n",
        "        X = miceforest_imputation(info, X_nan)\n",
        "    elif info['imp_method'] == 'gc':\n",
        "        X = gc_imputation(info, X_nan)\n",
        "    elif info['imp_method'] == 'l_d':  # listwise_deletion\n",
        "        #mask_from_X_nan = np.isnan(X_nan).astype(int)\n",
        "        X = listwise_delection(X_nan, mask_from_X_nan)\n",
        "        y = listwise_delection(y, mask_from_X_nan)\n",
        "        if len(X) == 0:  # no elements left, add an artificial element\n",
        "            X = np.zeros((1, X_nan.shape[-1]))\n",
        "            y = np.zeros(1)\n",
        "        mask_from_X_nan = np.zeros_like(X)\n",
        "    elif info['imp_method'] == 'oracle':\n",
        "        X = dict_obs_for_imp['X_train_masked'][0]\n",
        "        mask_from_X_nan = np.zeros_like(X)\n",
        "    else:\n",
        "      print(\"-------------------> ERROR: WRONG KEYWORD (in imputations)\")\n",
        "    return X, y, mask_from_X_nan\n",
        "\n",
        "\n",
        "def cov_strategy(info, dict_observations):\n",
        "    # uncertainty coming from considering the imputed values as true values (within uncertanty)\n",
        "    X_imputed = dict_observations['X_imputed']\n",
        "    X_nan = dict_observations['X_nan']\n",
        "    masks = dict_observations['masks_after_imputation']\n",
        "    print(np.sum(masks, axis=-1))\n",
        "    if info['cov_strategy'] == 'sd':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      #print(\"sd in cov strategy \", sd)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(sd)\n",
        "    elif info['cov_strategy'] == 'inv_sd':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(1 / sd)\n",
        "    elif info['cov_strategy'] == 'zero':\n",
        "      #sd = np.std(X_imputed, axis=0)\n",
        "      #S = np.diag(sd)  # check if here it is 1 / sd or sd. The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.zeros((X_imputed.shape[-1], X_imputed.shape[-1]))\n",
        "    elif info['cov_strategy'] == 'eye':\n",
        "      S = np.eye(X_imputed.shape[-1])\n",
        "    elif info['cov_strategy'] == 'threshold':\n",
        "      sd = np.std(X_imputed, axis=0)\n",
        "      sd[sd < info['threshold']] = info['threshold']\n",
        "      #S = np.diag(sd) The intuition is that, small covariance means small boxes where the points can move\n",
        "      S = np.diag(sd)\n",
        "    elif info['cov_strategy'] == 'std_nan':\n",
        "      if info['imp_method'] in ['oracle']:\n",
        "        print(\"DON'T USE std_nan with oracle and ld because you do not have any nan. Use sd\")\n",
        "      else:\n",
        "        std_columnwise = np.nanstd(X_nan, axis=0)\n",
        "        S = np.diag(std_columnwise)\n",
        "    elif info['imp_method'] in ['mi_pure', 'mi', 'mf_imp', 'mf_imp1']:\n",
        "      if info['cov_strategy'] == 'std_mi':   # std (standard_dev) of the imputed dataset, then the mean\n",
        "        std_vectors = np.std(X_imputed, axis=-2)  # shape: (m, d)\n",
        "        #print(\"std vectors \", std_vectors)\n",
        "        #s_within = np.mean(std_vectors, axis=0)  # within imputation variance  # shape : d\n",
        "        S = std_vectors[:, None, :] * np.eye(std_vectors.shape[-1])  # should be (m, d, d), with each diagonal the diagonals of std_vectors\n",
        "        #S = s_within\n",
        "        #S = np.diag(s_within)\n",
        "        print(\"final S.shape in cov strategy std_mi \", S.shape)\n",
        "    elif info['cov_strategy'] == 'lounici':\n",
        "      mu = np.nanmean(X_nan, axis=0)\n",
        "      print(\"means \", mu)\n",
        "      delta = 1 - np.mean(masks) # parameter missingness\n",
        "      print(\"delta \", delta)\n",
        "      X_0 = np.nan_to_num(X_nan - mu)  # check if this is correct\n",
        "      print(\"nbr obs\", X_0.shape[0])\n",
        "      S =  X_0.T @ X_0 / X_0.shape[0]\n",
        "      S = (1/delta - 1/(delta**2)) * np.diag(np.diag(S)) + 1/(delta**2) * S\n",
        "    else:\n",
        "      raise ValueError(\"-------------> ERROR: NO COVARIANCE METHOD HAS BEEN CHOSEN\")\n",
        "      #print(\"-------------> ERROR: NO COVARIANCE METHOD HAS BEEN CHOSEN\")\n",
        "      #S = np.diag(S)\n",
        "      #mu = np.mean(X_imputed, axis=0)\n",
        "      #sigma = np.cov(X_imputed, rowvar=False)\n",
        "    return S\n",
        "\n",
        "\n",
        "def cov_strategy_missing(info, dict_observations):\n",
        "    # uncertainty coming from sampling multiple values to get multiple datasets. It is zero for single imputation\n",
        "    X_imputed = dict_observations['X_imputed']\n",
        "    print(\"info in cov_strategy_missing\\n\", info)\n",
        "    if info['imp_method'] in ['mi', 'mi_pure', 'mf_imp', 'mf_imp1'] and 'cov_strategy_between' in info.keys():\n",
        "      m, n, d = X_imputed.shape\n",
        "      if info['cov_strategy_between'] == 'cond_var':\n",
        "        # we have imputed [X1,..,X_m], so shape (m, n, d)\n",
        "        s = np.std(X_imputed, axis=0)\n",
        "        s[s<1e-14] = 0  # set to zero values that are basically zero\n",
        "        #print(\"var \", s)\n",
        "        eye = np.array([np.eye(X_imputed.shape[-1])] * X_imputed.shape[-2])\n",
        "        S_mis = eye * s[:, None, :]\n",
        "        if info['post_imp'] == 'conc':\n",
        "          S_mis = np.tile(S_mis, (m, 1, 1))\n",
        "      elif info['cov_strategy_between'] == 'zero':\n",
        "        d = dict_observations['X_test'].shape[-1]\n",
        "        S_mis = np.zeros((d, d))\n",
        "    else:  # not using a mi method, so uncertainty on missing part should be zero\n",
        "      print(\"shape oject in cov strategy missing \", dict_observations['X_test'].shape[-1])\n",
        "      print(\"shape oject in cov strategy missing \", dict_observations['X_test'].shape)\n",
        "      d = dict_observations['X_test'].shape[-1]\n",
        "      S_mis = np.zeros((d, d))\n",
        "    return S_mis\n",
        "\n",
        "\n",
        "def post_imputation(info_imp, dict_dataset):\n",
        "  # X_imptued should be a matrix (n, d) or tensor (m, d, n) (in multiple imputations methods)\n",
        "    X_imputed = dict_dataset['X_imputed']\n",
        "    y_train = dict_dataset['y_from_X_imputed']\n",
        "    #print(\"info imp in post_imp\", info_imp)\n",
        "    print(\"shape X_imputed in post_imputation \", X_imputed.shape)\n",
        "    mask_train = dict_dataset['masks_after_imputation']\n",
        "    if 'post_imp' not in info_imp.keys():\n",
        "      X_train = X_imputed\n",
        "    elif info_imp['post_imp'] == 'mean':\n",
        "      #print(\"entered in pst_iputation, in mi_mean\")\n",
        "      X_train = np.mean(X_imputed, axis=0)\n",
        "    elif info_imp['post_imp'] == 'conc':\n",
        "      print(\"shape X_imputed \", X_imputed.shape)\n",
        "      X_train = np.concatenate(X_imputed)\n",
        "      y_train = np.tile(y_train, X_imputed.shape[0])\n",
        "    else:\n",
        "      X_train = X_imputed\n",
        "    return X_train, y_train, mask_train\n",
        "\n",
        "\n",
        "def generate_dataset(data, n_tot, dim, beta_gt, perc_test, p_miss, err):\n",
        "    print(data)\n",
        "    if data['data'] == 'Gaussian':\n",
        "      X_complete = np.random.randn(n_tot, dim)\n",
        "    elif data['data'] == 'Normal':\n",
        "      #print(\"you are here\")\n",
        "      if len(beta_gt) != len(data['mean']) or len(beta_gt) != data['cov'].shape[0]:\n",
        "        print(\"ERROR: DIMENSION MISSMATCH\")\n",
        "      X_complete = np.random.multivariate_normal(mean=data['mean'], cov=data['cov'], size=n_tot)\n",
        "    elif data['data'] == 'LogNormal':\n",
        "      if len(beta_gt) != len(data['mean']) or len(beta_gt) != data['cov'].shape[0]:\n",
        "        print(\"ERROR: DIMENSION MISSMATCH\")\n",
        "      X_complete = np.random.lognormal(mean=data['mean'], sigma=data['cov'], size=n_tot)\n",
        "    elif data['data'] == 'Uniform':\n",
        "      X_complete = np.random.rand(n_tot, dim) -0.5\n",
        "    elif data['data'] == 'Logistic':\n",
        "      X_complete = np.random.logistic(loc=0.0, scale=1.0, size=(n_tot, dim))\n",
        "    elif data['data'] == 'moons':\n",
        "      X_complete = make_moons(n_tot, noise=0.1)[0]\n",
        "    elif data['data'] == 'circles':\n",
        "      X_complete = make_circles(n_tot, noise=0.1, factor=0.4)[0]\n",
        "    elif data['data'] == 'Gaussian+Uniform':\n",
        "      #print(\"you are here\")\n",
        "      if len(beta_gt) != len(data['mean']) or len(beta_gt) != data['cov'].shape[0]:\n",
        "        print(\"ERROR: DIMENSION MISSMATCH\")\n",
        "      X_complete = np.random.multivariate_normal(mean=data['mean'], cov=data['cov'], size=n_tot)\n",
        "      X_complete = X_complete + np.random.rand(n_tot, dim) -0.5\n",
        "    elif data['data'] == 'Gaussian+Uniform2':\n",
        "      #print(\"you are here\")\n",
        "      if len(beta_gt) != len(data['mean']) or len(beta_gt) != data['cov'].shape[0]:\n",
        "        print(\"ERROR: DIMENSION MISSMATCH\")\n",
        "      Xg = np.random.multivariate_normal(mean=data['mean'], cov=data['cov'], size=n_tot)\n",
        "      Xu = np.random.rand(n_tot, dim) -0.5\n",
        "      X_complete = Xg + Xu + Xg ** 2 - Xu ** 2\n",
        "    if err['type'] == 'Gaussian_on_y':\n",
        "      #print(\"---> you have entered in GAUSSIAN ERROR \", \"scaling : \", err['scaling'])\n",
        "      error = np.random.randn(n_tot) * err['scaling']\n",
        "    elif err['type'] == 'Uniform_on_y':\n",
        "      error = (np.random.rand(n_tot)-0.5) * err['scaling']\n",
        "    elif err['type'] == 'Gaussian_on_X':\n",
        "      error = (np.random.randn(n_tot, dim) @ beta_gt) * err['scaling']  # error is of the form DX@beta_gt + error\n",
        "    elif err['type'] == 'Uniform_on_X':\n",
        "      error = ((np.random.rand(n_tot, dim)-0.5) @ beta_gt) * err['scaling']\n",
        "    #elif err['type'] == 'Gaussian':\n",
        "    #  error = np.random.randn(n_tot) * err['scaling']\n",
        "\n",
        "    print(X_complete.shape)\n",
        "\n",
        "    y_complete = X_complete @ beta_gt + error  #np.random.randn(n_tot) * err  # (np.random.rand(n_tot) - 0.5) * err\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_complete, y_complete, test_size=perc_test)\n",
        "    n_train = X_train.shape[0]\n",
        "    # masks_train = generate_masks_2d(n_train, p_miss)  # 1 missing, 0 observed\n",
        "    # masks_train = generate_masks_binomial(n_train, p_miss)  # 1 missing, 0 observed\n",
        "    #X_train, y_train, masks_train = clear_dataset(X_train, y_train, masks_train)\n",
        "    # M = np.sum(masks, axis=1)  # M[i] > 0 iff i has missing component\n",
        "    # dict_obs = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test, 'masks_train': masks_train}\n",
        "    dict_obs = {'X_train_masked': (X_train, []), 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}#, 'masks_train': masks_train}\n",
        "    return dict_obs\n",
        "\n",
        "\n",
        "def experiment_2d_ext_dataset(dict_obs, dict_imp, ax):\n",
        "    # dict_obs contains info on the observations, i.e. train, test, masks\n",
        "    # dict_imp contains info on the imputation an covariance methods used,\n",
        "    # dict_imp = {'imp_method': , 'cov_strategy': , .... }\n",
        "    # ax contains info for the plots\n",
        "\n",
        "    X_test = dict_obs['X_test']\n",
        "    y_test = dict_obs['y_test']\n",
        "    mask = dict_obs['X_train_masked'][1]\n",
        "\n",
        "    M = np.sum(mask, axis=1)  # M[i] > 0 iff i has missing component\n",
        "\n",
        "    X_nan_train = dict_obs['X_train_masked'][0].copy()\n",
        "    oracle_sd = np.std(X_nan_train, axis=0)\n",
        "    print(\"-------> ORACLE SD, std of the original dataset (with no missing)\", oracle_sd)\n",
        "    X_nan_train[mask == 1] = np.nan\n",
        "    #print(\"dict imp -----> \", dict_imp)\n",
        "    dict_obs = dict_obs | {'X_nan': X_nan_train} #, 'y_from_X_imputed': y_from_X_imputed, 'masks_after_imputation': mask_from_X_imputed}\n",
        "    if len(dict_obs['imp_ds'][dict_imp['imp_method']]) == 0:  # no previous imputation has been done\n",
        "      #results = imputations(dict_imp, X_nan_train, dict_obs['y_train'])\n",
        "      print(\"NO PREVIOUS IMPUTATION HAS BEEN DONE\")\n",
        "      results = imputations(dict_imp, dict_obs)\n",
        "      X_imputed, y_from_X_imputed, mask_from_X_imputed = results  # imputations(dict_imp, X_nan_train, dict_obs['y_train'])\n",
        "      print(\"X_imputed in experiment_2d_external_dataset \", np.sum(X_imputed, axis=(-1,-2)))\n",
        "      dict_obs['imp_ds'][dict_imp['imp_method']].append(results)\n",
        "      print(\"crush test-------------------------------------------------> \", np.sum(X_imputed))\n",
        "    else:\n",
        "      print(\"A PREVIOUS IMPUTATION HAS BEEN DONE\")\n",
        "      X_imputed, y_from_X_imputed, mask_from_X_imputed = dict_obs['imp_ds'][dict_imp['imp_method']][0]\n",
        "      print(\"crush test-------------------------------------------------> \", np.sum(X_imputed))\n",
        "    #print(\"X_imputed \", X_imputed)\n",
        "    n_imputed, n_test = X_imputed.shape[-2], X_test.shape[-2]\n",
        "    #print(\"X_train\\n \", X_train)\n",
        "    M = np.sum(mask_from_X_imputed, axis=1)  # M[i] > 0 iff i has missing component\n",
        "\n",
        "    dict_obs = dict_obs | {'X_imputed': X_imputed, 'y_from_X_imputed': y_from_X_imputed, 'masks_after_imputation': mask_from_X_imputed}\n",
        "    #  print(dict_obs)\n",
        "    S_dataset = cov_strategy(dict_imp, dict_obs) #* dict_imp['multip_dataset']\n",
        "    print(\"S dataset \\n\", S_dataset)\n",
        "    #  dict_obs = dict_obs | {'cov_within': S_within}\n",
        "    S_missing = cov_strategy_missing(dict_imp, dict_obs)  #* dict_imp['multip_missing']\n",
        "    print(\"S missing shape\\n \", S_missing.shape)\n",
        "    print(\"S missing\\n \", S_missing)\n",
        "    if 'post_imp' in dict_obs.keys():\n",
        "      if dict_obs['post_imp'] == 'conc':\n",
        "        print(S_missing)\n",
        "    #  dict_obs = dict_obs | {'cov_between': S_between}\n",
        "    S_dict = {'S_dts': S_dataset, 'S_mis': S_missing} | dict_obs['info_algo'] | {'algo_superv_learn': dict_imp['algo_superv_learn']}  # , 'multipliers_dts': dict_imp['multip_dataset'], 'multipliers_mis': dict_imp['multip_missing']}\n",
        "    # dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 1, 'adv_rad_times_delta_mis_max': 1, 'eps_adv_rad_times_delta_dts': 1e-4 'eps_adv_rad_times_delta_dts': 1e-4}}\n",
        "\n",
        "    #if True:  # check what to do of this part later\n",
        "      #S = S_dataset * dict_imp['multip_dataset'] + S_missing * dict_imp['multip_missing']\n",
        "      #if S.ndim == 2:\n",
        "      #  print(\"final S \\n\", S)\n",
        "\n",
        "\n",
        "    #print(\"matrices S \\n\", S)\n",
        "    #print(\"---....---....----....--> diag matrix: \", np.diag(S))\n",
        "\n",
        "    #if dict_imp['imp_method'] == 'mi':  # prepare the training set in case of multiple imputation\n",
        "    #  X_train = np.concatenate(X_train)  # X_train, if the method is mi, should be (mi_nbr, n, dim)\n",
        "    #  y_train = np.tile(y_train, reps=dict_imp['mi_nbr'])\n",
        "    #  mask_train = np.tile(mask_train, reps=(dict_imp['mi_nbr'], 1))\n",
        "    #  M = np.sum(mask_train, axis=1)\n",
        "    #print(\"final matrices (exp 2d ext run)\\n \", S)\n",
        "    X_train, y_train, mask_train = post_imputation(dict_imp, dict_obs)\n",
        "    n_train = X_train.shape[-2]\n",
        "    print(\"y_train length \", y_train.shape[0])\n",
        "    print(\"-------> size test: \", n_test, \" , size train: \", n_train, \"nbr_full_seen (train): \", np.sum(M == 0), \" nbr_at_least_one_miss : \", np.sum(M > 0))\n",
        "\n",
        "#    plt.tight_layout()\n",
        "    #S_between = S.copy()\n",
        "    if dict_imp['imp_method'] == 'mi' and dict_imp['cov_strategy'] == 'std_mi':  # run a standard multiple imputation procedure\n",
        "      best_coeff = np.zeros(X_train.shape[-1])\n",
        "      best_alpha = 0\n",
        "      min_score = 0\n",
        "      temporary_dictionary = copy.deepcopy(S_dict)\n",
        "      for i in range(dict_imp['mi_nbr']):\n",
        "        print(\"i  mi .-------------> \", i)\n",
        "        #dict_obs_i = {'X_imputed': X_train[i, :, :], 'X_nan': X_nan_train, 'masks': mask_train}\n",
        "        #dict_imp_new = {'imp_method': dict_imp['imp_method'], 'cov_strategy': dict_imp['cov_strategy_within']}\n",
        "        #S_within = cov_strategy(dict_imp_new, dict_obs_i)  # within the dataset\n",
        "        #print(\"S_within \", S_within)\n",
        "        #S = S_within[None, :, :] + S_between\n",
        "        #S = np.concatenate(S, axis=0)\n",
        "        #print(S)\n",
        "        #alphas_used, coeff_results = train_and_plot(X_train[i, :, :], y_train, S, [ax[1], ax[2]])\n",
        "        temporary_dictionary['S_dts'] = S_dict['S_dts'][i, :, :]\n",
        "        print(\"temporary dict \", temporary_dictionary)\n",
        "        hyper_p_used, coeff_results = train_and_plot(X_train[i, :, :], y_train, temporary_dictionary, [])\n",
        "        idx_best, min_score_partial = best_idx_predictor(X_test, coeff_results, y_test)\n",
        "        print(\"weee \", idx_best)\n",
        "        print(coeff_results.shape)\n",
        "        print(hyper_p_used.shape)\n",
        "        best_coeff_partial, _ = coeff_results[:, idx_best], hyper_p_used[:, idx_best]\n",
        "        print(\"best coeff partial \", best_coeff_partial)\n",
        "        best_coeff += best_coeff_partial\n",
        "        min_score += min_score_partial\n",
        "        #best_alpha += best_alpha_partial\n",
        "        if len(ax) > 0:\n",
        "          ax[0].scatter(X_train[i, M == 0, 0], X_train[i, M == 0, 1])\n",
        "          ax[0].scatter(X_train[i, M == 1, 0], X_train[i, M == 1, 1])\n",
        "          ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', n_s: ' + str(np.sum(M == 0)) + \" n_m: \" + str(np.sum(M > 0)))  # n_s = nbr seen, n_m = nbr missing\n",
        "          add_rectangles(X_train[i, :, 0], X_train[i, :, 1], S[0, 0] * best_alpha_partial, S[1, 1] * best_alpha_partial, ax[0])\n",
        "      best_coeff /= dict_imp['mi_nbr']\n",
        "      min_score /=  dict_imp['mi_nbr']\n",
        "      best_hyper_p = 0  # not important right now\n",
        "      best_alpha_delta_dts = 1  # not important right now\n",
        "      #best_alpha /= dict_imp['mi_nbr']\n",
        "    else:\n",
        "      #alphas_used, coeff_results = train_and_plot(X_train, y_train, S_dict, [ax[1], ax[2]])\n",
        "      hyper_p_used, coeff_results = train_and_plot(X_train, y_train, S_dict, [])\n",
        "      idx_best, min_score = best_idx_predictor(X_test, coeff_results, y_test)\n",
        "      #best_coeff, best_alpha = coeff_results[:, idx_best], alphas_used[idx_best]\n",
        "      #print(\"-----------------> shape hyper_p used \", hyper_p_used.shape)\n",
        "      best_coeff, best_hyper_p = coeff_results[:, idx_best], hyper_p_used[:, idx_best]\n",
        "      #print(\"hyper_p_used \", hyper_p_used.T)\n",
        "      #input()\n",
        "      #print(X_br_train[M == 0, 0])\n",
        "      best_alpha_delta_dts, best_alpha_delta_mis = best_hyper_p[0], best_hyper_p[1]\n",
        "#     print(\"best alpha ----> \", best_alpha_dts)\n",
        "      if len(ax) > 0:\n",
        "        ax[0].scatter(X_train[M == 0, 0], X_train[M == 0, 1])\n",
        "        ax[0].scatter(X_train[M == 1, 0], X_train[M == 1, 1])\n",
        "        #ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', n_s: ' + str(np.sum(M == 0)) + \" n_m: \" + str(np.sum(M > 0)))  # n_s = nbr seen, n_m = nbr missing\n",
        "        # 'multip_betw': 1, 'multip_with':1\n",
        "        ax[0].set_title(dict_imp['imp_method'] + ', ' + dict_imp['cov_strategy'] + ', dts:'+str(dict_imp['multip_dataset']) + ', mis:' + str(dict_imp['multip_missing']) )  # n_s = nbr seen, n_m = nbr missing\n",
        "        S_plot = S_dict['S_dts'] * best_alpha_delta_dts + S_dict['S_mis'] * best_alpha_delta_mis\n",
        "        #print(\"S_plot \", S_plot)\n",
        "        add_rectangles(X_train[:, 0], X_train[:, 1], S_plot, ax[0])\n",
        "        ax[0].set_aspect('equal')  # equal proportion of the axis\n",
        "    #print(\"X_train \", X_train)\n",
        "    #print(\"y_train \", y_train)\n",
        "    #print(\"mask_train \", mask_train)\n",
        "    #print(\"M \", M)\n",
        "\n",
        "\n",
        "    print(\"X_test shape, \", X_test.shape, \",   y_test shape \", y_test.shape)\n",
        "    #print(\"X_test shape, \", X_test.shape)\n",
        "    print(\"---------------------------------> best idx \", idx_best, \" best hyperp [best_alpha_delta_dst, best_alpha_delta_mis]: \", best_hyper_p, \", min score \", min_score)\n",
        "    print(\"---------------------------------> best coeff \", best_coeff)\n",
        "    #input()\n",
        "    #print(\"best 1/alpha \", 1 / best_alpha)\n",
        "    #print(\"min score \", min_score)\n",
        "\n",
        "    #\n",
        "    #add_rectangles(X_train[:, 0], X_train[:, 1], S[0, 0] * best_alpha, S[1, 1] * best_alpha, ax[0])\n",
        "\n",
        "\n",
        "    # obsere that one day you shoul add the return of alpha_delta_mis also\n",
        "    return best_coeff, min_score, -np.log10(best_hyper_p)  # -np.log10((best_alpha_delta_dts + best_alpha_delta_mis)/2)\n",
        "\n"
      ],
      "metadata": {
        "id": "OhNXUBahJgBL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(dictio, methods_strategy):  # ---------------------> new\n",
        "  # dictio: dictionary of lists that contains the parameters of generate_dataset.\n",
        "  # Each list should have the same length\n",
        "  # methods_strategy = list of dictionary, each one of the form\n",
        "  # {'imp_method': .., 'cov_strategy':.., extra info}\n",
        "\n",
        "    l = len(dictio['data'])  # how many trials shall we do\n",
        "    m = len(methods_strategy)\n",
        "    nbr_iter = len(methods_strategy)\n",
        "    coeff_fin = np.zeros((nbr_iter, 2, l))\n",
        "    scores_fin = np.zeros((nbr_iter, l))\n",
        "\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(3 * l , 9 *l), num='advtrain_linf_')\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(6 * l , 9 *l), num='advtrain_linf_')\n",
        "    #fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(6 * l / 2, 9 *l / 2), num='advtrain_linf_')\n",
        "    print(dictio['plots'])\n",
        "    print(dictio['plots'][0])\n",
        "    nbr_ima = len(dictio['plots'][0])\n",
        "    if nbr_ima == 1:\n",
        "      #nbr_ima = 1\n",
        "      fig, ax = plt.subplots(nbr_ima * nbr_iter, l, figsize=(3 * l, 8/3 * m), squeeze=False)#, num='advtrain_linf_')\n",
        "    elif nbr_ima == 3:  # == 3, one day should be more general\n",
        "      #nbr_ima = 3\n",
        "      fig, ax = plt.subplots(3 * nbr_iter, l, figsize=(3 * l, 8 * m), squeeze=False)#, num='advtrain_linf_')\n",
        "\n",
        "    res = {}\n",
        "    for info_imp_cov_dict in methods_strategy:\n",
        "      key_list = []\n",
        "      for value in info_imp_cov_dict.values():\n",
        "        print(value)\n",
        "        key_list.append(value)\n",
        "      key_tuple = tuple(key_list)\n",
        "      res[key_tuple] = {'best_coeff':[], 'l2_dist_best_coeff_gt':[], 'best_score':[], 'best_hyper_p':[], 'best_alpha_dts':[], 'best_alpha_mis':[]}\n",
        "      #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])] = {'best_coeff':[], 'l2_dist_best_coeff_gt':[], 'best_score':[], 'best_alpha':[]}\n",
        "\n",
        "    if dictio['generation'] == 'fixed':  # use this if you want to fix the generated data, and not change at every iteartion\n",
        "      dictio_obser_fixed  = generate_dataset(data=dictio['data'][0],\n",
        "                                    n_tot=dictio['n_tot'][-1],  # last one should be the biggest\n",
        "                                    dim=dictio['dim'][0],\n",
        "                                    beta_gt=dictio['beta_gt'][0],\n",
        "                                    perc_test=dictio['perc_test'][-1],\n",
        "                                    p_miss=dictio['p_miss'][0],\n",
        "                                    err=dictio['err'][0]\n",
        "                                             )  # return {'X_train_masked':(X_train, mask_train) , 'X_test':.., 'y_train':, 'y_test'}\n",
        "      #mask_no_both_seen = generate_masks_2d(dictio['n_train'][0], [0, 0.5, 0.5]) # generate a mask where there are no entries both seen. The idea then will be to consider percentage of this mask seen\n",
        "      full_masks = generate_masks(dictio)\n",
        "    dictio_obser_fixed_copy = copy.deepcopy(dictio_obser_fixed)\n",
        "\n",
        "    for i in range(l):\n",
        "      print(\"---------------------------------------------------------------------------------------------------------------------------> iteration \", i)\n",
        "      #  dict_obs = {'X_train_masked': (X_train, masks_train), 'X_test': ....., 'y_train': ....., 'y_test': ....}\n",
        "      dict_obser_partial = generate_dataset(data=dictio['data'][i],\n",
        "                                    n_tot=dictio['n_tot'][i],\n",
        "                                    dim=dictio['dim'][i],\n",
        "                                    beta_gt=dictio['beta_gt'][i],\n",
        "                                    perc_test=dictio['perc_test'][i],\n",
        "                                    p_miss=dictio['p_miss'][i],\n",
        "                                    err=dictio['err'][i])\n",
        "      if dictio['generation'] == 'fixed':\n",
        "        dict_obser = dictio_obser_fixed\n",
        "        if len(dictio['beta_gt'][0]) == 2:\n",
        "          #mask_partial = dict_obser_partial['X_train_masked'][1]\n",
        "          p_i = dictio['p_miss'][i][0]  # probability of seen both component at round i\n",
        "          n_train = full_masks.shape[0]\n",
        "          mask_partial = full_masks.copy()\n",
        "          mask_partial[0:int(n_train * p_i), :] = 0\n",
        "          tuple_partial = (dictio_obser_fixed['X_train_masked'][0], mask_partial)\n",
        "          dict_obser['X_train_masked'] = tuple_partial\n",
        "        else:\n",
        "          #print(\"size in run experiment\", dictio_obser_fixed_copy['X_train_masked'][0].shape, \"wee \", dictio_obser_fixed['y_train'].shape)\n",
        "          ## we use the next line of code with dictio_obser_fixed_copy because we need to test the mask with the original dataset, otherwise we get size error (the dataset change if an observation get fully hidden)\n",
        "          n_train = dictio['n_train']\n",
        "          print(\"n_tot_fll \", n_train, \",  \", n_train[i])\n",
        "          #print(dictio_obser_fixed_copy['X_train_masked'][0][0:n_train[i], :].shape)\n",
        "          #print(dictio_obser_fixed_copy['X_train_masked'][0].shape)\n",
        "          X_train_cleaned, y_train_cleaned, masks_train_cleaned = clear_dataset(dictio_obser_fixed_copy['X_train_masked'][0][0:n_train[i], :], dictio_obser_fixed_copy['y_train'][0:n_train[i]], full_masks[i][0:n_train[i], :])\n",
        "          print(\"shapes X_train cleaned, mask train cleaned, y train cleaned\")\n",
        "          print(X_train_cleaned.shape)\n",
        "          print(masks_train_cleaned.shape)\n",
        "          print(y_train_cleaned.shape)\n",
        "          #tuple_partial = (dictio_obser_fixed['X_train_masked'][0], full_masks[i])\n",
        "          print(\"full masks in run experiment \", full_masks[i])\n",
        "          dict_obser['X_train_masked'] = (X_train_cleaned, masks_train_cleaned)\n",
        "          dict_obser['y_train'] = y_train_cleaned\n",
        "      else:\n",
        "        dict_obser = dict_obser_partial\n",
        "\n",
        "      #print(\"dict obser \", dict_obser)\n",
        "      print(\"info algo in run experiments \", dictio['info_algo'])\n",
        "      #\n",
        "      # add an entry for imputed dataset, and info for algorithm. This is useful if we carry out the analysis on the same imputed dataset, changing some parameter or the supervised learning analysis,\n",
        "      # for example if we want to test adv vs lin_reg on the same imputed dataset\n",
        "      # observe that with mf_imp we cannot use the same dataset, because the imputed dataset depends on one of the hyperparameter (the one used by PMM), so we need to make artificially a new slot.\n",
        "      #\n",
        "      dict_obser = dict_obser | {'imp_ds':{'BR_si':[], 'l_d':[], 'oracle':[], 'mi':[], 'mf_imp':[], 'mf_imp1':[], 'gc':[]}} | {'info_algo': dictio['info_algo']}\n",
        "\n",
        "      print(\"ciaoooooo dict obser in run experiments \\n \", dict_obser)\n",
        "      for idx, info_imp_cov_dict in enumerate(methods_strategy):\n",
        "        print(\"----------------------------------------------> new method tested: \", info_imp_cov_dict)\n",
        "        if nbr_ima > 0:\n",
        "          coeff_round, score_round, hyper_p_round = experiment_2d_ext_dataset(dict_obser, info_imp_cov_dict, ax[(idx * nbr_ima):((idx+1) * nbr_ima), i])\n",
        "        else:  # == 0\n",
        "          coeff_round, score_round, hyper_p_round = experiment_2d_ext_dataset(dict_obser, info_imp_cov_dict, [])\n",
        "        r = coeff_round - dictio['beta_gt'][i]\n",
        "        l2_dist = np.linalg.norm(r)\n",
        "        key_list = []\n",
        "        for value in info_imp_cov_dict.values():\n",
        "          print(value)\n",
        "          key_list.append(value)\n",
        "        key_tuple = tuple(key_list)\n",
        "        res[key_tuple]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "        res[key_tuple]['best_coeff'].append(coeff_round)\n",
        "        res[key_tuple]['best_score'].append(score_round)\n",
        "        res[key_tuple]['best_hyper_p'].append(hyper_p_round)  # both hyperparameters\n",
        "        res[key_tuple]['best_alpha_dts'].append(hyper_p_round[0])  # one of the hyperparameter\n",
        "        res[key_tuple]['best_alpha_mis'].append(hyper_p_round[1])  # the other hyperparameter\n",
        "\n",
        "        #res[key_tuple]['best_alpha'].append(alpha_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_coeff'].append(coeff_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_score'].append(score_round)\n",
        "        #res[(info_imp_cov_dict['imp_method'], info_imp_cov_dict['cov_strategy'])]['best_alpha'].append(alpha_round)\n",
        "    plt.tight_layout()\n",
        "    return res\n",
        "\n",
        "\n",
        "def plot_res(x_axis_info, res, extra_info):\n",
        "  x_axis = x_axis_info['vector']\n",
        "  print(\"x_axis for print in plot_res----> \", x_axis)\n",
        "  l = len(x_axis)\n",
        "  lb = extra_info['what_to_plot']\n",
        "  nbr_plot = len(lb)\n",
        "  fig_res, ax_res = plt.subplots(1, nbr_plot, figsize=(45 * (nbr_plot / 3 ), 7.5))  # , num='advtrain_linf_res')\n",
        "  positions = range(l)\n",
        "\n",
        "  for key, values in res.items():\n",
        "    print(\"key in plot_res\", key, \": values\\n\", values)\n",
        "    #print(\"values \", values)\n",
        "  #print(\"res\\n \", res)\n",
        "\n",
        "  ch = ['o', 'x', '+', '*', '<', '>', 'p', 'D', 'd', 'v']\n",
        "  # lb = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha']\n",
        "  for i in range(nbr_plot):\n",
        "    for idx, (key, dictio) in enumerate(res.items()):\n",
        "      #print(dictio)\n",
        "      print(\"lb[i] in plot_res \", lb[i], \"  \", dictio[lb[i]])\n",
        "      print(\"key: \", key)\n",
        "      ax_res[i].plot(positions, dictio[lb[i]], marker=ch[idx], label=str(key), color=key[-1])  # the marker is linked to the key (= method), different key correspond to different marker\n",
        "      #ax_res[1].plot(positions, dictio[lb[idx]], marker=ch[idx], label=str(key))\n",
        "      #ax_res[2].plot(positions, -np.log(dictio['best_alpha']), marker=ch[idx], label=str(key))\n",
        "      #ax_res[0].xticks(positions, n_tot)  # Set custom labels for the x-axis\n",
        "    ax_res[i].set_xticks(positions)         # Set the tick positions\n",
        "    ax_res[i].set_xticklabels(x_axis)        # Set the labels at those positions\n",
        "    ax_res[i].set_xlabel(x_axis_info['name'])\n",
        "    #ax_res[i].legend(loc='upper center', bbox_to_anchor=(1, 1))\n",
        "    ax_res[i].legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0., fontsize=20,)\n",
        "  ax_res[0].set_ylabel(\"||hat_Beta - Beta^*||_2\", fontweight='bold', fontsize=16)\n",
        "  ax_res[1].set_ylabel(\"||hat_y - y||_2^2 / n_test\", fontweight='bold', fontsize=16)\n",
        "  dict_err = extra_info['err'][0]\n",
        "  #size_train = extra_info['n_tot'][0]\n",
        "  #ax_res[0].set_title(\"\")\n",
        "  n_test = extra_info['n_test'][0]\n",
        "  #ax_res[1].set_title(\"err: \" + dict_err['type'] + \", scale: \" + str(dict_err['scaling'])  + \", n_test: \" + str(n_test))\n",
        "  #ax_res[0].set_title('n_test: ' + str(n_test) + extra_info['title_infer_error'])\n",
        "  #ax_res[1].set_title('n_test: ' + str(n_test) + extra_info['title_test_error'])\n",
        "  ax_res[0].set_title(extra_info['title_infer_error'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[1].set_title(extra_info['title_test_error'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[2].set_title(extra_info['title_dts_radius'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[3].set_title(extra_info['title_mis_radius'], fontweight='bold', fontsize = 24)\n",
        "  ax_res[2].set_ylabel(\"-log10(alpha)\", fontsize=16, fontweight='bold')\n",
        "  ax_res[3].set_ylabel(\"-log10(alpha)\", fontsize=16, fontweight='bold')\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "def make_dictionary_data(nbr_experiments, n_train, n_test, data, beta_gt, p_miss, err_vector, plots):\n",
        "  # make a dictionary where each element is a list of nbr_experiments element made by the other element of the function\n",
        "  if isinstance(n_train, int):  # in case n_train is just a number\n",
        "    n_train = [n_train] * nbr_experiments\n",
        "  else:  # should be a list of integer\n",
        "    print(\"change nbr_experiments to match the size of n_train\")\n",
        "    nbr_experiments = len(n_train)\n",
        "  if isinstance(n_test, int):  # in case n_test is just a number\n",
        "    n_test = [n_test] * nbr_experiments\n",
        "  n_tot = [x + y for x, y in zip(n_train, n_test)]\n",
        "  perc_test = [x / (x+y) for x, y in zip(n_test, n_train)]\n",
        "  dim = beta_gt.size\n",
        "\n",
        "  list_errors = []\n",
        "  for i in range(nbr_experiments):\n",
        "    err_dic_app = {'type': err_vector[0], 'scaling': err_vector[1][i]}\n",
        "    list_errors.append(err_dic_app)\n",
        "\n",
        "  dictio = {'data':[data] * nbr_experiments,\n",
        "        'n_tot': n_tot,\n",
        "        'n_train': n_train,\n",
        "        'n_test': n_test,\n",
        "        'dim': [dim] * nbr_experiments,\n",
        "        'beta_gt': [beta_gt] * nbr_experiments,\n",
        "        'perc_test': perc_test,\n",
        "        #'p_miss': [p_miss] * nbr_experiments,\n",
        "        'err': list_errors,\n",
        "        'plots': [plots] * nbr_experiments\n",
        "        }\n",
        "  dictio['p_miss'] = p_miss\n",
        "\n",
        "  return dictio\n",
        "\n",
        "def make_probabilities(list_prob):\n",
        "  l = []\n",
        "  for x in list_prob:\n",
        "    l.append([x, 0.5 - x/2, 0.5 - x/2])\n",
        "  return l\n",
        "\n",
        "def make_info_axis(vector, name):\n",
        "  if name == 'train':\n",
        "    dictio = {'name': 'size train set', 'vector': vector}\n",
        "  elif name == 'p_seen':\n",
        "    dictio = {'name': 'probability seen full entries', 'vector': vector}\n",
        "  elif name == 'error':\n",
        "    dictio = {'name': 'error', 'vector': vector}\n",
        "  else:\n",
        "    print(\"wrong info_axis\")\n",
        "  return dictio\n",
        "\n",
        "def make_dictionary_method(list_meth):\n",
        "  # make a dictionary where each element is a list of nbr_experiments element made by the other element of the function\n",
        "  list_dictio=[]\n",
        "  list_key = ['imp_method', 'cov_strategy', 'mi_nbr']\n",
        "  for meth in list_meth:\n",
        "    dictio_imp = {}\n",
        "    for i in range(len(meth)):\n",
        "      dictio_imp[list_key[i]] = meth[i] #= {list_key[i]: meth[i]}\n",
        "      #print(dictio_imp)\n",
        "    list_dictio.append(dictio_imp)\n",
        "  return list_dictio\n",
        "\n",
        "\n",
        "def run_multiple_experiments(nbr_exp, rdm_seed, dictio, info_x_axis):\n",
        "  #rdm_seed = 4654321\n",
        "  np.random.seed(rdm_seed)\n",
        "  res = run_experiments(dictio, list_methods_strategy)\n",
        "  plot_res(info_x_axis, res, dictio)\n",
        "  '''\n",
        "  if nbr_exp > 1:\n",
        "    for k in res:\n",
        "      for h in res[k]:\n",
        "        res[k][h] = [res[k][h]]\n",
        "    for i in range(nbr_exp-1):\n",
        "      print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "      #np.random.seed(rdm_seed * (i+2))\n",
        "      res_partial = run_experiments(dictio, list_methods_strategy)\n",
        "      plot_res(info_x_axis, res_partial, dictio)\n",
        "      print(res)\n",
        "      for k in res:\n",
        "        res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "        res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "        res[k]['best_alpha'].append(res_partial[k]['best_alpha'])\n",
        "        #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "        #res.append(res['l2_dist_best_coeff_gt'])\n",
        "  '''\n",
        "  for k in res:\n",
        "    for h in res[k]:\n",
        "      res[k][h] = [res[k][h]]\n",
        "  for i in range(nbr_exp-1):\n",
        "      print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "      #np.random.seed(rdm_seed * (i+2))\n",
        "      res_partial = run_experiments(dictio, list_methods_strategy)\n",
        "      print(\"res partial \\n\")\n",
        "      for k, value in res_partial.items():\n",
        "        print(\"key: \", k, \" value: \", value)\n",
        "      plot_res(info_x_axis, res_partial, dictio)\n",
        "      print(\"res in run multipl experiments\\n\")\n",
        "#      for k, value in res.items():\n",
        "#        print(\"key: \", k, \" value: \", value)\n",
        "      for k in res:\n",
        "        res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "        res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "        res[k]['best_hyper_p'].append(res_partial[k]['best_hyper_p'])\n",
        "        res[k]['best_alpha_dts'].append(res_partial[k]['best_alpha_dts'])\n",
        "        res[k]['best_alpha_mis'].append(res_partial[k]['best_alpha_mis'])\n",
        "\n",
        "        #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "        #res.append(res['l2_dist_best_coeff_gt'])\n",
        "\n",
        "  print(\"final step, let's take the mean of the results\")\n",
        "  #print(\"res, after all the experimetns \", res)\n",
        "  for k in res:\n",
        "    print(\"key in res \", k)\n",
        "    print(np.array(res[k]['l2_dist_best_coeff_gt']))\n",
        "    print(\"mean l2_dist              \", np.mean(np.array(res[k]['l2_dist_best_coeff_gt']), axis=0))\n",
        "    print(\"mean_l2_dist diff method: \", np.mean(res[k]['l2_dist_best_coeff_gt'], axis=0))\n",
        "  #mean_res = {k: np.mean(v, axis=0) for k, v in res.items()}\n",
        "  mean_res = {k: {v: np.mean(w, axis=0) for v, w in res[k].items()} for k in res}\n",
        "  print(\"final dictionary, dictionary of the means:\")\n",
        "  for k, v in mean_res.items():\n",
        "    print(\"k:   \", k)\n",
        "    for s, t in v.items():\n",
        "      print(s, \": \", t)\n",
        "  return mean_res\n",
        "  #print(np.mean(res, axis=0))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_2LB5UnMpgCC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#info_axis = 'train'\n",
        "#n_train = [400, 800, 1200, 1600, 2000]\n",
        "#p_seen = make_probabilities([0.8, 0.8, 0.8, 0.8, 0.8])\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen\n",
        "#info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "\n",
        "# def get_path(X, y, estimator, amax, dts_max, mis_max, S_dict, eps_amax=1e-4, eps_dts_max=1e-3, eps_mis_max=1e-3, n_alphas=100, n_deltas_dts=2, n_deltas_mis=3):\n",
        "gen = 'fixed'\n",
        "info_axis = 'train'  # train or p_seen\n",
        "#p_seen_both = [1, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.02]\n",
        "#p_seen_both = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "#p_seen_both = [1, 0.9, 0.8/0.9, 0.7/0.8, 0.6/0.7, 0.5/0.6]\n",
        "#p_seen_both = [1, 0.9, 0.8/0.9, 0.7/0.8, 0.6/0.7, 0.5/0.6, 0.4/0.5, 0.3/0.4]\n",
        "n_train = [200, 300, 400]  # check how dataset are generated, there should be some problems with 'fixed'\n",
        "lenght_vec = len(n_train)\n",
        "p_seen_both = [0.6, 1, 1]\n",
        "#p_seen_both = [1, 0.9, 0.8]\n",
        "length_vec = len(p_seen_both)\n",
        "#n_train = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
        "error_vec =  [0] * length_vec\n",
        "#p_seen = make_probabilities(p_seen_both)\n",
        "p_seen = [p_seen_both] * length_vec\n",
        "if info_axis == 'train':\n",
        "  main_vec = n_train\n",
        "  fix_vec = 'prob_seen:' + str(p_seen_both[0])\n",
        "elif info_axis == 'p_seen':\n",
        "  main_vec = np.cumprod(p_seen_both)  # p_seen_both\n",
        "  fix_vec = 'n_train:' + str(n_train[0])\n",
        "elif info_axis == 'error':\n",
        "  main_vec = error_vec\n",
        "#main_vec = n_train if info_axis == 'train' else p_seen_both\n",
        "info_x_axis = make_info_axis(main_vec, info_axis)\n",
        "number_test = 20000\n",
        "#cov_var = 0.6\n",
        "#beta_gt = np.array([-0.5, 2, 1, 3, -2, -3, 4, 0.5, 7, -9, -1, -2, -3, 4, 5, 6, 7, 8])\n",
        "#beta_gt = np.array([2, 4, -0.5, 2, 1, 3, -2, -3, 4, 0.5, 7, -9, -1, -2, -3, 4])\n",
        "beta_gt = np.random.randn(10)\n",
        "print(beta_gt)\n",
        "dim = len(beta_gt)\n",
        "#nbr_feature = int(dim/2)\n",
        "nbr_feature = int(np.sqrt(dim))\n",
        "mean = np.array([0] * dim)\n",
        "matr = np.random.randn(dim, dim) * 1\n",
        "cov = matr.T @ matr + np.eye(dim) * 3\n",
        "# np.array([[1, cov_var], [cov_var, 1]])\n",
        "data_type = 'Gaussian+Unifor2'\n",
        "#cov = np.array([1, 1, 1])\n",
        "\n",
        "dicc = make_dictionary_data(\n",
        "    nbr_experiments= len(main_vec), n_train = n_train, n_test=number_test,\n",
        "    data = {'data': data_type, 'mean': mean, 'cov': cov},\n",
        "    beta_gt = beta_gt,\n",
        "    p_miss = p_seen,\n",
        "    err_vector = ['Gaussian_on_X', error_vec],\n",
        "    plots = [] #['points', 'l1_vs_coef', '1/alpha_vs_coef']\n",
        ")\n",
        "#dicc = dicc | {'generation':gen}\n",
        "dicc['what_to_plot'] = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts']\n",
        "dicc = dicc | {'generation': gen, 'title_infer_error':' inference_error', 'title_test_error':'  test_error'}\n",
        "dicc = dicc | {'title_dts_radius': 'dts_radius', 'title_mis_radius': 'mis_radius'}\n",
        "dicc = dicc | {'info_algo': {'adv_rad_times_delta_dts_max': 100, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg_max': 100,\n",
        "                             'eps_adv_rad_times_delta_dts': 1e-7, 'eps_adv_rad_times_delta_mis': 1e-5, 'eps_alpha_ridge_reg': 1e-7,\n",
        "                             'n_a_dts': 15, 'n_a_mis': 3, 'n_a_rid': 15}}\n",
        "dicc['what_to_plot'] = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts', 'best_alpha_mis']\n",
        "\n",
        "for key, value in dicc.items():\n",
        "  print(key, \": \" , value)\n",
        "\n",
        "# (imp method, cov strategy, mi_nbr)\n",
        "#list_imp_cov_methods = [('BR_si', 'sd'), ('l_d', 'sd'), ('mi', 'sd', 1)]\n",
        "\n",
        "#list_methods_strategy = make_dictionary_method(list_imp_cov_methods)\n",
        "mi_nbr = 5\n",
        "nbr_cand = 10\n",
        "it_mc = 2\n",
        "# def get_path(X, y, estimator, amax, dts_max, mis_max, S_dict, eps_amax=1e-4, eps_dts_max=1e-3, eps_mis_max=1e-3, n_alphas=100, n_deltas_dts=2, n_deltas_mis=3):\n",
        "\n",
        "list_methods_strategy = [#{'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'adv', 'color': 'b'},  #, 'multip_dataset': 3, 'multip_missing':0},\n",
        "\n",
        "                         #{'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'ridge', 'color': 'k'},  #, 'multip_dataset': 3, 'multip_missing':0},\n",
        "                        #{'imp_method': 'l_d', 'cov_strategy': 'std_nan', 'multip_dataset': 3, 'multip_missing':3},\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'zero', 'algo_superv_learn': 'adv'},\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'zero', 'algo_superv_learn': 'ridge'},  #, 'multip_dataset': 3, 'multip_missing': 0}\n",
        "\n",
        "                        # {'imp_method': 'oracle', 'cov_strategy': 'sd', 'algo_superv_learn': 'adv', 'color': 'orange'},\n",
        "\n",
        "                         #{'imp_method': 'oracle', 'cov_strategy': 'sd', 'algo_superv_learn': 'ridge', 'color': 'purple'},\n",
        "                        #{'imp_method': 'BR_si', 'cov_strategy': 'std_nan', 'algo_superv_learn': 'ridge'},#, 'multip_dataset': 3, 'multip_missing':0},\n",
        "                        #{'imp_method': 'l_d', 'cov_strategy': 'std_nan', 'multip_dataset': 3, 'multip_missing':3},\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'sd', 'algo_superv_learn':'ridge'},#, 'multip_dataset': 3, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'nbr_feature': dim, 'algo_superv_learn':'adv', 'color':'g'},  #, 'multip_dataset': 3, 'multip_missing': 3},\n",
        "                        {'imp_method': 'mf_imp1', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'it_mc': it_mc, 'nbr_candidates_mm': nbr_cand, 'algo_superv_learn':'adv', 'color':'r'},  #, 'multip_dataset': 3, 'multip_missing': 3},\n",
        "                        {'imp_method': 'mf_imp', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'it_mc': it_mc, 'nbr_candidates_mm': 0, 'algo_superv_learn':'adv', 'color':'purple'},  #, 'multip_dataset': 3, 'multip_missing': 3},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'algo_superv_learn':'adv', 'color': 'g'}, #, 'multip_dataset': 3, 'multip_missing': 3},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'zero', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'algo_superv_learn':'adv', 'color': 'r'}\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'algo_superv_learn':'adv'}\n",
        "                        #{'imp_method': 'oracle', 'cov_strategy': 'sd', 'multip_dataset': 0, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 1},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 3},\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'std_mi', 'mi_nbr': mi_nbr},\n",
        "                        #{'imp_method': 'mi_pure', 'cov_strategy': 'cond_var', 'cov_strategy_within': 'sd', 'mi_nbr': 5},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 1},\n",
        "                        #{'imp_method': 'mi_mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'eye', 'mi_nbr': 5},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'zero', 'mi_nbr': mi_nbr, 'multip_betw': 0, 'multip_with': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.2},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.4},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'RR', 'mi_nbr': mi_nbr, 'multip_betw': 1, 'multip_with': 0.6},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 0, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 0, 'multip_missing': 1},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr, 'multip_dataset': 3, 'multip_missing': 0},\n",
        "                        #{'imp_method': 'mi', 'post_imp':'conc', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': mi_nbr}#, 'multip_dataset': 3, 'multip_missing': 3}\n",
        "                        #{'imp_method': 'mi', 'cov_strategy': 'RR', 'mi_nbr': 5},\n",
        "                        ]\n",
        "print(list_methods_strategy)\n",
        "for el in list_methods_strategy:\n",
        "  for key, value in el.items():\n",
        "    print(key,\": \" , value)\n",
        "\n",
        "print(\"----> Starting experiments\")\n",
        "\n",
        "'''\n",
        "nbr_exp = 2\n",
        "#res[key_tuple]['l2_dist_best_coeff_gt'].append(l2_dist)\n",
        "#res[key_tuple]['best_coeff'].append(coeff_round)\n",
        "#res[key_tuple]['best_score'].append(score_round)\n",
        "#res[key_tuple]['best_alpha'].append(alpha_round)\n",
        "res_l2 = []\n",
        "\n",
        "rdm_seed = 4654321\n",
        "np.random.seed(rdm_seed)\n",
        "res = run_experiments(dicc, list_methods_strategy)\n",
        "plot_res(info_x_axis, res, dicc)\n",
        "if nbr_exp > 1:\n",
        "  for k in res:\n",
        "    for h in res[k]:\n",
        "      res[k][h] = [res[k][h]]\n",
        "  for i in range(nbr_exp-1):\n",
        "    print(\"--------------------------------------------------------------------------------------nbr_experiment external ---------------> \", i+2, \"-\", i+2, \" \", i+2, \"-\", i+2, \" \", i+2)\n",
        "    #np.random.seed(rdm_seed * (i+2))\n",
        "    res_partial = run_experiments(dicc, list_methods_strategy)\n",
        "    plot_res(info_x_axis, res_partial, dicc)\n",
        "    print(res)\n",
        "    for k in res:\n",
        "      res[k]['l2_dist_best_coeff_gt'].append(res_partial[k]['l2_dist_best_coeff_gt'])\n",
        "      res[k]['best_score'].append(res_partial[k]['best_score'])\n",
        "      res[k]['best_alpha'].append(res_partial[k]['best_alpha'])\n",
        "      #res[k]['best_coeff'].append(res_partial[k]['best_coeff\n",
        "    #res.append(res['l2_dist_best_coeff_gt'])\n",
        "\n",
        "print(\"final \")\n",
        "print(res)\n",
        "for k in res:\n",
        "  print(k)\n",
        "  print(np.array(res[k]['l2_dist_best_coeff_gt']))\n",
        "  print(np.mean(np.array(res[k]['l2_dist_best_coeff_gt']), axis=0))\n",
        "  print(np.mean(res[k]['l2_dist_best_coeff_gt'], axis=0))\n",
        "#mean_res = {k: np.mean(v, axis=0) for k, v in res.items()}\n",
        "mean_res = {k: {v: np.mean(w, axis=0) for v, w in res[k].items()} for k in res}\n",
        "for k, v in mean_res.items():\n",
        "  print(\"k:   \", k)\n",
        "  for s, t in v.items():\n",
        "    print(s, \": \", t)\n",
        "#print(np.mean(res, axis=0))\n",
        "'''\n",
        "\n",
        "nbr_exp = 5\n",
        "seed = 67\n",
        "mean_res = run_multiple_experiments(nbr_exp, seed, dicc, info_x_axis)\n",
        "print(\"PLOT OF THE MEANS\")\n",
        "dicc['title_infer_error'] = 'seed: ' + str(seed) + ', nbr_exp: ' + str(nbr_exp) + ', data: '+ data_type + ', dim: ' + str(dim) # ', cov: ' + str(cov_var)\n",
        "dicc['title_test_error'] = 'sigma_err: ' + str(error_vec[0]) + ', ' + fix_vec + ', n_test: ' + str(number_test)\n",
        "#dicc['title_dts_radius'] = 'dts_radius'\n",
        "#dicc['title_mis_radius'] = 'mis_radius'\n",
        "#dicc = dicc | {'title_dts_radius': 'title_dts_radius', 'title_mis_radius': 'title_mis_radius'}\n",
        "#dicc = dicc | {'generation':gen, 'title_infer_error':'mean_infer_error, rep: ' + str(nbr_exp), 'title_mean_error':'mean_test_error'}\n",
        "#dicc['what_to_plot'] = ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts', 'best_alpha_mis']\n",
        "plot_res(info_x_axis, mean_res, dicc)\n",
        "\n",
        "## you can see if you manage to take the index i that maximize alpha\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bXcjBX8GqIAF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20d82132-bd6f-469f-e8fc-2443db07e968"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.1975067   0.93634827 -0.87511873 -0.94209056 -0.00297886 -0.60034542\n",
            " -0.24725498 -2.00130205  0.80426056  0.969686  ]\n",
            "change nbr_experiments to match the size of n_train\n",
            "data :  [{'data': 'Gaussian+Unifor2', 'mean': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'cov': array([[12.93395282,  9.22753932,  0.51823955,  1.07352708,  8.11523995,\n",
            "        -5.07877084,  3.83896593,  0.53128432,  0.82775374, -2.19272077],\n",
            "       [ 9.22753932, 15.87491415,  0.05113863,  1.70100586,  6.92964267,\n",
            "        -4.80600553,  2.1236879 ,  3.78552386, -2.19307401, -3.1902206 ],\n",
            "       [ 0.51823955,  0.05113863, 11.87899693,  2.63080313,  2.0015445 ,\n",
            "        -2.4062545 ,  2.3557237 ,  1.30533156, -8.24494559, -5.745138  ],\n",
            "       [ 1.07352708,  1.70100586,  2.63080313, 18.80451621,  0.03596694,\n",
            "         2.65432729, -0.25015531,  0.76814063, -1.18598088, -9.79498111],\n",
            "       [ 8.11523995,  6.92964267,  2.0015445 ,  0.03596694, 24.88854206,\n",
            "        -9.9413815 ,  1.37658485, -1.56356712,  0.68223821, -5.07606467],\n",
            "       [-5.07877084, -4.80600553, -2.4062545 ,  2.65432729, -9.9413815 ,\n",
            "        17.41776253, -1.74838999, -3.53145536, -1.55743318,  5.16457272],\n",
            "       [ 3.83896593,  2.1236879 ,  2.3557237 , -0.25015531,  1.37658485,\n",
            "        -1.74838999, 16.52184354,  0.79261921, -1.78760174,  0.26236782],\n",
            "       [ 0.53128432,  3.78552386,  1.30533156,  0.76814063, -1.56356712,\n",
            "        -3.53145536,  0.79261921, 12.69845523,  4.02831109, -3.55894703],\n",
            "       [ 0.82775374, -2.19307401, -8.24494559, -1.18598088,  0.68223821,\n",
            "        -1.55743318, -1.78760174,  4.02831109, 28.09085047,  2.88801392],\n",
            "       [-2.19272077, -3.1902206 , -5.745138  , -9.79498111, -5.07606467,\n",
            "         5.16457272,  0.26236782, -3.55894703,  2.88801392, 13.85688661]])}, {'data': 'Gaussian+Unifor2', 'mean': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'cov': array([[12.93395282,  9.22753932,  0.51823955,  1.07352708,  8.11523995,\n",
            "        -5.07877084,  3.83896593,  0.53128432,  0.82775374, -2.19272077],\n",
            "       [ 9.22753932, 15.87491415,  0.05113863,  1.70100586,  6.92964267,\n",
            "        -4.80600553,  2.1236879 ,  3.78552386, -2.19307401, -3.1902206 ],\n",
            "       [ 0.51823955,  0.05113863, 11.87899693,  2.63080313,  2.0015445 ,\n",
            "        -2.4062545 ,  2.3557237 ,  1.30533156, -8.24494559, -5.745138  ],\n",
            "       [ 1.07352708,  1.70100586,  2.63080313, 18.80451621,  0.03596694,\n",
            "         2.65432729, -0.25015531,  0.76814063, -1.18598088, -9.79498111],\n",
            "       [ 8.11523995,  6.92964267,  2.0015445 ,  0.03596694, 24.88854206,\n",
            "        -9.9413815 ,  1.37658485, -1.56356712,  0.68223821, -5.07606467],\n",
            "       [-5.07877084, -4.80600553, -2.4062545 ,  2.65432729, -9.9413815 ,\n",
            "        17.41776253, -1.74838999, -3.53145536, -1.55743318,  5.16457272],\n",
            "       [ 3.83896593,  2.1236879 ,  2.3557237 , -0.25015531,  1.37658485,\n",
            "        -1.74838999, 16.52184354,  0.79261921, -1.78760174,  0.26236782],\n",
            "       [ 0.53128432,  3.78552386,  1.30533156,  0.76814063, -1.56356712,\n",
            "        -3.53145536,  0.79261921, 12.69845523,  4.02831109, -3.55894703],\n",
            "       [ 0.82775374, -2.19307401, -8.24494559, -1.18598088,  0.68223821,\n",
            "        -1.55743318, -1.78760174,  4.02831109, 28.09085047,  2.88801392],\n",
            "       [-2.19272077, -3.1902206 , -5.745138  , -9.79498111, -5.07606467,\n",
            "         5.16457272,  0.26236782, -3.55894703,  2.88801392, 13.85688661]])}, {'data': 'Gaussian+Unifor2', 'mean': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'cov': array([[12.93395282,  9.22753932,  0.51823955,  1.07352708,  8.11523995,\n",
            "        -5.07877084,  3.83896593,  0.53128432,  0.82775374, -2.19272077],\n",
            "       [ 9.22753932, 15.87491415,  0.05113863,  1.70100586,  6.92964267,\n",
            "        -4.80600553,  2.1236879 ,  3.78552386, -2.19307401, -3.1902206 ],\n",
            "       [ 0.51823955,  0.05113863, 11.87899693,  2.63080313,  2.0015445 ,\n",
            "        -2.4062545 ,  2.3557237 ,  1.30533156, -8.24494559, -5.745138  ],\n",
            "       [ 1.07352708,  1.70100586,  2.63080313, 18.80451621,  0.03596694,\n",
            "         2.65432729, -0.25015531,  0.76814063, -1.18598088, -9.79498111],\n",
            "       [ 8.11523995,  6.92964267,  2.0015445 ,  0.03596694, 24.88854206,\n",
            "        -9.9413815 ,  1.37658485, -1.56356712,  0.68223821, -5.07606467],\n",
            "       [-5.07877084, -4.80600553, -2.4062545 ,  2.65432729, -9.9413815 ,\n",
            "        17.41776253, -1.74838999, -3.53145536, -1.55743318,  5.16457272],\n",
            "       [ 3.83896593,  2.1236879 ,  2.3557237 , -0.25015531,  1.37658485,\n",
            "        -1.74838999, 16.52184354,  0.79261921, -1.78760174,  0.26236782],\n",
            "       [ 0.53128432,  3.78552386,  1.30533156,  0.76814063, -1.56356712,\n",
            "        -3.53145536,  0.79261921, 12.69845523,  4.02831109, -3.55894703],\n",
            "       [ 0.82775374, -2.19307401, -8.24494559, -1.18598088,  0.68223821,\n",
            "        -1.55743318, -1.78760174,  4.02831109, 28.09085047,  2.88801392],\n",
            "       [-2.19272077, -3.1902206 , -5.745138  , -9.79498111, -5.07606467,\n",
            "         5.16457272,  0.26236782, -3.55894703,  2.88801392, 13.85688661]])}]\n",
            "n_tot :  [20200, 20300, 20400]\n",
            "n_train :  [200, 300, 400]\n",
            "n_test :  [20000, 20000, 20000]\n",
            "dim :  [10, 10, 10]\n",
            "beta_gt :  [array([-0.1975067 ,  0.93634827, -0.87511873, -0.94209056, -0.00297886,\n",
            "       -0.60034542, -0.24725498, -2.00130205,  0.80426056,  0.969686  ]), array([-0.1975067 ,  0.93634827, -0.87511873, -0.94209056, -0.00297886,\n",
            "       -0.60034542, -0.24725498, -2.00130205,  0.80426056,  0.969686  ]), array([-0.1975067 ,  0.93634827, -0.87511873, -0.94209056, -0.00297886,\n",
            "       -0.60034542, -0.24725498, -2.00130205,  0.80426056,  0.969686  ])]\n",
            "perc_test :  [0.9900990099009901, 0.9852216748768473, 0.9803921568627451]\n",
            "err :  [{'type': 'Gaussian_on_X', 'scaling': 0}, {'type': 'Gaussian_on_X', 'scaling': 0}, {'type': 'Gaussian_on_X', 'scaling': 0}]\n",
            "plots :  [[], [], []]\n",
            "p_miss :  [[0.6, 1, 1], [0.6, 1, 1], [0.6, 1, 1]]\n",
            "what_to_plot :  ['l2_dist_best_coeff_gt', 'best_score', 'best_alpha_dts', 'best_alpha_mis']\n",
            "generation :  fixed\n",
            "title_infer_error :   inference_error\n",
            "title_test_error :    test_error\n",
            "title_dts_radius :  dts_radius\n",
            "title_mis_radius :  mis_radius\n",
            "info_algo :  {'adv_rad_times_delta_dts_max': 100, 'adv_rad_times_delta_mis_max': 1, 'alpha_ridge_reg_max': 100, 'eps_adv_rad_times_delta_dts': 1e-07, 'eps_adv_rad_times_delta_mis': 1e-05, 'eps_alpha_ridge_reg': 1e-07, 'n_a_dts': 15, 'n_a_mis': 3, 'n_a_rid': 15}\n",
            "[{'imp_method': 'mf_imp1', 'post_imp': 'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': 5, 'it_mc': 2, 'nbr_candidates_mm': 10, 'algo_superv_learn': 'adv', 'color': 'r'}, {'imp_method': 'mf_imp', 'post_imp': 'mean', 'cov_strategy_between': 'cond_var', 'cov_strategy': 'std_nan', 'mi_nbr': 5, 'it_mc': 2, 'nbr_candidates_mm': 0, 'algo_superv_learn': 'adv', 'color': 'purple'}]\n",
            "imp_method :  mf_imp1\n",
            "post_imp :  mean\n",
            "cov_strategy_between :  cond_var\n",
            "cov_strategy :  std_nan\n",
            "mi_nbr :  5\n",
            "it_mc :  2\n",
            "nbr_candidates_mm :  10\n",
            "algo_superv_learn :  adv\n",
            "color :  r\n",
            "imp_method :  mf_imp\n",
            "post_imp :  mean\n",
            "cov_strategy_between :  cond_var\n",
            "cov_strategy :  std_nan\n",
            "mi_nbr :  5\n",
            "it_mc :  2\n",
            "nbr_candidates_mm :  0\n",
            "algo_superv_learn :  adv\n",
            "color :  purple\n",
            "----> Starting experiments\n",
            "[[], [], []]\n",
            "[]\n",
            "mf_imp1\n",
            "mean\n",
            "cond_var\n",
            "std_nan\n",
            "5\n",
            "2\n",
            "10\n",
            "adv\n",
            "r\n",
            "mf_imp\n",
            "mean\n",
            "cond_var\n",
            "std_nan\n",
            "5\n",
            "2\n",
            "0\n",
            "adv\n",
            "purple\n",
            "{'data': 'Gaussian+Unifor2', 'mean': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'cov': array([[12.93395282,  9.22753932,  0.51823955,  1.07352708,  8.11523995,\n",
            "        -5.07877084,  3.83896593,  0.53128432,  0.82775374, -2.19272077],\n",
            "       [ 9.22753932, 15.87491415,  0.05113863,  1.70100586,  6.92964267,\n",
            "        -4.80600553,  2.1236879 ,  3.78552386, -2.19307401, -3.1902206 ],\n",
            "       [ 0.51823955,  0.05113863, 11.87899693,  2.63080313,  2.0015445 ,\n",
            "        -2.4062545 ,  2.3557237 ,  1.30533156, -8.24494559, -5.745138  ],\n",
            "       [ 1.07352708,  1.70100586,  2.63080313, 18.80451621,  0.03596694,\n",
            "         2.65432729, -0.25015531,  0.76814063, -1.18598088, -9.79498111],\n",
            "       [ 8.11523995,  6.92964267,  2.0015445 ,  0.03596694, 24.88854206,\n",
            "        -9.9413815 ,  1.37658485, -1.56356712,  0.68223821, -5.07606467],\n",
            "       [-5.07877084, -4.80600553, -2.4062545 ,  2.65432729, -9.9413815 ,\n",
            "        17.41776253, -1.74838999, -3.53145536, -1.55743318,  5.16457272],\n",
            "       [ 3.83896593,  2.1236879 ,  2.3557237 , -0.25015531,  1.37658485,\n",
            "        -1.74838999, 16.52184354,  0.79261921, -1.78760174,  0.26236782],\n",
            "       [ 0.53128432,  3.78552386,  1.30533156,  0.76814063, -1.56356712,\n",
            "        -3.53145536,  0.79261921, 12.69845523,  4.02831109, -3.55894703],\n",
            "       [ 0.82775374, -2.19307401, -8.24494559, -1.18598088,  0.68223821,\n",
            "        -1.55743318, -1.78760174,  4.02831109, 28.09085047,  2.88801392],\n",
            "       [-2.19272077, -3.1902206 , -5.745138  , -9.79498111, -5.07606467,\n",
            "         5.16457272,  0.26236782, -3.55894703,  2.88801392, 13.85688661]])}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'X_complete' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-813620926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0mnbr_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m67\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m \u001b[0mmean_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_multiple_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_x_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLOT OF THE MEANS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0mdicc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_infer_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'seed: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', nbr_exp: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_exp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', data: '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', dim: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ', cov: ' + str(cov_var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3937397914.py\u001b[0m in \u001b[0;36mrun_multiple_experiments\u001b[0;34m(nbr_exp, rdm_seed, dictio, info_x_axis)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0;31m#rdm_seed = 4654321\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdm_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_methods_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m   \u001b[0mplot_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_x_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   '''\n",
            "\u001b[0;32m/tmp/ipython-input-3937397914.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(dictio, methods_strategy)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fixed'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use this if you want to fix the generated data, and not change at every iteartion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       dictio_obser_fixed  = generate_dataset(data=dictio['data'][0],\n\u001b[0m\u001b[1;32m     38\u001b[0m                                     \u001b[0mn_tot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_tot'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# last one should be the biggest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                     \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-887489672.py\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[0;34m(data, n_tot, dim, beta_gt, perc_test, p_miss, err)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m#  error = np.random.randn(n_tot) * err['scaling']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_complete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0my_complete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_complete\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mbeta_gt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merror\u001b[0m  \u001b[0;31m#np.random.randn(n_tot) * err  # (np.random.rand(n_tot) - 0.5) * err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'X_complete' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DJHHiKFqIUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FhKy2v5qIb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "840TDfGaqIei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SijOuiZYgmon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TG_Fsg4Mgmrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRfmkoYlgmuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vbxlxxNgmyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zJgEeongm1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkEtCuWEgm3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjbzANmcg_cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JsQXdlOg_fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4FH2Zo7g_j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxm1VFuIg_q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cxU4-RZg_uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktlqpLJ9g_ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doiemfrVg_zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bzoCpqTg_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwoz_Lp2g_4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-Kc_0IPg_7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMqppMFSgm5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSeaLOr-qLYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azRmYEueqLbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4l2vYHzqLd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7X3AQIZqLgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iv34YkpnqLju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCRKDvv-qLmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJ6b5Zd_vZ8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7swocNpvZ_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cxt5tWj1vaCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCvPWMOtvaE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9kxssOuRMv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWcLfNW2RM1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S5O3hTLTvaIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fE8VHJ90vaMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZnN2xpSvaOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze5Q9M4Tycsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzWhOlCoycwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZMXiJW-ycz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAINCofcyc20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uORnN_wO05n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQ441A0hO09K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iF_8aBrWO1Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHLiMt5wO1Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gB_XEEFvaQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFwxXhyWefs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Kw8bLBTefv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXN7C2Jfefzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeFvCalref4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "An113TsYef7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kF5dEO1zef-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zuC2em6egA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MM7Zk7OWegDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVL97vbaegFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA3cOcmeegJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FpYA6ClegNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3MxLnLIqLol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DgSOhmwgm7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSdrq6HmqIhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4nfPNbTqIjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4xXWwHeeMR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oonp7YBzeMUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnVLZhvbeMXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9Yk_s6leMZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5asNezNqImF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwGCYcYWqIrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6dLlbTgqIt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lu0iNCNHc_0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "031VAAc5c_4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quVErgChc_-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVqmuefndAEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7M3O9KqdAL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LN_xYsFMdAQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wo4YT1OODeeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wl-gtIlyDeh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnKMsLPgDemY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKCZUoDYDesF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1vpjYZ9dAVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1Fx16kedAX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randint(2, 5, size=(2, 2, 2))\n",
        "print(X)\n",
        "\n",
        "XX = np.concatenate(X)\n",
        "print(XX)\n",
        "\n",
        "\n",
        "Y = np.random.randint(2, 5, size=(1, 3, 2))\n",
        "print(Y)\n",
        "\n",
        "YY = np.concatenate(Y)\n",
        "print(YY)\n",
        "\n",
        "\n",
        "Z = np.random.randint(2, 5, size=(5, 2))\n",
        "print(Z)\n",
        "\n",
        "ZZ = np.concatenate(Z)\n",
        "print(ZZ)\n",
        "\n",
        "print(\"other\")\n",
        "s = np.random.randint(2, 4, 5)\n",
        "print(s)\n",
        "z = np.tile(s, reps=3)  # np.array([s] * 2)\n",
        "print(z)\n",
        "\n",
        "\n",
        "print(\"other mult\")\n",
        "s = np.random.randint(2, 8, size=(3, 2))\n",
        "print(s)\n",
        "z = np.tile(s, reps=(3, 1))  # np.array([s] * 2)\n",
        "print(z)\n"
      ],
      "metadata": {
        "id": "J-KLwpDqTkGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBkG1_lacBfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBILRQvDuI4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dPT52NAbKyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2ShK9JYb6c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fk7A_5N_c1gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgcEt2LBhEBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FiP-uNujLRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6uENE-JShLaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JeqAEblFooY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXByx8OrjqZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4TCzI5siopUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4pGls4IpDT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWpiTpQ5lVg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ASCmfdEnvjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jq6GembmmAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nb6YB8DbvKVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9pv_OW7pJtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgNt4tVYQk7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlebwxRZ1_QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1dmiXA-FcI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDRrQMKGU3Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnNTv-mXVIB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBlyABpt0-qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQgiJb-V-hIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCuJj9HPb2cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VM2QQJkmcsdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## random forest imputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_estimator = RandomForestRegressor(n_estimators=4, max_depth=10, bootstrap=True, max_samples=0.5, n_jobs=2, random_state=0)\n",
        "\n",
        "X_rf = single_imputation(X_nan, rf_estimator)\n",
        "print(X_rf.shape)\n",
        "sd_rf = np.std(X_rf, axis=0)\n",
        "S_inv_rf = np.diag(1 / sd_rf)\n",
        "print(\"std_orig: \\n\", np.std(X_orig, axis=0))\n",
        "print(\"std rf\\n \", sd_rf)\n",
        "fig, ax = plt.subplots(num='advtrain_linf_rf')\n",
        "linfadvtrain_rf = AdversarialTraining(X_rf, y, S_inv_rf, p=np.inf)\n",
        "estimator_rf = lambda X, y, a:  linfadvtrain_rf(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_rf  = get_path(X_rf, y, estimator_rf, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_rf, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "uSgnV3aVXL1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## iterative imputer Bayesian Ridge\n",
        "\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "br_estimator = BayesianRidge()\n",
        "\n",
        "X_br = single_imputation(X_nan, br_estimator)\n",
        "sd_br = np.std(X_br, axis=0)\n",
        "S_inv_br = np.diag(1 / sd_br)\n",
        "print(\"std_orig: \\n\", np.std(X_orig, axis=0))\n",
        "print(\"std  br\\n \", sd_br)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_br')\n",
        "linfadvtrain_br = AdversarialTraining(X_br, y, S_inv_br, p=np.inf)\n",
        "estimator_br = lambda X, y, a:  linfadvtrain_br(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_br  = get_path(X_br, y, estimator_br, 1e4)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_br, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "pgNaP74gWAga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## mean imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X_mean = imp_mean.fit_transform(X_nan)\n",
        "sd_mean = np.std(X_mean, axis=0)\n",
        "print(sd_mean)\n",
        "S_inv_mean = np.diag(1 / sd_mean)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_mean')\n",
        "linfadvtrain_mean = AdversarialTraining(X_mean, y, S_inv_mean, p=np.inf)\n",
        "estimator_mean = lambda X, y, a:  linfadvtrain_mean(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_mean  = get_path(X_mean, y, estimator_mean, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_mean, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "u0kpCJCkFbcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# imputation elliptic\n",
        "\n",
        "mu = np.nanmean(X_nan, axis=0)\n",
        "print(\"means \", mu)\n",
        "delta = np.mean(masks) # parameter missingness\n",
        "print(\"delta \", delta)\n",
        "X_0 = np.nan_to_num(X_nan)\n",
        "print(\"nbr obs\", X_0.shape[0])\n",
        "S_ellp =  X_0.T @ X_0 / X_0.shape[0]\n",
        "S_ellp = (1/delta - 1/(delta**2)) * np.diag(np.diag(S_ellp)) + 1/(delta**2) * S_ellp\n",
        "print(\"eig cov \", np.linalg.eigvalsh(S_ellp))\n",
        "X_ellp = imputation_elliptic(mu, S_ellp, X_nan, masks)\n",
        "#S_inv_ellp = np.linalg.inv(S_ellp)  # other variance\n",
        "sd_inv_ellp = np.std(X_ellp, axis=0)\n",
        "print(\"sd ellp\", sd_inv_ellp)\n",
        "\n",
        "fig, ax = plt.subplots(num='advtrain_linf_ellp')\n",
        "linfadvtrain_ellp = AdversarialTraining(X_ellp, y, S_ellp, p=np.inf)\n",
        "estimator_ellp = lambda X, y, a:  linfadvtrain_ellp(adv_radius=a)\n",
        "alphas_adv, coefs_advtrain_linf_ellp  = get_path(X_ellp, y, estimator_ellp, 1e1)\n",
        "plot_coefs_l1norm(coefs_advtrain_linf_ellp, ax)\n",
        "'''"
      ],
      "metadata": {
        "id": "2RYR4_BJhXjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mlM-FR-OfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgxEbR071wT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pwDiPU0D_ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BdM7Mk_mjf0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYa8pmuMk4jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMwgzXI1_rEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Example data\n",
        "x_test_rect = np.random.rand(10)\n",
        "y_test_rect = np.random.rand(10)\n",
        "\n",
        "# Plot the points\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x_test_rect, y_test_rect)\n",
        "\n",
        "width = 0.1\n",
        "height = 0.1\n",
        "\n",
        "#add_rectangles(x_test_rect, y_test_rect, width, height, ax)\n",
        "\n",
        "# Add the rectangle to the plot\n"
      ],
      "metadata": {
        "id": "-9qaVcwZUB6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WZeO2EOWHwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell for some tests\n",
        "\n",
        "def test_clear_dataset(n, d):\n",
        "  print(\"test clear dataset\")\n",
        "  X = np.random.randint(1, 3, size=(n, d))\n",
        "  y = np.random.randint(1, 3, size=n)\n",
        "  masks = np.random.binomial(1, 0.3, size=(n, d))\n",
        "  print(\"X \\n\", X)\n",
        "  print(\"y\\n\", y)\n",
        "  print(\"masks \\n\", masks)\n",
        "  masks[:, 0] = np.ones(n)\n",
        "  masks[0, :] = np.ones(d)\n",
        "  X_res, y_res, masks_res = clear_dataset(X, y, masks)\n",
        "  print(\"X_res \\n\", X_res)\n",
        "  print(\"y\\n\", y_res)\n",
        "  print(\"masks \\n\", masks_res)\n",
        "  print(\"test clear dataset ended successfully\")\n",
        "\n",
        "def test_generate_X():\n",
        "    print(\"test generate_X started\")\n",
        "    fig, ax = plt.subplots(3, 1, figsize=(10, 8), num='advtrain_linf')\n",
        "    gen = generate_X('circles', 2)\n",
        "    data = gen(1000)\n",
        "    print(data.shape)\n",
        "    ax[0].scatter(data[:, 0], data[:, 1])\n",
        "    print(\"test generate passed syccessfully\")\n",
        "\n",
        "def test_preparation_dataset(n, d):\n",
        "      print(\"\\ntest preparation dataset started\")\n",
        "      X_train = np.random.rand(n, d)\n",
        "      print(\"X_train \\n\", X_train)\n",
        "      mask = np.random.binomial(1, 0.5, (n, d))\n",
        "      print(\"mask, 0 seen, 1 missing \\n \", mask)\n",
        "      X_masked = X_train * (1 - mask)\n",
        "      print(\"X_masked \\n\", X_masked)\n",
        "      X_nan_train = X_train.copy()\n",
        "      X_nan_train[mask == 1] = np.nan\n",
        "      print(\"X_nan_train \\n\", X_nan_train)\n",
        "      X_br_train = single_imputation(X_nan_train, BayesianRidge())\n",
        "      print(\"X_br_train\\n \", X_br_train)\n",
        "\n",
        "      print(\"what happens if we run single_imputation of full dataset\")\n",
        "      X_br_full = single_imputation(X_train, BayesianRidge())\n",
        "      print(\"X_br_full\\n \", X_br_full)\n",
        "      np.testing.assert_allclose(X_train, X_br_full)  # shuold be untouched\n",
        "      print(\"test preparation dataset ended successfully\")\n",
        "\n",
        "def test_listwise_delection(n, d):\n",
        "    print(\"\\n test list_wise delection started\")\n",
        "    X = np.random.rand(n, d)\n",
        "    print(\"data\\n\", X)\n",
        "    mask = np.random.binomial(1, 0.2, (n, d))\n",
        "    print(\"mask \\n\", mask)\n",
        "    X_ld = listwise_delection(X, mask)\n",
        "    print(\"after calling function, X_ld \\n\", X_ld)\n",
        "\n",
        "    print(\"edge cases, all missing\")\n",
        "    mask_1 = np.ones_like(X)  # all missing\n",
        "    X1 = listwise_delection(X, mask_1)\n",
        "    print(\"X1 \\n\", X1)  # should be empty\n",
        "    mask_0 = np.zeros_like(X)  # all seen\n",
        "    X0 = listwise_delection(X, mask_0)\n",
        "    print(\"X0 \\n\", X0)\n",
        "    np.testing.assert_allclose(X0, X)  # should be the original dataset\n",
        "\n",
        "    print(\"one dimnsional array\")\n",
        "    y = np.random.rand(n)\n",
        "    print(\"y before \", y)\n",
        "    y_ld = listwise_delection(y, mask)\n",
        "    print(\"y after ld \", y_ld)\n",
        "    print(\"test listwise_delection passed\")\n",
        "\n",
        "\n",
        "test_generate_X()\n",
        "test_preparation_dataset(3, 4)\n",
        "test_listwise_delection(3, 4)\n",
        "test_clear_dataset(6, 3)\n",
        "\n",
        "xxx = np.random.randint(2, 5, size=(3, 3)) * 1.0\n",
        "mmm = np.random.binomial(1, 0.5, size=(3, 3))\n",
        "print(xxx)\n",
        "print(mmm)\n",
        "print(mmm == 1)\n",
        "print(xxx[mmm == 1])\n",
        "xxx[mmm == 1] = np.nan\n",
        "print(xxx)\n",
        "mask_from_xxx = np.isnan(xxx).astype(int)\n",
        "print(\"mask from xxx \\n\", mask_from_xxx)\n"
      ],
      "metadata": {
        "id": "SDHMAeapZVgK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test best predictor\n",
        "\n",
        "def test_best_predictor(n, d, nb_coeff):\n",
        "  X_test = np.random.randint(1, 9, size=(n, d))\n",
        "  beta_gt_test = np.random.randint(1, 7, size=d)\n",
        "  y_test = X_test @ beta_gt_test\n",
        "  #print(\"X_test \\n\", X_test, \"\\n beta_gt\", beta_gt_test, \"\\n y_test = X_test @ beta_gt_test \", y_test)\n",
        "  coeff_test = np.random.randint(1, 5, size=(d, nb_coeff))\n",
        "  rdm_idx = np.random.randint(1, d+1, size=1)\n",
        "  print(rdm_idx)\n",
        "  #print(\"coeff test partial \", coeff_test[:, -1])\n",
        "  rng = np.arange(nb_coeff)\n",
        "  #print(rng != rdm_idx)\n",
        "  coeff_test[:, rng != rdm_idx] = coeff_test[:, rng != rdm_idx] + 1000  # increase artificially the value of the other coefficient, to induce the minimum index to be rdm_idx\n",
        "  #print(\"coeff_test \\n\", coeff_test)\n",
        "  best_coeff, best_score = best_predictor(X_test, coeff_test, y_test)\n",
        "  print(\"best coeff \", best_coeff)\n",
        "  print(\"best score \", best_score)\n",
        "  np.testing.assert_allclose(best_coeff, coeff_test[:,rdm_idx].squeeze())\n",
        "  print(\"test best predictor passed\")\n",
        "\n",
        "test_best_predictor(100, 5, 20)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YJk1Yaj1ReIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test train_and_plot\n",
        "\n",
        "X_diab, y_diab = datasets.load_diabetes(return_X_y=True)\n",
        "n, d = X_diab.shape\n",
        "print(\"n:  \", n, \", d: \", d)\n",
        "# Standardize data\n",
        "X_diab -= X_diab.mean(axis=0)\n",
        "X_diab /= X_diab.std(axis=0)\n",
        "\n",
        "## original lasso\n",
        "fig_l, ax_l = plt.subplots(num='lasso')\n",
        "alphas_lasso, coefs_lasso, _ = get_lasso_path(X_diab, y_diab)\n",
        "plot_coefs_l1norm(coefs_lasso, ax_l)\n",
        "\n",
        "## Antonio's algo, 1 matrix\n",
        "S_diab_eye = np.eye(X_diab.shape[1])\n",
        "fig, ax_1 = plt.subplots(1, 1, num='advtrain_linf_diab')\n",
        "fig, ax_2 = plt.subplots(1, 1, num='advtrain_linf_diab_2')\n",
        "train_and_plot(X_diab, y_diab, S_diab_eye, [ax_1, ax_2])\n",
        "\n",
        "## Antonio's algo, multiple diagonal matrix\n",
        "#S_diab = np.eye(X_diab.shape[1])\n",
        "#S_diab = np.random.randint(1, 3, size=(n, d))\n",
        "#print(S_diab)\n",
        "#fig, ax_5 = plt.subplots(1, 1, num='advtrain_linf_diab_5')\n",
        "#fig, ax_6 = plt.subplots(1, 1, num='advtrain_linf_diab_6')\n",
        "#train_and_plot(X_diab, y_diab, S_diab, [ax_5, ax_6])\n",
        "\n",
        "\n",
        "## Antonio's algo, multiple matrices (same matrix stacked multiple time)\n",
        "S_diab_stacked = np.array([S_diab_eye] * X_diab.shape[0])\n",
        "S_diab_stacked = np.concatenate(S_diab_stacked)\n",
        "fig, ax_3 = plt.subplots(1, 1, num='advtrain_linf_diab_3')\n",
        "fig, ax_4 = plt.subplots(1, 1, num='advtrain_linf_diab_4')\n",
        "train_and_plot(X_diab, y_diab, S_diab_stacked, [ax_3, ax_4])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KjpHk0mYdiFh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test imputations\n",
        "\n",
        "np.random.seed(45)\n",
        "\n",
        "\n",
        "def test_imputations(n, d):\n",
        "  X = np.random.randint(2, 5, size=(n, d)) * 1.0\n",
        "  y = X @ np.random.randint(1, 3, size=d)\n",
        "  m = np.random.binomial(1, 0.4, size=(n, d))  # 1 missing, 0 seen\n",
        "  print(\"m original\\n\", m)\n",
        "  X, y, m = clear_dataset(X, y, m)\n",
        "  print(m)\n",
        "  X_nan = X.copy()\n",
        "  X_nan[m == 1] = np.nan\n",
        "\n",
        "  #mask_from_xxx = np.isnan(xxx).astype(int)\n",
        "  print(\"X\\n \", X)\n",
        "  print(\"masks \\n\", m)\n",
        "  print(\"X_nan\\n \", X_nan)\n",
        "  methods = ['BR_si', 'mi', 'l_d']\n",
        "  nbr_mi = [1, 3]\n",
        "  #for method in methods:\n",
        "  #  dict_info = {'imp_method': method, 'mi_nbr':nbr_mi}\n",
        "  #dict_info = {'imp_method':methods, 'mi_nbr':nbr_mi}\n",
        "  for method in methods:\n",
        "    print(\"---------- method: \", method)\n",
        "    if method == 'mi':\n",
        "      for x in nbr_mi:\n",
        "        print(\"-------------------- nbr mi: \", x)\n",
        "        dict_info = {'imp_method':method, 'mi_nbr':x}\n",
        "        #print(\"XNANNANAN \", X_nan)\n",
        "        X_res, y_res, mask_res = imputations(dict_info, X_nan, y)\n",
        "        print(X_res, y_res, \"\\n\", mask_res)\n",
        "    else:\n",
        "      dict_info = {'imp_method': method}\n",
        "      X_res, y_res, mask_res = imputations(dict_info, X_nan, y)\n",
        "      print(X_res, y_res, \"\\n\", mask_res)\n",
        "    print(\"test imputations ended successfully\")\n",
        "\n",
        "test_imputations(6, 3)\n"
      ],
      "metadata": {
        "id": "z5crxb1usyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "y = np.array([1, 2])\n",
        "x.append(y)\n",
        "x.append(y)\n",
        "x.append(y)\n",
        "xx = np.stack(x)\n",
        "print(x)\n",
        "print(xx)\n",
        "print(type(xx))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sizes = [100, 1000, 10000, 100000]\n",
        "values = [0.8, 0.85, 0.9, 0.92]\n",
        "positions = range(len(sizes))\n",
        "\n",
        "plt.plot(positions, values, marker='o', label='Model Accuracy')  # Add label here\n",
        "plt.xticks(positions, sizes)\n",
        "\n",
        "plt.xlabel(\"Dataset Size (equispaced)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Performance vs Dataset Size (equispaced x-axis)\")\n",
        "#plt.legend()  # Show legend\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GU2RjW63SNaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dictio = {'a':1, 'b':2, 'c':3}\n",
        "vv = dictio.values()\n",
        "#print(vv)\n",
        "#print(vv[1])\n",
        "\n",
        "x1 = np.array([1, 2, 3])\n",
        "x2 = np.array([3, 2 ,1])\n",
        "v = np.maximum(x1, x2)\n",
        "print(v)\n"
      ],
      "metadata": {
        "id": "UE1NuR4D2h8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "648zfFp8ERD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n, d = 2, 3, 2\n",
        "x_int = np.random.randint(1, 9, (m, n, d))\n",
        "print(x_int)\n",
        "s = np.std(x_int, axis=0)\n",
        "print(s)\n",
        "\n",
        "# manual\n",
        "print(\"manual computation\")\n",
        "x = np.zeros((m, d))\n",
        "for i in range(n):\n",
        "  print(\"i -----> \", i)\n",
        "  x = x_int[:, i, :]\n",
        "  print(\"x\\n\", x)\n",
        "  ss = np.std(x, axis=0)\n",
        "  print(ss)\n",
        "\n",
        "\n",
        "print(\"little exp on squeeze\")\n",
        "sss = np.random.rand(1, 3, 3)\n",
        "print(sss)\n",
        "print(sss.squeeze())\n",
        "print(sss.squeeze())\n",
        "\n"
      ],
      "metadata": {
        "id": "vpBvPibeERnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int(34.99)\n",
        "\n",
        "xxxx = np.random.randint(2, 4, (5, 2))\n",
        "print(xxxx)\n",
        "xxxx[0:2, :] = 1\n",
        "print(xxxx)\n",
        "\n",
        "print(\"yyyy\\n\")\n",
        "yy = []\n",
        "yy.append([1, 2, 3])\n",
        "yy.append([4, 5, 6])\n",
        "print(yy)\n",
        "print(np.stack( yy ).T)\n",
        "print(\"\\n\\n\")\n",
        "yyy = np.random.randint(1, 10, size=(3 , 3))\n",
        "print(yyy)\n",
        "yyy_a = np.array([yyy] * 2)\n",
        "print(yyy_a.shape)\n",
        "print(np.concatenate([yyy] * 2))\n",
        "#print(np.tile(yyy_a, (2, 1, 1) ))\n",
        "\n",
        "zzz = np.zeros((2, 2))\n",
        "\n",
        "np.sum(np.zeros((2, 2)) == zzz)"
      ],
      "metadata": {
        "id": "et578OpzERsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def multiple_imputation1(nbr_mi, X_nan):\n",
        "    n, d = X_nan.shape\n",
        "    res = np.zeros((nbr_mi, n, d))\n",
        "    for i in range(nbr_mi):\n",
        "       n_i = np.random.randint(0, 1000)\n",
        "       ice = IterativeImputer(random_state=n_i, max_iter=50, sample_posterior=True)\n",
        "       res[i, :, :] = ice.fit_transform(X_nan)\n",
        "       #print(\"fin res shape\", res.shape)\n",
        "       #if nbr_mi == 1:\n",
        "        #res = res[0, :, :]\n",
        "        #print(\"fin res shape\", res.shape)\n",
        "    return res\n",
        "\n",
        "\n",
        "Xx = np.random.randint(1, 3, (4, 4)) * 1.0\n",
        "mm = np.random.binomial(1, 0.25, (4, 4))\n",
        "print(Xx)\n",
        "print(mm)\n",
        "Xx[mm == 1] = np.nan\n",
        "print(Xx)\n",
        "\n",
        "ice = IterativeImputer(random_state=18, max_iter=50, sample_posterior=True)\n",
        "ice.fit(Xx)\n",
        "XxX = np.random.randint(1, 3, (2, 4)) * 1.0\n",
        "mmM = np.random.binomial(1, 0.5, (2, 4))\n",
        "print(XxX)\n",
        "print(mmM)\n",
        "XxX[mmM == 1] = np.nan\n",
        "print(XxX)\n",
        "\n",
        "print(ice.transform(XxX))\n",
        "print(ice.transform(XxX))\n",
        "\n",
        "print(\"new\")\n",
        "\n",
        "ls = [[[]],[[]]]\n",
        "print(ls)\n",
        "ls[0][0]\n",
        "\n"
      ],
      "metadata": {
        "id": "_U_r_qaJ9pw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "XX = np.random.randint(1, 7, (2, 3, 3))\n",
        "print(XX)\n",
        "XXX = np.tile(XX, (2, 1, 1))\n",
        "print(XXX)\n",
        "\n",
        "print(np.zeros(2))\n",
        "\n",
        "y_o = np.array([1, 2])\n",
        "y_oo = np.tile(y_o, 3)\n",
        "print(y_oo)\n"
      ],
      "metadata": {
        "id": "PXfceAK8es4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def generate_masks_binomial_general(nbr_of_sample, p_missing):\n",
        "    # nbr_of_sample is the number of masks\n",
        "    # p_missing=[p00, p01, p10], where p00 is the probability of seeing both components,\n",
        "    # p10 is the probability of seeing the right component, p01 is the probability of seeing the left component\n",
        "    masks = np.zeros((nbr_of_sample, 2))\n",
        "    v = np.random.choice(a=3, size=nbr_of_sample, p=p_missing)\n",
        "    masks[v == 0, :] = np.array([0, 0])  # both seen\n",
        "    masks[v == 1, :] = np.array([0, 1])  # left seen\n",
        "    masks[v == 2, :] = np.array([1, 0])  # right seen\n",
        "    return masks\n",
        "\n",
        "\n",
        "#mm = np.random.binomial(1, [[0.2, 0.2, 0.2], [0.8, 0.8, 0.8]], (2, 3, 3))\n",
        "#print(mm)\n",
        "cc = np.array([np.random.binomial(1, x, (4, 4)) for x in [0.2, 0.2, 0.2]])\n",
        "print(cc)\n",
        "s_cc = np.cumsum(cc, axis=0)\n",
        "print(s_cc)\n",
        "s_cc[s_cc>1] = 1\n",
        "print(s_cc)\n",
        "\n",
        "s_v = np.random.randint(1, 4, (3, 4))\n",
        "print(s_v)\n",
        "s_vv = s_v[:, None, :] * np.eye(4)\n",
        "print(s_vv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "auugsvFPZ88A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import ridge_regression\n",
        "\n",
        "X = np.random.randn(100, 4) #rng.randn(100, 4)\n",
        "\n",
        "y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * np.random.randn(100)\n",
        "np.random.seed(4)\n",
        "alphas = [0.00001, 0.001, 0.1, 1]\n",
        "estim = lambda XX, yy, rad: ridge_regression(XX, yy, alpha=rad, return_intercept=False, random_state=0)\n",
        "for a in alphas:\n",
        "  #coef, intercept = estim(X, y, a)\n",
        "  coef = estim(X, y, a)\n",
        "  print(\"coef : \", coef)\n",
        "  #print(\"intercpt \", intercept)\n",
        "  coef, intercept = ridge_regression(X, y, alpha=a, return_intercept=True, random_state=0)\n",
        "  #print(\"coef : \", coef)\n",
        "  #print(\"intercpt \", intercept)\n",
        "\n",
        "\n",
        "lg = np.random.logistic(loc=0.0, scale=1.0, size=(4, 3))\n",
        "print(\"log res \", lg)\n",
        "\n"
      ],
      "metadata": {
        "id": "t3KaZyJKwnRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import miceforest as mf\n",
        "#from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "d = 5\n",
        "n = 10\n",
        "print(\"ciao \")\n",
        "x = np.random.randint(low=0, high=100, size=(n, d))\n",
        "x = pd.DataFrame(x)\n",
        "x.columns = x.columns.astype(str)\n",
        "m = np.random.binomial(1, 0.3, size=(n, d))\n",
        "mm = (m==1)\n",
        "x[mm] = np.nan\n",
        "print(mm)\n",
        "print(x)\n",
        "\n",
        "\n",
        "# Create kernel.\n",
        "kernel = mf.ImputationKernel(\n",
        "  x,\n",
        "  num_datasets=4,\n",
        "  random_state=1,\n",
        "  mean_match_candidates=0\n",
        ")\n",
        "\n",
        "# Run the MICE algorithm for 2 iterations on each of the datasets\n",
        "\n",
        "%time kernel.mice(3)\n",
        "\n",
        "\n",
        "cd1 = kernel.complete_data(dataset=1)\n",
        "cd2 = kernel.complete_data(dataset=2)\n",
        "#print(\"cd1\\n\", cd1, \"\\ncd2\\n\", cd2)\n",
        "res = np.zeros((4, n, d))\n",
        "\n",
        "for i in range(4):\n",
        "  res[i, :, :] = kernel.complete_data(dataset=i)\n",
        "\n",
        "print(\"res\\n \", res)\n",
        "\n",
        "# Printing the kernel will show you some high level information.\n",
        "print(kernel)\n",
        "\n"
      ],
      "metadata": {
        "id": "X4Yyr2ZEACBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72e69d0d"
      },
      "source": [
        "# test miceforest\n",
        "\n",
        "np.random.seed(11)\n",
        "\n",
        "\n",
        "def test_miceforest(n, d, p_seen, nbr_c):\n",
        "    x_mf_test = np.random.randint(low=0, high=99, size=(n, d)) + 0.0\n",
        "    m_mf_test = np.random.binomial(1, p_seen, size=(n, d))\n",
        "    x_mf_test[m_mf_test == 1] = np.nan\n",
        "    print(x_mf_test)\n",
        "    info_mf_test = {'mi_nbr': 2, 'nbr_candidates_mm': nbr_c}\n",
        "\n",
        "    print(np.isnan(x_mf_test))\n",
        "    print(np.isnan(x_mf_test).sum() )\n",
        "\n",
        "\n",
        "    res = miceforest_imputation(info_mf_test, x_mf_test)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "\n",
        "test_miceforest(n=13, d=5, p_seen=0.3, nbr_c=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0972221c"
      },
      "source": [
        "def test_gc_impute(n, d, p_seen, nbr_c, mi_nbr):\n",
        "    xt = np.random.randint(low=0, high=99, size=(n, d)) + 0.0\n",
        "    mt = np.random.binomial(1, p_seen, size=(n, d))\n",
        "    xt[mt == 1] = np.nan\n",
        "    print(\"original nan\\n\", xt)\n",
        "    info_gc = {'nbr_mi':mi_nbr}\n",
        "    res = gc_imputation(info_gc, xt)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "\n",
        "test_gc_impute(n=13, d=5, p_seen=0.3, nbr_c=0, mi_nbr=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8ebaea"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}